[
    {
        "index": 1,
        "raw": "Mohannad Alhanahnah, Yazan Boshmaf, and Benoit Baudry. 2024. DepsRAG: Towards Managing Software Dependencies using Large Language Models. arXiv:2405.20455 [cs.SE] https://arxiv.org/abs/2405.20455",
        "doi": null,
        "year": "2024",
        "has_doi": false,
        "title_from_ref": "DepsRAG: Towards Managing Software Dependencies using Large Language Models",
        "arxiv_block": "arXiv:2405.20455 [cs.SE]",
        "pdf_url": "https://arxiv.org/pdf/2405.20455.pdf",
        "source": "arxiv_direct",
        "abstract": null,
        "title": "DepsRAG: Towards Managing Software Dependencies using Large Language Models",
        "local_pdf_path": "downloaded_papers\\DepsRAG Towards Managing Software Dependencies using Large Language Models.pdf"
    },
    {
        "index": 2,
        "raw": "Zhiyu An, Xianzhong Ding, Yen-Chun Fu, Cheng-Chung Chu, Yan Li, and Wan Du. 2024. Golden-Retriever: HighFidelity Agentic Retrieval Augmented Generation for Industrial Knowledge Base. arXiv:2408.00798 [cs.IR] https: //arxiv.org/abs/2408.00798",
        "doi": null,
        "year": "2024",
        "has_doi": false,
        "title_from_ref": "Golden-Retriever: HighFidelity Agentic Retrieval Augmented Generation for Industrial Knowledge Base",
        "arxiv_block": "arXiv:2408.00798 [cs.IR]",
        "pdf_url": "https://arxiv.org/pdf/2408.00798.pdf",
        "source": "arxiv_direct",
        "abstract": null,
        "title": "Golden-Retriever: HighFidelity Agentic Retrieval Augmented Generation for Industrial Knowledge Base",
        "local_pdf_path": "downloaded_papers\\Golden-Retriever HighFidelity Agentic Retrieval Augmented Generation for Industrial Knowledge Base.pdf"
    },
    {
        "index": 3,
        "raw": "Muhammad Arslan and Christophe Cruz. 2024. Business-RAG: Information Extraction for Business Insights. ICSBT 2024 (2024), 88.",
        "doi": "10.5220/0012812800003764",
        "year": "2024",
        "has_doi": false,
        "title_from_ref": "Business-RAG: Information Extraction for Business Insights",
        "arxiv_block": null,
        "pdf_url": "https://www.scitepress.org/Papers/2024/128128/128128.pdf",
        "source": "scitepress_direct",
        "abstract": null,
        "title": "Business-RAG: Information Extraction for Business Insights",
        "local_pdf_path": "downloaded_papers\\Business-RAG Information Extraction for Business Insights.pdf"
    },
    {
        "index": 4,
        "raw": "Sören Auer, Christian Bizer, Georgi Kobilarov, Jens Lehmann, Richard Cyganiak, and Zachary G. Ives. 2007. DBpedia: A Nucleus for a Web of Open Data. In The Semantic Web, 6th International Semantic Web Conference, 2nd Asian Semantic Web Conference, ISWC 2007 + ASWC 2007, Busan, Korea, November 11-15, 2007 (Lecture Notes in Computer Science, Vol. 4825) . 722–735.",
        "doi": "10.1007/978-3-540-76298-0_52",
        "year": "2007",
        "has_doi": false,
        "title_from_ref": "DBpedia: A Nucleus for a Web of Open Data",
        "arxiv_block": null,
        "pdf_url": null,
        "source": "openalex",
        "abstract": null,
        "title": "DBpedia: A Nucleus for a Web of Open Data",
        "local_pdf_path": null
    },
    {
        "index": 5,
        "raw": "Jinheon Baek, Alham Fikri Aji, Jens Lehmann, and Sung Ju Hwang. 2023. Direct Fact Retrieval from Knowledge Graphs without Entity Linking. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023 . 10038–10055.",
        "doi": null,
        "year": "2023",
        "has_doi": false,
        "title_from_ref": "Direct Fact Retrieval from Knowledge Graphs without Entity Linking",
        "arxiv_block": null,
        "pdf_url": "https://arxiv.org/pdf/2305.12416v1.pdf",
        "source": "arxiv_api",
        "abstract": "There has been a surge of interest in utilizing Knowledge Graphs (KGs) for various natural language processing/understanding tasks. The conventional mechanism to retrieve facts in KGs usually involves three steps: entity span detection, entity disambiguation, and relation classification. However, this approach requires additional labels for training each of the three subcomponents in addition to pairs of input texts and facts, and also may accumulate errors propagated from failures in previous steps. To tackle these limitations, we propose a simple knowledge retrieval framework, which directly retrieves facts from the KGs given the input text based on their representational similarities, which we refer to as Direct Fact Retrieval (DiFaR). Specifically, we first embed all facts in KGs onto a dense embedding space by using a language model trained by only pairs of input texts and facts, and then provide the nearest facts in response to the input text. Since the fact, consisting of only two entities and one relation, has little context to encode, we propose to further refine ranks of top-k retrieved facts with a reranker that contextualizes the input text and the fact jointly. We validate our DiFaR framework on multiple fact retrieval tasks, showing that it significantly outperforms relevant baselines that use the three-step approach.",
        "title": "Direct Fact Retrieval from Knowledge Graphs without Entity Linking",
        "local_pdf_path": "downloaded_papers\\Direct Fact Retrieval from Knowledge Graphs without Entity Linking.pdf"
    },
    {
        "index": 6,
        "raw": "Jinheon Baek, Alham Fikri Aji, and Amir Saffari. 2023. Knowledge-Augmented Language Model Prompting for Zero-Shot Knowledge Graph Question Answering. arXiv:2306.04136 [cs.CL] https://arxiv.org/abs/2306.04136",
        "doi": null,
        "year": "2023",
        "has_doi": false,
        "title_from_ref": "Knowledge-Augmented Language Model Prompting for Zero-Shot Knowledge Graph Question Answering",
        "arxiv_block": "arXiv:2306.04136 [cs.CL]",
        "pdf_url": "https://arxiv.org/pdf/2306.04136.pdf",
        "source": "arxiv_direct",
        "abstract": null,
        "title": "Knowledge-Augmented Language Model Prompting for Zero-Shot Knowledge Graph Question Answering",
        "local_pdf_path": "downloaded_papers\\Knowledge-Augmented Language Model Prompting for Zero-Shot Knowledge Graph Question Answering.pdf"
    },
    {
        "index": 7,
        "raw": "Jinheon Baek, Soyeong Jeong, Minki Kang, Jong C. Park, and Sung Ju Hwang. 2023. Knowledge-Augmented Language Model Verification. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023 . 1720–1736.",
        "doi": null,
        "year": "2023",
        "has_doi": false,
        "title_from_ref": "Knowledge-Augmented Language Model Verification",
        "arxiv_block": null,
        "pdf_url": "https://arxiv.org/pdf/2310.12836v1.pdf",
        "source": "arxiv_api",
        "abstract": "Recent Language Models (LMs) have shown impressive capabilities in generating texts with the knowledge internalized in parameters. Yet, LMs often generate the factually incorrect responses to the given queries, since their knowledge may be inaccurate, incomplete, and outdated. To address this problem, previous works propose to augment LMs with the knowledge retrieved from an external knowledge source. However, such approaches often show suboptimal text generation performance due to two reasons: 1) the model may fail to retrieve the knowledge relevant to the given query, or 2) the model may not faithfully reflect the retrieved knowledge in the generated text. To overcome these, we propose to verify the output and the knowledge of the knowledge-augmented LMs with a separate verifier, which is a small LM that is trained to detect those two types of errors through instruction-finetuning. Then, when the verifier recognizes an error, we can rectify it by either retrieving new knowledge or generating new text. Further, we use an ensemble of the outputs from different instructions with a single verifier to enhance the reliability of the verification processes. We validate the effectiveness of the proposed verification steps on multiple question answering benchmarks, whose results show that the proposed verifier effectively identifies retrieval and generation errors, allowing LMs to provide more factually correct outputs. Our code is available at https://github.com/JinheonBaek/KALMV.",
        "title": "Knowledge-Augmented Language Model Verification",
        "local_pdf_path": "downloaded_papers\\Knowledge-Augmented Language Model Verification.pdf"
    },
    {
        "index": 8,
        "raw": "Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013. Semantic Parsing on Freebase from QuestionAnswer Pairs. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, EMNLP 2013, 18-21 October 2013, Grand Hyatt Seattle, Seattle, Washington, USA, A meeting of SIGDAT, a Special Interest Group of the ACL . 1533–1544.",
        "doi": "10.18653/v1/d13-1160",
        "year": "2013",
        "has_doi": false,
        "title_from_ref": "Semantic Parsing on Freebase from QuestionAnswer Pairs",
        "arxiv_block": null,
        "pdf_url": "https://aclanthology.org/D14-1067.pdf",
        "source": "openalex",
        "abstract": null,
        "title": "Semantic Parsing on Freebase from QuestionAnswer Pairs",
        "local_pdf_path": "downloaded_papers\\Semantic Parsing on Freebase from QuestionAnswer Pairs.pdf"
    },
    {
        "index": 9,
        "raw": "Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. 2020. PIQA: Reasoning about Physical Commonsense in Natural Language. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020 . 7432–7439.",
        "doi": null,
        "year": "2020",
        "has_doi": false,
        "title_from_ref": "PIQA: Reasoning about Physical Commonsense in Natural Language",
        "arxiv_block": null,
        "pdf_url": "https://arxiv.org/pdf/1911.11641v1.pdf",
        "source": "arxiv_api",
        "abstract": "To apply eyeshadow without a brush, should I use a cotton swab or a toothpick? Questions requiring this kind of physical commonsense pose a challenge to today's natural language understanding systems. While recent pretrained models (such as BERT) have made progress on question answering over more abstract domains - such as news articles and encyclopedia entries, where text is plentiful - in more physical domains, text is inherently limited due to reporting bias. Can AI systems learn to reliably answer physical common-sense questions without experiencing the physical world? In this paper, we introduce the task of physical commonsense reasoning and a corresponding benchmark dataset Physical Interaction: Question Answering or PIQA. Though humans find the dataset easy (95% accuracy), large pretrained models struggle (77%). We provide analysis about the dimensions of knowledge that existing models lack, which offers significant opportunities for future research.",
        "title": "PIQA: Reasoning about Physical Commonsense in Natural Language",
        "local_pdf_path": "downloaded_papers\\PIQA Reasoning about Physical Commonsense in Natural Language.pdf"
    },
    {
        "index": 10,
        "raw": "Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. 2008. Freebase: a collaboratively created graph database for structuring human knowledge. In Proceedings of the 2008 ACM SIGMOD international conference on Management of data . 1247–1250.",
        "doi": "10.1145/3638884.3638982",
        "year": "2008",
        "has_doi": false,
        "title_from_ref": "Freebase: a collaboratively created graph database for structuring human knowledge",
        "arxiv_block": null,
        "pdf_url": null,
        "source": "semantic_scholar",
        "abstract": null,
        "title": "Freebase: a collaboratively created graph database for structuring human knowledge",
        "local_pdf_path": null
    },
    {
        "index": 11,
        "raw": "Kurt D. Bollacker, Colin Evans, Praveen K. Paritosh, Tim Sturge, and Jamie Taylor. 2008. Freebase: a collaboratively created graph database for structuring human knowledge. In Proceedings of the ACM SIGMOD International Conference on Management of Data, SIGMOD 2008, Vancouver, BC, Canada, June 10-12, 2008 . 1247–1250.",
        "doi": "10.1145/3638884.3638982",
        "year": "2008",
        "has_doi": false,
        "title_from_ref": "Freebase: a collaboratively created graph database for structuring human knowledge",
        "arxiv_block": null,
        "pdf_url": null,
        "source": "openalex",
        "abstract": null,
        "title": "Freebase: a collaboratively created graph database for structuring human knowledge",
        "local_pdf_path": null
    },
    {
        "index": 12,
        "raw": "Antoine Bordes, Nicolas Usunier, Sumit Chopra, and Jason Weston. 2015. Large-scale Simple Question Answering with Memory Networks. arXiv:1506.02075 [cs.LG] https://arxiv.org/abs/1506.02075",
        "doi": null,
        "year": "2015",
        "has_doi": false,
        "title_from_ref": "Large-scale Simple Question Answering with Memory Networks",
        "arxiv_block": "arXiv:1506.02075 [cs.LG]",
        "pdf_url": "https://arxiv.org/pdf/1506.02075.pdf",
        "source": "arxiv_direct",
        "abstract": null,
        "title": "Large-scale Simple Question Answering with Memory Networks",
        "local_pdf_path": "downloaded_papers\\Large-scale Simple Question Answering with Memory Networks.pdf"
    },
    {
        "index": 13,
        "raw": "Antoine Bordes, Nicolas Usunier, Sumit Chopra, and Jason Weston. 2015. Large-scale Simple Question Answering with Memory Networks. arXiv:1506.02075 [cs.LG] https://arxiv.org/abs/1506.02075",
        "doi": null,
        "year": "2015",
        "has_doi": false,
        "title_from_ref": "Large-scale Simple Question Answering with Memory Networks",
        "arxiv_block": "arXiv:1506.02075 [cs.LG]",
        "pdf_url": "https://arxiv.org/pdf/1506.02075.pdf",
        "source": "arxiv_direct",
        "abstract": null,
        "title": "Large-scale Simple Question Answering with Memory Networks",
        "local_pdf_path": "downloaded_papers\\Large-scale Simple Question Answering with Memory Networks.pdf"
    },
    {
        "index": 14,
        "raw": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al .2020. Language models are few-shot learners. Advances in neural information processing systems 33 (2020), 1877–1901.",
        "doi": null,
        "year": "2020",
        "has_doi": false,
        "title_from_ref": "Language models are few-shot learners",
        "arxiv_block": null,
        "pdf_url": "https://arxiv.org/pdf/2005.14165v4.pdf",
        "source": "arxiv_api",
        "abstract": "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.",
        "title": "Language models are few-shot learners",
        "local_pdf_path": "downloaded_papers\\Language models are few-shot learners.pdf"
    },
    {
        "index": 15,
        "raw": "Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr Settles, Estevam R. Hruschka Jr., and Tom M. Mitchell. 2010. Toward an Architecture for Never-Ending Language Learning. In Proceedings of the Twenty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2010, Atlanta, Georgia, USA, July 11-15, 2010 . 1306–1313. J. ACM, Vol. 37, No. 4, Article 111. Publication date: September 2024. 111:32 Peng et al.",
        "doi": "10.1609/aaai.v24i1.7519",
        "year": "2010",
        "has_doi": false,
        "title_from_ref": "Toward an Architecture for Never-Ending Language Learning",
        "arxiv_block": null,
        "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/download/7519/7380",
        "source": "openalex",
        "abstract": null,
        "title": "Toward an Architecture for Never-Ending Language Learning",
        "local_pdf_path": "downloaded_papers\\Toward an Architecture for Never-Ending Language Learning.pdf"
    },
    {
        "index": 16,
        "raw": "Abir Chakraborty. 2024. Multi-hop Question Answering over Knowledge Graphs using Large Language Models. arXiv:2404.19234 [cs.AI] https://arxiv.org/abs/2404.19234",
        "doi": null,
        "year": "2024",
        "has_doi": false,
        "title_from_ref": "Multi-hop Question Answering over Knowledge Graphs using Large Language Models",
        "arxiv_block": "arXiv:2404.19234 [cs.AI]",
        "pdf_url": "https://arxiv.org/pdf/2404.19234.pdf",
        "source": "arxiv_direct",
        "abstract": null,
        "title": "Multi-hop Question Answering over Knowledge Graphs using Large Language Models",
        "local_pdf_path": "downloaded_papers\\Multi-hop Question Answering over Knowledge Graphs using Large Language Models.pdf"
    },
    {
        "index": 17,
        "raw": "Huajun Chen. 2024. Large Knowledge Model: Perspectives and Challenges. arXiv:2312.02706 [cs.AI] https: //arxiv.org/abs/2312.02706",
        "doi": null,
        "year": "2024",
        "has_doi": false,
        "title_from_ref": "Large Knowledge Model: Perspectives and Challenges",
        "arxiv_block": "arXiv:2312.02706 [cs.AI]",
        "pdf_url": "https://arxiv.org/pdf/2312.02706.pdf",
        "source": "arxiv_direct",
        "abstract": null,
        "title": "Large Knowledge Model: Perspectives and Challenges",
        "local_pdf_path": "downloaded_papers\\Large Knowledge Model Perspectives and Challenges.pdf"
    },
    {
        "index": 18,
        "raw": "Runjin Chen, Tong Zhao, Ajay Jaiswal, Neil Shah, and Zhangyang Wang. 2024. LLaGA: Large Language and Graph Assistant. arXiv:2402.08170 [cs.LG] https://arxiv.org/abs/2402.08170",
        "doi": null,
        "year": "2024",
        "has_doi": false,
        "title_from_ref": "LLaGA: Large Language and Graph Assistant",
        "arxiv_block": "arXiv:2402.08170 [cs.LG]",
        "pdf_url": "https://arxiv.org/pdf/2402.08170.pdf",
        "source": "arxiv_direct",
        "abstract": null,
        "title": "LLaGA: Large Language and Graph Assistant",
        "local_pdf_path": "downloaded_papers\\LLaGA Large Language and Graph Assistant.pdf"
    },
    {
        "index": 19,
        "raw": "Shuang Chen, Qian Liu, Zhiwei Yu, Chin-Yew Lin, Jian-Guang Lou, and Feng Jiang. 2021. ReTraCk: A flexible and efficient framework for knowledge base question answering. In Proceedings of the 59th annual meeting of the association for computational linguistics and the 11th international joint conference on natural language processing: system demonstrations . 325–336.",
        "doi": "10.18653/v1/2021.acl-demo.39",
        "year": "2021",
        "has_doi": false,
        "title_from_ref": "ReTraCk: A flexible and efficient framework for knowledge base question answering",
        "arxiv_block": null,
        "pdf_url": "https://aclanthology.org/2021.acl-demo.39.pdf",
        "source": "semantic_scholar",
        "abstract": "We present Retriever-Transducer-Checker (ReTraCk), a neural semantic parsing framework for large scale knowledge base question answering (KBQA). ReTraCk is designed as a modular framework to maintain high flexibility. It includes a retriever to retrieve relevant KB items efficiently, a transducer to generate logical form with syntax correctness guarantees and a checker to improve transduction procedure. ReTraCk is ranked at top1 overall performance on the GrailQA leaderboard and obtains highly competitive performance on the typical WebQuestionsSP benchmark. Our system can interact with users timely, demonstrating the efficiency of the proposed framework.",
        "title": "ReTraCk: A flexible and efficient framework for knowledge base question answering",
        "local_pdf_path": "downloaded_papers\\ReTraCk A flexible and efficient framework for knowledge base question answering.pdf"
    },
    {
        "index": 20,
        "raw": "Keyuan Cheng, Gang Lin, Haoyang Fei, Yuxuan zhai, Lu Yu, Muhammad Asif Ali, Lijie Hu, and Di Wang. 2024. Multi-hop Question Answering under Temporal Knowledge Editing. arXiv:2404.00492 [cs.CL] https://arxiv.org/abs/ 2404.00492",
        "doi": null,
        "year": "2024",
        "has_doi": false,
        "title_from_ref": "Multi-hop Question Answering under Temporal Knowledge Editing",
        "arxiv_block": "arXiv:2404.00492 [cs.CL]",
        "pdf_url": "https://arxiv.org/pdf/2404.00492.pdf",
        "source": "arxiv_direct",
        "abstract": null,
        "title": "Multi-hop Question Answering under Temporal Knowledge Editing",
        "local_pdf_path": "downloaded_papers\\Multi-hop Question Answering under Temporal Knowledge Editing.pdf"
    },
    {
        "index": 21,
        "raw": "Hyeong Kyu Choi, Seunghun Lee, Jaewon Chu, and Hyunwoo J. Kim. 2023. NuTrea: Neural Tree Search for Contextguided Multi-hop KGQA. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023 .",
        "doi": "10.3233/jifs-239090",
        "year": "2023",
        "has_doi": false,
        "title_from_ref": "NuTrea: Neural Tree Search for Contextguided Multi-hop KGQA",
        "arxiv_block": null,
        "pdf_url": null,
        "source": "raw",
        "abstract": null,
        "title": "NuTrea: Neural Tree Search for Contextguided Multi-hop KGQA",
        "local_pdf_path": null
    },
    {
        "index": 22,
        "raw": "Nurendra Choudhary and Chandan K. Reddy. 2024. Complex Logical Reasoning over Knowledge Graphs using Large Language Models. arXiv:2305.01157 [cs.LO] https://arxiv.org/abs/2305.01157",
        "doi": null,
        "year": "2024",
        "has_doi": false,
        "title_from_ref": "Complex Logical Reasoning over Knowledge Graphs using Large Language Models",
        "arxiv_block": "arXiv:2305.01157 [cs.LO]",
        "pdf_url": "https://arxiv.org/pdf/2305.01157.pdf",
        "source": "arxiv_direct",
        "abstract": null,
        "title": "Complex Logical Reasoning over Knowledge Graphs using Large Language Models",
        "local_pdf_path": "downloaded_papers\\Complex Logical Reasoning over Knowledge Graphs using Large Language Models.pdf"
    },
    {
        "index": 23,
        "raw": "Rajarshi Das, Shehzaad Dhuliawala, Manzil Zaheer, Luke Vilnis, Ishan Durugkar, Akshay Krishnamurthy, Alex Smola, and Andrew McCallum. 2018. Go for a Walk and Arrive at the Answer: Reasoning Over Paths in Knowledge Bases using Reinforcement Learning. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings .",
        "doi": "10.1016/j.asoc.2021.107144",
        "year": "2018",
        "has_doi": false,
        "title_from_ref": "Go for a Walk and Arrive at the Answer: Reasoning Over Paths in Knowledge Bases using Reinforcement Learning",
        "arxiv_block": null,
        "pdf_url": null,
        "source": "semantic_scholar",
        "abstract": "Knowledge bases (KB), both automatically and manually constructed, are often incomplete --- many valid facts can be inferred from the KB by synthesizing existing information. A popular approach to KB completion is to infer new relations by combinatory reasoning over the information found along other paths connecting a pair of entities. Given the enormous size of KBs and the exponential number of paths, previous path-based models have considered only the problem of predicting a missing relation given two entities or evaluating the truth of a proposed triple. Additionally, these methods have traditionally used random paths between fixed entity pairs or more recently learned to pick paths between them. We propose a new algorithm MINERVA, which addresses the much more difficult and practical task of answering questions where the relation is known, but only one entity. Since random walks are impractical in a setting with combinatorially many destinations from a start node, we present a neural reinforcement learning approach which learns how to navigate the graph conditioned on the input query to find predictive paths. Empirically, this approach obtains state-of-the-art results on several datasets, significantly outperforming prior methods.",
        "title": "Go for a Walk and Arrive at the Answer: Reasoning Over Paths in Knowledge Bases using Reinforcement Learning",
        "local_pdf_path": null
    },
    {
        "index": 24,
        "raw": "Mohammad Dehghan, Mohammad Ali Alomrani, Sunyam Bagga, David Alfonso-Hermelo, Khalil Bibi, Abbas Ghaddar, Yingxue Zhang, Xiaoguang Li, Jianye Hao, Qun Liu, Jimmy Lin, Boxing Chen, Prasanna Parthasarathi, Mahdi Biparva, and Mehdi Rezagholizadeh. 2024. EWEK-QA : Enhanced Web and Efficient Knowledge Graph Retrieval for Citationbased Question Answering Systems. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024 . 14169–14187.",
        "doi": "10.18653/v1/2024.acl-long.764",
        "year": "2024",
        "has_doi": false,
        "title_from_ref": "EWEK-QA : Enhanced Web and Efficient Knowledge Graph Retrieval for Citationbased Question Answering Systems",
        "arxiv_block": null,
        "pdf_url": null,
        "source": "semantic_scholar",
        "abstract": "The emerging citation-based QA systems are gaining more attention especially in generative AI search applications. The importance of extracted knowledge provided to these systems is vital from both accuracy (completeness of information) and efficiency (extracting the information in a timely manner). In this regard, citation-based QA systems are suffering from two shortcomings. First, they usually rely only on web as a source of extracted knowledge and adding other external knowledge sources can hamper the efficiency of the system. Second, web-retrieved contents are usually obtained by some simple heuristics such as fixed length or breakpoints which might lead to splitting information into pieces. To mitigate these issues, we propose our enhanced web and efficient knowledge graph (KG) retrieval solution (EWEK-QA) to enrich the content of the extracted knowledge fed to the system. This has been done through designing an adaptive web retriever and incorporating KGs triples in an efficient manner. We demonstrate the effectiveness of EWEK-QA over the open-source state-of-the-art (SoTA) web-based and KG baseline models using a comprehensive set of quantitative and human evaluation experiments. Our model is able to: first, improve the web-retriever baseline in terms of extracting more relevant passages (>20\\%), the coverage of answer span (>25\\%) and self containment (>35\\%); second, obtain and integrate KG triples into its pipeline very efficiently (by avoiding any LLM calls) to outperform the web-only and KG-only SoTA baselines significantly in 7 quantitative QA tasks and our human evaluation.",
        "title": "EWEK-QA : Enhanced Web and Efficient Knowledge Graph Retrieval for Citationbased Question Answering Systems",
        "local_pdf_path": null
    },
    {
        "index": 25,
        "raw": "Yashar Deldjoo, Zhankui He, Julian McAuley, Anton Korikov, Scott Sanner, Arnau Ramisa, René Vidal, Maheswaran Sathiamoorthy, Atoosa Kasirzadeh, and Silvia Milano. 2024. A Review of Modern Recommender Systems Using Generative Models (Gen-RecSys). arXiv:2404.00579 [cs.IR] https://arxiv.org/abs/2404.00579",
        "doi": null,
        "year": "2024",
        "has_doi": false,
        "title_from_ref": "A Review of Modern Recommender Systems Using Generative Models (Gen-RecSys)",
        "arxiv_block": "arXiv:2404.00579 [cs.IR]",
        "pdf_url": "https://arxiv.org/pdf/2404.00579.pdf",
        "source": "arxiv_direct",
        "abstract": null,
        "title": "A Review of Modern Recommender Systems Using Generative Models (Gen-RecSys)",
        "local_pdf_path": "downloaded_papers\\A Review of Modern Recommender Systems Using Generative Models (Gen-RecSys).pdf"
    },
    {
        "index": 26,
        "raw": "Julien Delile, Srayanta Mukherjee, Anton Van Pamel, and Leonid Zhukov. 2024. Graph-Based Retriever Captures the Long Tail of Biomedical Knowledge. arXiv:2402.12352 [cs.CL] https://arxiv.org/abs/2402.12352",
        "doi": null,
        "year": "2024",
        "has_doi": false,
        "title_from_ref": "Graph-Based Retriever Captures the Long Tail of Biomedical Knowledge",
        "arxiv_block": "arXiv:2402.12352 [cs.CL]",
        "pdf_url": "https://arxiv.org/pdf/2402.12352.pdf",
        "source": "arxiv_direct",
        "abstract": null,
        "title": "Graph-Based Retriever Captures the Long Tail of Biomedical Knowledge",
        "local_pdf_path": "downloaded_papers\\Graph-Based Retriever Captures the Long Tail of Biomedical Knowledge.pdf"
    },
    {
        "index": 27,
        "raw": "Tim Dettmers, Pasquale Minervini, Pontus Stenetorp, and Sebastian Riedel. 2018. Convolutional 2D Knowledge Graph Embeddings. In Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018 . 1811–1818.",
        "doi": null,
        "year": "2018",
        "has_doi": false,
        "title_from_ref": "Convolutional 2D Knowledge Graph Embeddings",
        "arxiv_block": null,
        "pdf_url": "https://arxiv.org/pdf/1707.01476v6.pdf",
        "source": "arxiv_api",
        "abstract": "Link prediction for knowledge graphs is the task of predicting missing relationships between entities. Previous work on link prediction has focused on shallow, fast models which can scale to large knowledge graphs. However, these models learn less expressive features than deep, multi-layer models -- which potentially limits performance. In this work, we introduce ConvE, a multi-layer convolutional network model for link prediction, and report state-of-the-art results for several established datasets. We also show that the model is highly parameter efficient, yielding the same performance as DistMult and R-GCN with 8x and 17x fewer parameters. Analysis of our model suggests that it is particularly effective at modelling nodes with high indegree -- which are common in highly-connected, complex knowledge graphs such as Freebase and YAGO3. In addition, it has been noted that the WN18 and FB15k datasets suffer from test set leakage, due to inverse relations from the training set being present in the test set -- however, the extent of this issue has so far not been quantified. We find this problem to be severe: a simple rule-based model can achieve state-of-the-art results on both WN18 and FB15k. To ensure that models are evaluated on datasets where simply exploiting inverse relations cannot yield competitive results, we investigate and validate several commonly used datasets -- deriving robust variants where necessary. We then perform experiments on these robust datasets for our own and several previously proposed models and find that ConvE achieves state-of-the-art Mean Reciprocal Rank across most datasets.",
        "title": "Convolutional 2D Knowledge Graph Embeddings",
        "local_pdf_path": "downloaded_papers\\Convolutional 2D Knowledge Graph Embeddings.pdf"
    },
    {
        "index": 28,
        "raw": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) . 4171–4186.",
        "doi": null,
        "year": "2019",
        "has_doi": false,
        "title_from_ref": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
        "arxiv_block": null,
        "pdf_url": "https://arxiv.org/pdf/1810.04805v2.pdf",
        "source": "arxiv_api",
        "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\n  BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
        "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
        "local_pdf_path": "downloaded_papers\\BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf"
    },
    {
        "index": 29,
        "raw": "Guanting Dong, Rumei Li, Sirui Wang, Yupeng Zhang, Yunsen Xian, and Weiran Xu. 2023. Bridging the KB-Text Gap: Leveraging Structured Knowledge-aware Pre-training for KBQA. In Proceedings of the 32nd ACM International Conference on Information and Knowledge Management, CIKM 2023, Birmingham, United Kingdom, October 21-25, 2023 . 3854–3859.",
        "doi": null,
        "year": "2023",
        "has_doi": false,
        "title_from_ref": "Bridging the KB-Text Gap: Leveraging Structured Knowledge-aware Pre-training for KBQA",
        "arxiv_block": null,
        "pdf_url": "https://arxiv.org/pdf/2308.14436v1.pdf",
        "source": "arxiv_api",
        "abstract": "Knowledge Base Question Answering (KBQA) aims to answer natural language questions with factual information such as entities and relations in KBs. However, traditional Pre-trained Language Models (PLMs) are directly pre-trained on large-scale natural language corpus, which poses challenges for them in understanding and representing complex subgraphs in structured KBs. To bridge the gap between texts and structured KBs, we propose a Structured Knowledge-aware Pre-training method (SKP). In the pre-training stage, we introduce two novel structured knowledge-aware tasks, guiding the model to effectively learn the implicit relationship and better representations of complex subgraphs. In downstream KBQA task, we further design an efficient linearization strategy and an interval attention mechanism, which assist the model to better encode complex subgraphs and shield the interference of irrelevant subgraphs during reasoning respectively. Detailed experiments and analyses on WebQSP verify the effectiveness of SKP, especially the significant improvement in subgraph retrieval (+4.08% H@10).",
        "title": "Bridging the KB-Text Gap: Leveraging Structured Knowledge-aware Pre-training for KBQA",
        "local_pdf_path": "downloaded_papers\\Bridging the KB-Text Gap Leveraging Structured Knowledge-aware Pre-training for KBQA.pdf"
    },
    {
        "index": 30,
        "raw": "Junnan Dong, Qinggang Zhang, Xiao Huang, Keyu Duan, Qiaoyu Tan, and Zhimeng Jiang. 2023. Hierarchy-Aware Multi-Hop Question Answering over Knowledge Graphs. In Proceedings of the ACM Web Conference 2023, WWW 2023, Austin, TX, USA, 30 April 2023 - 4 May 2023 . ACM, 2519–2527.",
        "doi": "10.1145/3543507.3583376",
        "year": "2023",
        "has_doi": false,
        "title_from_ref": "Hierarchy-Aware Multi-Hop Question Answering over Knowledge Graphs",
        "arxiv_block": null,
        "pdf_url": "http://ira.lib.polyu.edu.hk/bitstream/10397/102025/1/Dong_Hierarchy-Aware_Multi-Hop_Question.pdf",
        "source": "semantic_scholar",
        "abstract": "Knowledge graphs (KGs) have been widely used to enhance complex question answering (QA). To understand complex questions, existing studies employ language models (LMs) to encode contexts. Despite the simplicity, they neglect the latent relational information among question concepts and answers in KGs. While question concepts ubiquitously present hyponymy at the semantic level, e.g., mammals and animals, this feature is identically reflected in the hierarchical relations in KGs, e.g., a_type_of. Therefore, we are motivated to explore comprehensive reasoning by the hierarchical structures in KGs to help understand questions. However, it is non-trivial to reason over tree-like structures compared with chained paths. Moreover, identifying appropriate hierarchies relies on expertise. To this end, we propose HamQA, a novel Hierarchy-aware multi-hop Question Answering framework on knowledge graphs, to effectively align the mutual hierarchical information between question contexts and KGs. The entire learning is conducted in Hyperbolic space, inspired by its advantages of embedding hierarchical structures. Specifically, (i) we design a context-aware graph attentive network to capture context information. (ii) Hierarchical structures are continuously preserved in KGs by minimizing the Hyperbolic geodesic distances. The comprehensive reasoning is conducted to jointly train both components and provide a top-ranked candidate as an optimal answer. We achieve a higher ranking than the state-of-the-art multi-hop baselines on the official OpenBookQA leaderboard with an accuracy of 85%.",
        "title": "Hierarchy-Aware Multi-Hop Question Answering over Knowledge Graphs",
        "local_pdf_path": "downloaded_papers\\Hierarchy-Aware Multi-Hop Question Answering over Knowledge Graphs.pdf"
    },
    {
        "index": 31,
        "raw": "Abhimanyu Dubey, Abhinav Jauhri, and et al. 2024. The Llama 3 Herd of Models. arXiv:2407.21783 [cs.AI] https://arxiv.org/abs/2407.21783",
        "doi": null,
        "year": "2024",
        "has_doi": false,
        "title_from_ref": "The Llama 3 Herd of Models",
        "arxiv_block": "arXiv:2407.21783 [cs.AI]",
        "pdf_url": "https://arxiv.org/pdf/2407.21783.pdf",
        "source": "arxiv_direct",
        "abstract": null,
        "title": "The Llama 3 Herd of Models",
        "local_pdf_path": "downloaded_papers\\The Llama 3 Herd of Models.pdf"
    },
    {
        "index": 32,
        "raw": "Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, and Jonathan Larson. 2024. From Local to Global: A Graph RAG Approach to Query-Focused Summarization. arXiv:2404.16130 [cs.CL] https://arxiv.org/abs/2404.16130",
        "doi": null,
        "year": "2024",
        "has_doi": false,
        "title_from_ref": "From Local to Global: A Graph RAG Approach to Query-Focused Summarization",
        "arxiv_block": "arXiv:2404.16130 [cs.CL]",
        "pdf_url": "https://arxiv.org/pdf/2404.16130.pdf",
        "source": "arxiv_direct",
        "abstract": null,
        "title": "From Local to Global: A Graph RAG Approach to Query-Focused Summarization",
        "local_pdf_path": "downloaded_papers\\From Local to Global A Graph RAG Approach to Query-Focused Summarization.pdf"
    },
    {
        "index": 33,
        "raw": "Hady ElSahar, Pavlos Vougiouklis, Arslen Remaci, Christophe Gravier, Jonathon S. Hare, Frédérique Laforest, and Elena Simperl. 2018. T-REx: A Large Scale Alignment of Natural Language with Knowledge Base Triples. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation, LREC 2018, Miyazaki, Japan, May 7-12, 2018. J. ACM, Vol. 37, No. 4, Article 111. Publication date: September 2024. Graph Retrieval-Augmented Generation: A Survey 111:33",
        "doi": "10.3724/2096-7004.di.2025.0074",
        "year": "2018",
        "has_doi": false,
        "title_from_ref": "T-REx: A Large Scale Alignment of Natural Language with Knowledge Base Triples",
        "arxiv_block": null,
        "pdf_url": null,
        "source": "semantic_scholar",
        "abstract": null,
        "title": "T-REx: A Large Scale Alignment of Natural Language with Knowledge Base Triples",
        "local_pdf_path": null
    },
    {
        "index": 34,
        "raw": "Wenqi Fan, Yujuan Ding, Liangbo Ning, Shijie Wang, Hengyun Li, Dawei Yin, Tat-Seng Chua, and Qing Li. 2024. A Survey on RAG Meeting LLMs: Towards Retrieval-Augmented Large Language Models. arXiv:2405.06211 [cs.CL] https://arxiv.org/abs/2405.06211",
        "doi": null,
        "year": "2024",
        "has_doi": false,
        "title_from_ref": "A Survey on RAG Meeting LLMs: Towards Retrieval-Augmented Large Language Models",
        "arxiv_block": "arXiv:2405.06211 [cs.CL]",
        "pdf_url": "https://arxiv.org/pdf/2405.06211.pdf",
        "source": "arxiv_direct",
        "abstract": null,
        "title": "A Survey on RAG Meeting LLMs: Towards Retrieval-Augmented Large Language Models",
        "local_pdf_path": "downloaded_papers\\A Survey on RAG Meeting LLMs Towards Retrieval-Augmented Large Language Models.pdf"
    },
    {
        "index": 35,
        "raw": "Wenqi Fan, Shijie Wang, Jiani Huang, Zhikai Chen, Yu Song, Wenzhuo Tang, Haitao Mao, Hui Liu, Xiaorui Liu, Dawei Yin, and Qing Li. 2024. Graph Machine Learning in the Era of Large Language Models (LLMs). arXiv:2404.14928 [cs.LG] https://arxiv.org/abs/2404.14928",
        "doi": null,
        "year": "2024",
        "has_doi": false,
        "title_from_ref": "Graph Machine Learning in the Era of Large Language Models (LLMs)",
        "arxiv_block": "arXiv:2404.14928 [cs.LG]",
        "pdf_url": "https://arxiv.org/pdf/2404.14928.pdf",
        "source": "arxiv_direct",
        "abstract": null,
        "title": "Graph Machine Learning in the Era of Large Language Models (LLMs)",
        "local_pdf_path": "downloaded_papers\\Graph Machine Learning in the Era of Large Language Models (LLMs).pdf"
    },
    {
        "index": 36,
        "raw": "Haishuo Fang, Xiaodan Zhu, and Iryna Gurevych. 2024. DARA: Decomposition-Alignment-Reasoning Autonomous Language Agent for Question Answering over Knowledge Graphs. arXiv:2406.07080 [cs.CL] https://arxiv.org/abs/ 2406.07080",
        "doi": null,
        "year": "2024",
        "has_doi": false,
        "title_from_ref": "DARA: Decomposition-Alignment-Reasoning Autonomous Language Agent for Question Answering over Knowledge Graphs",
        "arxiv_block": "arXiv:2406.07080 [cs.CL]",
        "pdf_url": "https://arxiv.org/pdf/2406.07080.pdf",
        "source": "arxiv_direct",
        "abstract": null,
        "title": "DARA: Decomposition-Alignment-Reasoning Autonomous Language Agent for Question Answering over Knowledge Graphs",
        "local_pdf_path": "downloaded_papers\\DARA Decomposition-Alignment-Reasoning Autonomous Language Agent for Question Answering over Knowled.pdf"
    },
    {
        "index": 37,
        "raw": "Jinyuan Fang, Zaiqiao Meng, and Craig MacDonald. 2024. REANO: Optimising Retrieval-Augmented Reader Models through Knowledge Graph Generation. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024 . 2094–2112.",
        "doi": "10.18653/v1/2024.acl-long.115",
        "year": "2024",
        "has_doi": false,
        "title_from_ref": "REANO: Optimising Retrieval-Augmented Reader Models through Knowledge Graph Generation",
        "arxiv_block": null,
        "pdf_url": null,
        "source": "openalex",
        "abstract": null,
        "title": "REANO: Optimising Retrieval-Augmented Reader Models through Knowledge Graph Generation",
        "local_pdf_path": null
    },
    {
        "index": 38,
        "raw": "Bahare Fatemi, Jonathan Halcrow, and Bryan Perozzi. 2023. Talk like a Graph: Encoding Graphs for Large Language Models. arXiv:2310.04560 [cs.LG] https://arxiv.org/abs/2310.04560",
        "doi": null,
        "year": "2023",
        "has_doi": false,
        "title_from_ref": "Talk like a Graph: Encoding Graphs for Large Language Models",
        "arxiv_block": "arXiv:2310.04560 [cs.LG]",
        "pdf_url": "https://arxiv.org/pdf/2310.04560.pdf",
        "source": "arxiv_direct",
        "abstract": null,
        "title": "Talk like a Graph: Encoding Graphs for Large Language Models",
        "local_pdf_path": "downloaded_papers\\Talk like a Graph Encoding Graphs for Large Language Models.pdf"
    },
    {
        "index": 39,
        "raw": "Chao Feng, Xinyu Zhang, and Zichu Fei. 2023. Knowledge Solver: Teaching LLMs to Search for Domain Knowledge from Knowledge Graphs. arXiv:2309.03118 [cs.CL] https://arxiv.org/abs/2309.03118",
        "doi": null,
        "year": "2023",
        "has_doi": false,
        "title_from_ref": "Knowledge Solver: Teaching LLMs to Search for Domain Knowledge from Knowledge Graphs",
        "arxiv_block": "arXiv:2309.03118 [cs.CL]",
        "pdf_url": "https://arxiv.org/pdf/2309.03118.pdf",
        "source": "arxiv_direct",
        "abstract": null,
        "title": "Knowledge Solver: Teaching LLMs to Search for Domain Knowledge from Knowledge Graphs",
        "local_pdf_path": "downloaded_papers\\Knowledge Solver Teaching LLMs to Search for Domain Knowledge from Knowledge Graphs.pdf"
    },
    {
        "index": 40,
        "raw": "Yanlin Feng, Xinyue Chen, Bill Yuchen Lin, Peifeng Wang, Jun Yan, and Xiang Ren. 2020. Scalable Multi-Hop Relational Reasoning for Knowledge-Aware Question Answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020 . 1295–1309.",
        "doi": null,
        "year": "2020",
        "has_doi": false,
        "title_from_ref": "Scalable Multi-Hop Relational Reasoning for Knowledge-Aware Question Answering",
        "arxiv_block": null,
        "pdf_url": "https://arxiv.org/pdf/2005.00646v2.pdf",
        "source": "arxiv_api",
        "abstract": "Existing work on augmenting question answering (QA) models with external knowledge (e.g., knowledge graphs) either struggle to model multi-hop relations efficiently, or lack transparency into the model's prediction rationale. In this paper, we propose a novel knowledge-aware approach that equips pre-trained language models (PTLMs) with a multi-hop relational reasoning module, named multi-hop graph relation network (MHGRN). It performs multi-hop, multi-relational reasoning over subgraphs extracted from external knowledge graphs. The proposed reasoning module unifies path-based reasoning methods and graph neural networks to achieve better interpretability and scalability. We also empirically show its effectiveness and scalability on CommonsenseQA and OpenbookQA datasets, and interpret its behaviors with case studies.",
        "title": "Scalable Multi-Hop Relational Reasoning for Knowledge-Aware Question Answering",
        "local_pdf_path": "downloaded_papers\\Scalable Multi-Hop Relational Reasoning for Knowledge-Aware Question Answering.pdf"
    },
    {
        "index": 41,
        "raw": "Bin Fu, Yunqi Qiu, Chengguang Tang, Yang Li, Haiyang Yu, and Jian Sun. 2020. A Survey on Complex Question Answering over Knowledge Base: Recent Advances and Challenges. arXiv:2007.13069 [cs.CL] https://arxiv.org/abs/ 2007.13069",
        "doi": null,
        "year": "2020",
        "has_doi": false,
        "title_from_ref": "A Survey on Complex Question Answering over Knowledge Base: Recent Advances and Challenges",
        "arxiv_block": "arXiv:2007.13069 [cs.CL]",
        "pdf_url": "https://arxiv.org/pdf/2007.13069.pdf",
        "source": "arxiv_direct",
        "abstract": null,
        "title": "A Survey on Complex Question Answering over Knowledge Base: Recent Advances and Challenges",
        "local_pdf_path": "downloaded_papers\\A Survey on Complex Question Answering over Knowledge Base Recent Advances and Challenges.pdf"
    },
    {
        "index": 42,
        "raw": "Mikhail Galkin, Xinyu Yuan, Hesham Mostafa, Jian Tang, and Zhaocheng Zhu. 2023. Towards Foundation Models for Knowledge Graph Reasoning. In The Twelfth International Conference on Learning Representations .",
        "doi": null,
        "year": "2023",
        "has_doi": false,
        "title_from_ref": "Towards Foundation Models for Knowledge Graph Reasoning",
        "arxiv_block": null,
        "pdf_url": "https://arxiv.org/pdf/2310.04562v2.pdf",
        "source": "arxiv_api",
        "abstract": "Foundation models in language and vision have the ability to run inference on any textual and visual inputs thanks to the transferable representations such as a vocabulary of tokens in language. Knowledge graphs (KGs) have different entity and relation vocabularies that generally do not overlap. The key challenge of designing foundation models on KGs is to learn such transferable representations that enable inference on any graph with arbitrary entity and relation vocabularies. In this work, we make a step towards such foundation models and present ULTRA, an approach for learning universal and transferable graph representations. ULTRA builds relational representations as a function conditioned on their interactions. Such a conditioning strategy allows a pre-trained ULTRA model to inductively generalize to any unseen KG with any relation vocabulary and to be fine-tuned on any graph. Conducting link prediction experiments on 57 different KGs, we find that the zero-shot inductive inference performance of a single pre-trained ULTRA model on unseen graphs of various sizes is often on par or better than strong baselines trained on specific graphs. Fine-tuning further boosts the performance.",
        "title": "Towards Foundation Models for Knowledge Graph Reasoning",
        "local_pdf_path": "downloaded_papers\\Towards Foundation Models for Knowledge Graph Reasoning.pdf"
    },
    {
        "index": 43,
        "raw": "Hanning Gao, Lingfei Wu, Po Hu, Zhihua Wei, Fangli Xu, and Bo Long. 2022. Graph-augmented Learning to Rank for Querying Large-scale Knowledge Graph. In Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing, AACL/IJCNLP 2022 - Volume 1: Long Papers, Online Only, November 20-23, 2022 . 82–92.",
        "doi": null,
        "year": "2022",
        "has_doi": false,
        "title_from_ref": "Graph-augmented Learning to Rank for Querying Large-scale Knowledge Graph",
        "arxiv_block": null,
        "pdf_url": "https://arxiv.org/pdf/2111.10541v4.pdf",
        "source": "arxiv_api",
        "abstract": "Knowledge graph question answering (KGQA) based on information retrieval aims to answer a question by retrieving answer from a large-scale knowledge graph. Most existing methods first roughly retrieve the knowledge subgraphs (KSG) that may contain candidate answer, and then search for the exact answer in the KSG. However, the KSG may contain thousands of candidate nodes since the knowledge graph involved in querying is often of large scale, thus decreasing the performance of answer selection. To tackle this problem, we first propose to partition the retrieved KSG to several smaller sub-KSGs via a new subgraph partition algorithm and then present a graph-augmented learning to rank model to select the top-ranked sub-KSGs from them. Our proposed model combines a novel subgraph matching networks to capture global interactions in both question and subgraphs, and an Enhanced Bilateral Multi-Perspective Matching model is proposed to capture local interactions. Finally, we apply an answer selection model on the full KSG and the top-ranked sub-KSGs respectively to validate the effectiveness of our proposed graph-augmented learning to rank method. The experimental results on multiple benchmark datasets have demonstrated the effectiveness of our approach.",
        "title": "Graph-augmented Learning to Rank for Querying Large-scale Knowledge Graph",
        "local_pdf_path": "downloaded_papers\\Graph-augmented Learning to Rank for Querying Large-scale Knowledge Graph.pdf"
    },
    {
        "index": 44,
        "raw": "Yifu Gao, Linbo Qiao, Zhigang Kan, Zhihua Wen, Yongquan He, and Dongsheng Li. 2024. Two-stage Generative Question Answering on Temporal Knowledge Graph Using Large Language Models. arXiv:2402.16568 [cs.CL] https://arxiv.org/abs/2402.16568",
        "doi": null,
        "year": "2024",
        "has_doi": false,
        "title_from_ref": "Two-stage Generative Question Answering on Temporal Knowledge Graph Using Large Language Models",
        "arxiv_block": "arXiv:2402.16568 [cs.CL]",
        "pdf_url": "https://arxiv.org/pdf/2402.16568.pdf",
        "source": "arxiv_direct",
        "abstract": null,
        "title": "Two-stage Generative Question Answering on Temporal Knowledge Graph Using Large Language Models",
        "local_pdf_path": "downloaded_papers\\Two-stage Generative Question Answering on Temporal Knowledge Graph Using Large Language Models.pdf"
    },
    {
        "index": 45,
        "raw": "Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, and Haofen Wang. 2024. Retrieval-Augmented Generation for Large Language Models: A Survey. arXiv:2312.10997 [cs.CL] https://arxiv.org/abs/2312.10997",
        "doi": null,
        "year": "2024",
        "has_doi": false,
        "title_from_ref": "Retrieval-Augmented Generation for Large Language Models: A Survey",
        "arxiv_block": "arXiv:2312.10997 [cs.CL]",
        "pdf_url": "https://arxiv.org/pdf/2312.10997.pdf",
        "source": "arxiv_direct",
        "abstract": null,
        "title": "Retrieval-Augmented Generation for Large Language Models: A Survey",
        "local_pdf_path": "downloaded_papers\\Retrieval-Augmented Generation for Large Language Models A Survey.pdf"
    },
    {
        "index": 46,
        "raw": "Aashish Ghimire, James Prather, and John Edwards. 2024. Generative AI in Education: A Study of Educators’ Awareness, Sentiments, and Influencing Factors. arXiv:2403.15586 [cs.AI] https://arxiv.org/abs/2403.15586",
        "doi": null,
        "year": "2024",
        "has_doi": false,
        "title_from_ref": "Generative AI in Education: A Study of Educators’ Awareness, Sentiments, and Influencing Factors",
        "arxiv_block": "arXiv:2403.15586 [cs.AI]",
        "pdf_url": "https://arxiv.org/pdf/2403.15586.pdf",
        "source": "arxiv_direct",
        "abstract": null,
        "title": "Generative AI in Education: A Study of Educators’ Awareness, Sentiments, and Influencing Factors",
        "local_pdf_path": "downloaded_papers\\Generative AI in Education A Study of Educators’ Awareness, Sentiments, and Influencing Factors.pdf"
    },
    {
        "index": 47,
        "raw": "Yu Gu, Sue Kase, Michelle Vanni, Brian M. Sadler, Percy Liang, Xifeng Yan, and Yu Su. 2021. Beyond I.I.D.: Three Levels of Generalization for Question Answering on Knowledge Bases. In WWW ’21: The Web Conference 2021, Virtual Event / Ljubljana, Slovenia, April 19-23, 2021 . 3477–3488.",
        "doi": null,
        "year": "2021",
        "has_doi": false,
        "title_from_ref": "Beyond I",
        "arxiv_block": null,
        "pdf_url": "https://arxiv.org/pdf/1001.1866v1.pdf",
        "source": "arxiv_api",
        "abstract": "Arguments on the need, and usefulness, of going beyond the usual Hausdorff-Kuratowski-Bourbaki, or in short, HKB concept of topology are presented. The motivation comes, among others, from well known {\\it topological type processes}, or in short TTP-s, in the theories of Measure, Integration and Ordered Spaces. These TTP-s, as shown by the classical characterization given by the {\\it four Moore-Smith conditions}, can {\\it no longer} be incorporated within the usual HKB topologies. One of the most successful recent ways to go beyond HKB topologies is that developed in Beattie & Butzmann. It is shown in this work how that extended concept of topology is a {\\it particular} case of the earlier one suggested and used by the first author in the study of generalized solutions of large classes of nonlinear partial differential equations.",
        "title": "Beyond I",
        "local_pdf_path": "downloaded_papers\\Beyond I.pdf"
    },
    {
        "index": 48,
        "raw": "Yu Gu and Yu Su. 2022. ArcaneQA: Dynamic Program Induction and Contextualized Encoding for Knowledge Base Question Answering. In Proceedings of the 29th International Conference on Computational Linguistics . 1718–1731.",
        "doi": "10.2498/cit.1001912",
        "year": "2022",
        "has_doi": false,
        "title_from_ref": "ArcaneQA: Dynamic Program Induction and Contextualized Encoding for Knowledge Base Question Answering",
        "arxiv_block": null,
        "pdf_url": "http://arxiv.org/pdf/2204.08109",
        "source": "semantic_scholar",
        "abstract": "Question answering on knowledge bases (KBQA) poses a unique challenge for semantic parsing research due to two intertwined challenges: large search space and ambiguities in schema linking. Conventional ranking-based KBQA models, which rely on a candidate enumeration step to reduce the search space, struggle with flexibility in predicting complicated queries and have impractical running time. In this paper, we present ArcaneQA, a novel generation-based model that addresses both the large search space and the schema linking challenges in a unified framework with two mutually boosting ingredients: dynamic program induction for tackling the large search space and dynamic contextualized encoding for schema linking. Experimental results on multiple popular KBQA datasets demonstrate the highly competitive performance of ArcaneQA in both effectiveness and efficiency.",
        "title": "ArcaneQA: Dynamic Program Induction and Contextualized Encoding for Knowledge Base Question Answering",
        "local_pdf_path": "downloaded_papers\\ArcaneQA Dynamic Program Induction and Contextualized Encoding for Knowledge Base Question Answering.pdf"
    },
    {
        "index": 49,
        "raw": "Jiayan Guo, Lun Du, Hengyu Liu, Mengyu Zhou, Xinyi He, and Shi Han. 2023. GPT4Graph: Can Large Language Models Understand Graph Structured Data ? An Empirical Evaluation and Benchmarking. arXiv:2305.15066 [cs.AI] https://arxiv.org/abs/2305.15066",
        "doi": null,
        "year": "2023",
        "has_doi": false,
        "title_from_ref": "GPT4Graph: Can Large Language Models Understand Graph Structured Data ? An Empirical Evaluation and Benchmarking",
        "arxiv_block": "arXiv:2305.15066 [cs.AI]",
        "pdf_url": "https://arxiv.org/pdf/2305.15066.pdf",
        "source": "arxiv_direct",
        "abstract": null,
        "title": "GPT4Graph: Can Large Language Models Understand Graph Structured Data ? An Empirical Evaluation and Benchmarking",
        "local_pdf_path": "downloaded_papers\\GPT4Graph Can Large Language Models Understand Graph Structured Data  An Empirical Evaluation and Be.pdf"
    },
    {
        "index": 50,
        "raw": "Tiezheng Guo, Qingwen Yang, Chen Wang, Yanyi Liu, Pan Li, Jiawei Tang, Dapeng Li, and Yingyou Wen. 2024. KnowledgeNavigator: Leveraging Large Language Models for Enhanced Reasoning over Knowledge Graph. arXiv:2312.15880 [cs.CL] https://arxiv.org/abs/2312.15880",
        "doi": null,
        "year": "2024",
        "has_doi": false,
        "title_from_ref": "KnowledgeNavigator: Leveraging Large Language Models for Enhanced Reasoning over Knowledge Graph",
        "arxiv_block": "arXiv:2312.15880 [cs.CL]",
        "pdf_url": "https://arxiv.org/pdf/2312.15880.pdf",
        "source": "arxiv_direct",
        "abstract": null,
        "title": "KnowledgeNavigator: Leveraging Large Language Models for Enhanced Reasoning over Knowledge Graph",
        "local_pdf_path": "downloaded_papers\\KnowledgeNavigator Leveraging Large Language Models for Enhanced Reasoning over Knowledge Graph.pdf"
    },
    {
        "index": 51,
        "raw": "Bernal Jiménez Gutiérrez, Yiheng Shu, Yu Gu, Michihiro Yasunaga, and Yu Su. 2024. HippoRAG: Neurobiologically Inspired Long-Term Memory for Large Language Models. arXiv:2405.14831 [cs.CL] https://arxiv.org/abs/2405.14831",
        "doi": null,
        "year": "2024",
        "has_doi": false,
        "title_from_ref": "HippoRAG: Neurobiologically Inspired Long-Term Memory for Large Language Models",
        "arxiv_block": "arXiv:2405.14831 [cs.CL]",
        "pdf_url": "https://arxiv.org/pdf/2405.14831.pdf",
        "source": "arxiv_direct",
        "abstract": null,
        "title": "HippoRAG: Neurobiologically Inspired Long-Term Memory for Large Language Models",
        "local_pdf_path": "downloaded_papers\\HippoRAG Neurobiologically Inspired Long-Term Memory for Large Language Models.pdf"
    },
    {
        "index": 52,
        "raw": "William L. Hamilton, Zhitao Ying, and Jure Leskovec. 2017. Inductive Representation Learning on Large Graphs. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA . 1024–1034.",
        "doi": null,
        "year": "2017",
        "has_doi": false,
        "title_from_ref": "Inductive Representation Learning on Large Graphs",
        "arxiv_block": null,
        "pdf_url": "https://arxiv.org/pdf/1706.02216v4.pdf",
        "source": "arxiv_api",
        "abstract": "Low-dimensional embeddings of nodes in large graphs have proved extremely useful in a variety of prediction tasks, from content recommendation to identifying protein functions. However, most existing approaches require that all nodes in the graph are present during training of the embeddings; these previous approaches are inherently transductive and do not naturally generalize to unseen nodes. Here we present GraphSAGE, a general, inductive framework that leverages node feature information (e.g., text attributes) to efficiently generate node embeddings for previously unseen data. Instead of training individual embeddings for each node, we learn a function that generates embeddings by sampling and aggregating features from a node's local neighborhood. Our algorithm outperforms strong baselines on three inductive node-classification benchmarks: we classify the category of unseen nodes in evolving information graphs based on citation and Reddit post data, and we show that our algorithm generalizes to completely unseen graphs using a multi-graph dataset of protein-protein interactions.",
        "title": "Inductive Representation Learning on Large Graphs",
        "local_pdf_path": "downloaded_papers\\Inductive Representation Learning on Large Graphs.pdf"
    },
    {
        "index": 53,
        "raw": "Zhen Han, Yue Feng, and Mingming Sun. 2023. A Graph-Guided Reasoning Approach for Open-ended Commonsense Question Answering. arXiv:2303.10395 [cs.CL] https://arxiv.org/abs/2303.10395",
        "doi": null,
        "year": "2023",
        "has_doi": false,
        "title_from_ref": "A Graph-Guided Reasoning Approach for Open-ended Commonsense Question Answering",
        "arxiv_block": "arXiv:2303.10395 [cs.CL]",
        "pdf_url": "https://arxiv.org/pdf/2303.10395.pdf",
        "source": "arxiv_direct",
        "abstract": null,
        "title": "A Graph-Guided Reasoning Approach for Open-ended Commonsense Question Answering",
        "local_pdf_path": "downloaded_papers\\A Graph-Guided Reasoning Approach for Open-ended Commonsense Question Answering.pdf"
    },
    {
        "index": 54,
        "raw": "Gaole He, Yunshi Lan, Jing Jiang, Wayne Xin Zhao, and Ji-Rong Wen. 2021. Improving Multi-hop Knowledge Base Question Answering by Learning Intermediate Supervision Signals. In WSDM ’21, The Fourteenth ACM International Conference on Web Search and Data Mining, Virtual Event, Israel, March 8-12, 2021 . 553–561. J. ACM, Vol. 37, No. 4, Article 111. Publication date: September 2024. 111:34 Peng et al.",
        "doi": null,
        "year": "2021",
        "has_doi": false,
        "title_from_ref": "Improving Multi-hop Knowledge Base Question Answering by Learning Intermediate Supervision Signals",
        "arxiv_block": null,
        "pdf_url": "https://arxiv.org/pdf/2101.03737v2.pdf",
        "source": "arxiv_api",
        "abstract": "Multi-hop Knowledge Base Question Answering (KBQA) aims to find the answer entities that are multiple hops away in the Knowledge Base (KB) from the entities in the question. A major challenge is the lack of supervision signals at intermediate steps. Therefore, multi-hop KBQA algorithms can only receive the feedback from the final answer, which makes the learning unstable or ineffective.\n  To address this challenge, we propose a novel teacher-student approach for the multi-hop KBQA task. In our approach, the student network aims to find the correct answer to the query, while the teacher network tries to learn intermediate supervision signals for improving the reasoning capacity of the student network. The major novelty lies in the design of the teacher network, where we utilize both forward and backward reasoning to enhance the learning of intermediate entity distributions. By considering bidirectional reasoning, the teacher network can produce more reliable intermediate supervision signals, which can alleviate the issue of spurious reasoning. Extensive experiments on three benchmark datasets have demonstrated the effectiveness of our approach on the KBQA task. The code to reproduce our analysis is available at https://github.com/RichardHGL/WSDM2021_NSM.",
        "title": "Improving Multi-hop Knowledge Base Question Answering by Learning Intermediate Supervision Signals",
        "local_pdf_path": "downloaded_papers\\Improving Multi-hop Knowledge Base Question Answering by Learning Intermediate Supervision Signals.pdf"
    },
    {
        "index": 55,
        "raw": "Xiaoxin He, Yijun Tian, Yifei Sun, Nitesh V. Chawla, Thomas Laurent, Yann LeCun, Xavier Bresson, and Bryan Hooi. 2024. G-Retriever: Retrieval-Augmented Generation for Textual Graph Understanding and Question Answering. arXiv:2402.07630 [cs.LG] https://arxiv.org/abs/2402.07630",
        "doi": null,
        "year": "2024",
        "has_doi": false,
        "title_from_ref": "G-Retriever: Retrieval-Augmented Generation for Textual Graph Understanding and Question Answering",
        "arxiv_block": "arXiv:2402.07630 [cs.LG]",
        "pdf_url": "https://arxiv.org/pdf/2402.07630.pdf",
        "source": "arxiv_direct",
        "abstract": null,
        "title": "G-Retriever: Retrieval-Augmented Generation for Textual Graph Understanding and Question Answering",
        "local_pdf_path": "downloaded_papers\\G-Retriever Retrieval-Augmented Generation for Textual Graph Understanding and Question Answering.pdf"
    },
    {
        "index": 56,
        "raw": "Michael Himsolt. 1996. GML: Graph Modelling Language. University of Passau (1996).",
        "doi": "10.1007/978-3-319-17885-1_480",
        "year": "1996",
        "has_doi": false,
        "title_from_ref": "GML: Graph Modelling Language",
        "arxiv_block": null,
        "pdf_url": "https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0143111&type=printable",
        "source": "openalex",
        "abstract": null,
        "title": "GML: Graph Modelling Language",
        "local_pdf_path": "downloaded_papers\\GML Graph Modelling Language.pdf"
    },
    {
        "index": 57,
        "raw": "Johannes Hoffart, Mohamed Amir Yosef, Ilaria Bordino, Hagen Fürstenau, Manfred Pinkal, Marc Spaniol, Bilyana Taneva, Stefan Thater, and Gerhard Weikum. 2011. Robust Disambiguation of Named Entities in Text. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, EMNLP 2011, 27-31 July 2011, John McIntyre Conference Centre, Edinburgh, UK, A meeting of SIGDAT, a Special Interest Group of the ACL . 782–792.",
        "doi": "10.1016/j.softx.2023.101480",
        "year": "2011",
        "has_doi": false,
        "title_from_ref": "Robust Disambiguation of Named Entities in Text",
        "arxiv_block": null,
        "pdf_url": null,
        "source": "openalex",
        "abstract": null,
        "title": "Robust Disambiguation of Named Entities in Text",
        "local_pdf_path": null
    },
    {
        "index": 58,
        "raw": "Yuntong Hu, Zhihan Lei, Zheng Zhang, Bo Pan, Chen Ling, and Liang Zhao. 2024. GRAG: Graph Retrieval-Augmented Generation. arXiv:2405.16506 [cs.LG] https://arxiv.org/abs/2405.16506",
        "doi": null,
        "year": "2024",
        "has_doi": false,
        "title_from_ref": "GRAG: Graph Retrieval-Augmented Generation",
        "arxiv_block": "arXiv:2405.16506 [cs.LG]",
        "pdf_url": "https://arxiv.org/pdf/2405.16506.pdf",
        "source": "arxiv_direct",
        "abstract": null,
        "title": "GRAG: Graph Retrieval-Augmented Generation",
        "local_pdf_path": "downloaded_papers\\GRAG Graph Retrieval-Augmented Generation.pdf"
    },
    {
        "index": 59,
        "raw": "Yucheng Hu and Yuxing Lu. 2024. RAG and RAU: A Survey on Retrieval-Augmented Language Model in Natural Language Processing. arXiv:2404.19543 [cs.CL] https://arxiv.org/abs/2404.19543",
        "doi": null,
        "year": "2024",
        "has_doi": false,
        "title_from_ref": "RAG and RAU: A Survey on Retrieval-Augmented Language Model in Natural Language Processing",
        "arxiv_block": "arXiv:2404.19543 [cs.CL]",
        "pdf_url": "https://arxiv.org/pdf/2404.19543.pdf",
        "source": "arxiv_direct",
        "abstract": null,
        "title": "RAG and RAU: A Survey on Retrieval-Augmented Language Model in Natural Language Processing",
        "local_pdf_path": "downloaded_papers\\RAG and RAU A Survey on Retrieval-Augmented Language Model in Natural Language Processing.pdf"
    },
    {
        "index": 60,
        "raw": "Ziniu Hu, Yichong Xu, Wenhao Yu, Shuohang Wang, Ziyi Yang, Chenguang Zhu, Kai-Wei Chang, and Yizhou Sun. 2022. Empowering Language Models with Knowledge Graph Reasoning for Open-Domain Question Answering. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022 . 9562–9581.",
        "doi": "10.18653/v1/2022.emnlp-main.650",
        "year": "2022",
        "has_doi": false,
        "title_from_ref": "Empowering Language Models with Knowledge Graph Reasoning for Open-Domain Question Answering",
        "arxiv_block": null,
        "pdf_url": "https://aclanthology.org/2022.emnlp-main.650.pdf",
        "source": "openalex",
        "abstract": null,
        "title": "Empowering Language Models with Knowledge Graph Reasoning for Open-Domain Question Answering",
        "local_pdf_path": "downloaded_papers\\Empowering Language Models with Knowledge Graph Reasoning for Open-Domain Question Answering.pdf"
    },
    {
        "index": 61,
        "raw": "Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, and Ting Liu. 2023. A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions. arXiv:2311.05232 [cs.CL] https://arxiv.org/abs/2311.05232",
        "doi": null,
        "year": "2023",
        "has_doi": false,
        "title_from_ref": "A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions",
        "arxiv_block": "arXiv:2311.05232 [cs.CL]",
        "pdf_url": "https://arxiv.org/pdf/2311.05232.pdf",
        "source": "arxiv_direct",
        "abstract": null,
        "title": "A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions",
        "local_pdf_path": "downloaded_papers\\A Survey on Hallucination in Large Language Models Principles, Taxonomy, Challenges, and Open Questi.pdf"
    },
    {
        "index": 62,
        "raw": "Yizheng Huang and Jimmy Huang. 2024. A Survey on Retrieval-Augmented Text Generation for Large Language Models. arXiv:2404.10981 [cs.IR] https://arxiv.org/abs/2404.10981",
        "doi": null,
        "year": "2024",
        "has_doi": false,
        "title_from_ref": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "arxiv_block": "arXiv:2404.10981 [cs.IR]",
        "pdf_url": "https://arxiv.org/pdf/2404.10981.pdf",
        "source": "arxiv_direct",
        "abstract": null,
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "local_pdf_path": "downloaded_papers\\A Survey on Retrieval-Augmented Text Generation for Large Language Models.pdf"
    },
    {
        "index": 63,
        "raw": "Yongfeng Huang, Yanyang Li, Yichong Xu, Lin Zhang, Ruyi Gan, Jiaxing Zhang, and Liwei Wang. 2023. MVP-Tuning: Multi-View Knowledge Retrieval with Prompt Tuning for Commonsense Reasoning. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023. 13417–13432.",
        "doi": "10.18653/v1/2023.acl-long.750",
        "year": "2023",
        "has_doi": false,
        "title_from_ref": "MVP-Tuning: Multi-View Knowledge Retrieval with Prompt Tuning for Commonsense Reasoning",
        "arxiv_block": null,
        "pdf_url": "https://aclanthology.org/2023.acl-long.750.pdf",
        "source": "openalex",
        "abstract": null,
        "title": "MVP-Tuning: Multi-View Knowledge Retrieval with Prompt Tuning for Commonsense Reasoning",
        "local_pdf_path": "downloaded_papers\\MVP-Tuning Multi-View Knowledge Retrieval with Prompt Tuning for Commonsense Reasoning.pdf"
    },
    {
        "index": 64,
        "raw": "Jena D. Hwang, Chandra Bhagavatula, Ronan Le Bras, Jeff Da, Keisuke Sakaguchi, Antoine Bosselut, and Yejin Choi. 2021. (Comet-) Atomic 2020: On Symbolic and Neural Commonsense Knowledge Graphs. In Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event, February 2-9, 2021 . 6384–6392.",
        "doi": "10.1609/aaai.v35i7.16792",
        "year": "2021",
        "has_doi": false,
        "title_from_ref": "(Comet-) Atomic 2020: On Symbolic and Neural Commonsense Knowledge Graphs",
        "arxiv_block": null,
        "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/download/16792/16599",
        "source": "openalex",
        "abstract": null,
        "title": "(Comet-) Atomic 2020: On Symbolic and Neural Commonsense Knowledge Graphs",
        "local_pdf_path": "downloaded_papers\\(Comet-) Atomic 2020 On Symbolic and Neural Commonsense Knowledge Graphs.pdf"
    },
    {
        "index": 65,
        "raw": "Gautier Izacard and Edouard Grave. 2021. Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, EACL 2021, Online, April 19 - 23, 2021 . 874–880.",
        "doi": null,
        "year": "2021",
        "has_doi": false,
        "title_from_ref": "Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering",
        "arxiv_block": null,
        "pdf_url": "https://arxiv.org/pdf/2007.01282v2.pdf",
        "source": "arxiv_api",
        "abstract": "Generative models for open domain question answering have proven to be competitive, without resorting to external knowledge. While promising, this approach requires to use models with billions of parameters, which are expensive to train and query. In this paper, we investigate how much these models can benefit from retrieving text passages, potentially containing evidence. We obtain state-of-the-art results on the Natural Questions and TriviaQA open benchmarks. Interestingly, we observe that the performance of this method significantly improves when increasing the number of retrieved passages. This is evidence that generative models are good at aggregating and combining evidence from multiple passages.",
        "title": "Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering",
        "local_pdf_path": "downloaded_papers\\Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering.pdf"
    },
    {
        "index": 66,
        "raw": "Omid Jafari, Preeti Maurya, Parth Nagarkar, Khandker Mushfiqul Islam, and Chidambaram Crushev. 2021. A Survey on Locality Sensitive Hashing Algorithms and their Applications. arXiv:2102.08942 [cs.DB] https://arxiv.org/abs/ 2102.08942",
        "doi": null,
        "year": "2021",
        "has_doi": false,
        "title_from_ref": "A Survey on Locality Sensitive Hashing Algorithms and their Applications",
        "arxiv_block": "arXiv:2102.08942 [cs.DB]",
        "pdf_url": "https://arxiv.org/pdf/2102.08942.pdf",
        "source": "arxiv_direct",
        "abstract": null,
        "title": "A Survey on Locality Sensitive Hashing Algorithms and their Applications",
        "local_pdf_path": "downloaded_papers\\A Survey on Locality Sensitive Hashing Algorithms and their Applications.pdf"
    },
    {
        "index": 67,
        "raw": "Jinhao Jiang, Kun Zhou, Zican Dong, Keming Ye, Xin Zhao, and Ji-Rong Wen. 2023. StructGPT: A General Framework for Large Language Model to Reason over Structured Data. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023 . 9237–9251.",
        "doi": null,
        "year": "2023",
        "has_doi": false,
        "title_from_ref": "StructGPT: A General Framework for Large Language Model to Reason over Structured Data",
        "arxiv_block": null,
        "pdf_url": "https://arxiv.org/pdf/2305.09645v2.pdf",
        "source": "arxiv_api",
        "abstract": "In this paper, we study how to improve the zero-shot reasoning ability of large language models~(LLMs) over structured data in a unified way. Inspired by the study on tool augmentation for LLMs, we develop an \\emph{Iterative Reading-then-Reasoning~(IRR)} approach for solving question answering tasks based on structured data, called \\textbf{StructGPT}. In our approach, we construct the specialized function to collect relevant evidence from structured data (\\ie \\emph{reading}), and let LLMs concentrate the reasoning task based on the collected information (\\ie \\emph{reasoning}). Specially, we propose an \\emph{invoking-linearization-generation} procedure to support LLMs in reasoning on the structured data with the help of the external interfaces. By iterating this procedures with provided interfaces, our approach can gradually approach the target answer to a given query. Extensive experiments conducted on three types of structured data demonstrate the effectiveness of our approach, which can significantly boost the performance of ChatGPT and achieve comparable performance against the full-data supervised-tuning baselines. Our codes and data are publicly available at~\\url{https://github.com/RUCAIBox/StructGPT}.",
        "title": "StructGPT: A General Framework for Large Language Model to Reason over Structured Data",
        "local_pdf_path": "downloaded_papers\\StructGPT A General Framework for Large Language Model to Reason over Structured Data.pdf"
    },
    {
        "index": 68,
        "raw": "Jinhao Jiang, Kun Zhou, Ji-Rong Wen, and Wayne Xin Zhao. 2022. $Great Truths are Always Simple: $ A Rather Simple Knowledge Encoder for Enhancing the Commonsense Reasoning Capacity of Pre-Trained Models. In Findings of the Association for Computational Linguistics: NAACL 2022, Seattle, WA, United States, July 10-15, 2022 . 1730–1741.",
        "doi": "10.18653/v1/2022.findings-naacl.131",
        "year": "2022",
        "has_doi": false,
        "title_from_ref": "$Great Truths are Always Simple: $ A Rather Simple Knowledge Encoder for Enhancing the Commonsense Reasoning Capacity of Pre-Trained Models",
        "arxiv_block": null,
        "pdf_url": "https://arxiv.org/pdf/2205.01841",
        "source": "openalex",
        "abstract": null,
        "title": "$Great Truths are Always Simple: $ A Rather Simple Knowledge Encoder for Enhancing the Commonsense Reasoning Capacity of Pre-Trained Models",
        "local_pdf_path": "downloaded_papers\\$Great Truths are Always Simple $ A Rather Simple Knowledge Encoder for Enhancing the Commonsense Re.pdf"
    },
    {
        "index": 69,
        "raw": "Jinhao Jiang, Kun Zhou, Wayne Xin Zhao, Yang Song, Chen Zhu, Hengshu Zhu, and Ji-Rong Wen. 2024. KG-Agent: An Efficient Autonomous Agent Framework for Complex Reasoning over Knowledge Graph. arXiv:2402.11163 [cs.CL] https://arxiv.org/abs/2402.11163",
        "doi": null,
        "year": "2024",
        "has_doi": false,
        "title_from_ref": "KG-Agent: An Efficient Autonomous Agent Framework for Complex Reasoning over Knowledge Graph",
        "arxiv_block": "arXiv:2402.11163 [cs.CL]",
        "pdf_url": "https://arxiv.org/pdf/2402.11163.pdf",
        "source": "arxiv_direct",
        "abstract": null,
        "title": "KG-Agent: An Efficient Autonomous Agent Framework for Complex Reasoning over Knowledge Graph",
        "local_pdf_path": "downloaded_papers\\KG-Agent An Efficient Autonomous Agent Framework for Complex Reasoning over Knowledge Graph.pdf"
    },
    {
        "index": 70,
        "raw": "Jinhao Jiang, Kun Zhou, Xin Zhao, and Ji-Rong Wen. 2023. UniKGQA: Unified Retrieval and Reasoning for Solving Multi-hop Question Answering Over Knowledge Graph. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023 .",
        "doi": "10.23919/cje.2023.00.044",
        "year": "2023",
        "has_doi": false,
        "title_from_ref": "UniKGQA: Unified Retrieval and Reasoning for Solving Multi-hop Question Answering Over Knowledge Graph",
        "arxiv_block": null,
        "pdf_url": "https://arxiv.org/pdf/2212.00959",
        "source": "openalex",
        "abstract": null,
        "title": "UniKGQA: Unified Retrieval and Reasoning for Solving Multi-hop Question Answering Over Knowledge Graph",
        "local_pdf_path": "downloaded_papers\\UniKGQA Unified Retrieval and Reasoning for Solving Multi-hop Question Answering Over Knowledge Grap.pdf"
    },
    {
        "index": 71,
        "raw": "Jinhao Jiang, Kun Zhou, Xin Zhao, and Ji-Rong Wen. 2023. UniKGQA: Unified Retrieval and Reasoning for Solving Multi-hop Question Answering Over Knowledge Graph. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023 .",
        "doi": "10.23919/cje.2023.00.044",
        "year": "2023",
        "has_doi": false,
        "title_from_ref": "UniKGQA: Unified Retrieval and Reasoning for Solving Multi-hop Question Answering Over Knowledge Graph",
        "arxiv_block": null,
        "pdf_url": "http://arxiv.org/pdf/2212.00959",
        "source": "semantic_scholar",
        "abstract": "Multi-hop Question Answering over Knowledge Graph~(KGQA) aims to find the answer entities that are multiple hops away from the topic entities mentioned in a natural language question on a large-scale Knowledge Graph (KG). To cope with the vast search space, existing work usually adopts a two-stage approach: it first retrieves a relatively small subgraph related to the question and then performs the reasoning on the subgraph to find the answer entities accurately. Although these two stages are highly related, previous work employs very different technical solutions for developing the retrieval and reasoning models, neglecting their relatedness in task essence. In this paper, we propose UniKGQA, a novel approach for multi-hop KGQA task, by unifying retrieval and reasoning in both model architecture and parameter learning. For model architecture, UniKGQA consists of a semantic matching module based on a pre-trained language model~(PLM) for question-relation semantic matching, and a matching information propagation module to propagate the matching information along the directed edges on KGs. For parameter learning, we design a shared pre-training task based on question-relation matching for both retrieval and reasoning models, and then propose retrieval- and reasoning-oriented fine-tuning strategies. Compared with previous studies, our approach is more unified, tightly relating the retrieval and reasoning stages. Extensive experiments on three benchmark datasets have demonstrated the effectiveness of our method on the multi-hop KGQA task. Our codes and data are publicly available at~\\url{https://github.com/RUCAIBox/UniKGQA}.",
        "title": "UniKGQA: Unified Retrieval and Reasoning for Solving Multi-hop Question Answering Over Knowledge Graph",
        "local_pdf_path": "downloaded_papers\\UniKGQA Unified Retrieval and Reasoning for Solving Multi-hop Question Answering Over Knowledge Grap.pdf"
    },
    {
        "index": 72,
        "raw": "Kelvin Jiang, Dekun Wu, and Hui Jiang. 2019. FreebaseQA: A New Factoid QA Data Set Matching Trivia-Style Question-Answer Pairs with Freebase. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers) . 318–323.",
        "doi": "10.18653/v1/d13-1160",
        "year": "2019",
        "has_doi": false,
        "title_from_ref": "FreebaseQA: A New Factoid QA Data Set Matching Trivia-Style Question-Answer Pairs with Freebase",
        "arxiv_block": null,
        "pdf_url": "https://www.researchsquare.com/article/rs-1553541/latest.pdf",
        "source": "openalex",
        "abstract": null,
        "title": "FreebaseQA: A New Factoid QA Data Set Matching Trivia-Style Question-Answer Pairs with Freebase",
        "local_pdf_path": "downloaded_papers\\FreebaseQA A New Factoid QA Data Set Matching Trivia-Style Question-Answer Pairs with Freebase.pdf"
    },
    {
        "index": 73,
        "raw": "Xinke Jiang, Ruizhe Zhang, Yongxin Xu, Rihong Qiu, Yue Fang, Zhiyuan Wang, Jinyi Tang, Hongxin Ding, Xu Chu, Junfeng Zhao, and Yasha Wang. 2024. HyKGE: A Hypothesis Knowledge Graph Enhanced Framework for Accurate J. ACM, Vol. 37, No. 4, Article 111. Publication date: September 2024. Graph Retrieval-Augmented Generation: A Survey 111:35 and Reliable Medical LLMs Responses. arXiv:2312.15883 [cs.CL] https://arxiv.org/abs/2312.15883",
        "doi": null,
        "year": "2024",
        "has_doi": false,
        "title_from_ref": "HyKGE: A Hypothesis Knowledge Graph Enhanced Framework for Accurate J",
        "arxiv_block": "arXiv:2312.15883 [cs.CL]",
        "pdf_url": "https://arxiv.org/pdf/2312.15883.pdf",
        "source": "arxiv_direct",
        "abstract": null,
        "title": "HyKGE: A Hypothesis Knowledge Graph Enhanced Framework for Accurate J",
        "local_pdf_path": "downloaded_papers\\HyKGE A Hypothesis Knowledge Graph Enhanced Framework for Accurate J.pdf"
    },
    {
        "index": 74,
        "raw": "Bowen Jin, Gang Liu, Chi Han, Meng Jiang, Heng Ji, and Jiawei Han. 2024. Large Language Models on Graphs: A Comprehensive Survey. arXiv:2312.02783 [cs.CL] https://arxiv.org/abs/2312.02783",
        "doi": null,
        "year": "2024",
        "has_doi": false,
        "title_from_ref": "Large Language Models on Graphs: A Comprehensive Survey",
        "arxiv_block": "arXiv:2312.02783 [cs.CL]",
        "pdf_url": "https://arxiv.org/pdf/2312.02783.pdf",
        "source": "arxiv_direct",
        "abstract": null,
        "title": "Large Language Models on Graphs: A Comprehensive Survey",
        "local_pdf_path": "downloaded_papers\\Large Language Models on Graphs A Comprehensive Survey.pdf"
    },
    {
        "index": 75,
        "raw": "Bowen Jin, Chulin Xie, Jiawei Zhang, Kashob Kumar Roy, Yu Zhang, Zheng Li, Ruirui Li, Xianfeng Tang, Suhang Wang, Yu Meng, and Jiawei Han. 2024. Graph Chain-of-Thought: Augmenting Large Language Models by Reasoning on Graphs. arXiv:2404.07103 [cs.CL] https://arxiv.org/abs/2404.07103",
        "doi": null,
        "year": "2024",
        "has_doi": false,
        "title_from_ref": "Graph Chain-of-Thought: Augmenting Large Language Models by Reasoning on Graphs",
        "arxiv_block": "arXiv:2404.07103 [cs.CL]",
        "pdf_url": "https://arxiv.org/pdf/2404.07103.pdf",
        "source": "arxiv_direct",
        "abstract": null,
        "title": "Graph Chain-of-Thought: Augmenting Large Language Models by Reasoning on Graphs",
        "local_pdf_path": "downloaded_papers\\Graph Chain-of-Thought Augmenting Large Language Models by Reasoning on Graphs.pdf"
    },
    {
        "index": 76,
        "raw": "Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter Szolovits. 2020. What Disease does this Patient Have? A Large-scale Open Domain Question Answering Dataset from Medical Exams. arXiv:2009.13081 [cs.CL] https://arxiv.org/abs/2009.13081",
        "doi": null,
        "year": "2020",
        "has_doi": false,
        "title_from_ref": "What Disease does this Patient Have? A Large-scale Open Domain Question Answering Dataset from Medical Exams",
        "arxiv_block": "arXiv:2009.13081 [cs.CL]",
        "pdf_url": "https://arxiv.org/pdf/2009.13081.pdf",
        "source": "arxiv_direct",
        "abstract": null,
        "title": "What Disease does this Patient Have? A Large-scale Open Domain Question Answering Dataset from Medical Exams",
        "local_pdf_path": "downloaded_papers\\What Disease does this Patient Have A Large-scale Open Domain Question Answering Dataset from Medica.pdf"
    },
    {
        "index": 77,
        "raw": "Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. 2017. TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017, Vancouver, Canada, July 30 - August 4, Volume 1: Long Papers . 1601–1611.",
        "doi": null,
        "year": "2017",
        "has_doi": false,
        "title_from_ref": "TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension",
        "arxiv_block": null,
        "pdf_url": "https://arxiv.org/pdf/1705.03551v2.pdf",
        "source": "arxiv_api",
        "abstract": "We present TriviaQA, a challenging reading comprehension dataset containing over 650K question-answer-evidence triples. TriviaQA includes 95K question-answer pairs authored by trivia enthusiasts and independently gathered evidence documents, six per question on average, that provide high quality distant supervision for answering the questions. We show that, in comparison to other recently introduced large-scale datasets, TriviaQA (1) has relatively complex, compositional questions, (2) has considerable syntactic and lexical variability between questions and corresponding answer-evidence sentences, and (3) requires more cross sentence reasoning to find answers. We also present two baseline algorithms: a feature-based classifier and a state-of-the-art neural network, that performs well on SQuAD reading comprehension. Neither approach comes close to human performance (23% and 40% vs. 80%), suggesting that TriviaQA is a challenging testbed that is worth significant future study. Data and code available at -- http://nlp.cs.washington.edu/triviaqa/",
        "title": "TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension",
        "local_pdf_path": "downloaded_papers\\TriviaQA A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension.pdf"
    },
    {
        "index": 78,
        "raw": "Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick S. H. Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wentau Yih. 2020. Dense Passage Retrieval for Open-Domain Question Answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020 . 6769–6781.",
        "doi": null,
        "year": "2020",
        "has_doi": false,
        "title_from_ref": "Dense Passage Retrieval for Open-Domain Question Answering",
        "arxiv_block": null,
        "pdf_url": "https://arxiv.org/pdf/2004.04906v3.pdf",
        "source": "arxiv_api",
        "abstract": "Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system largely by 9%-19% absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks.",
        "title": "Dense Passage Retrieval for Open-Domain Question Answering",
        "local_pdf_path": "downloaded_papers\\Dense Passage Retrieval for Open-Domain Question Answering.pdf"
    },
    {
        "index": 79,
        "raw": "Sohum Kashyap et al. 2024. Knowledge Graph Assisted Large Language Models. (2024).",
        "doi": null,
        "year": "2024",
        "has_doi": false,
        "title_from_ref": "Knowledge Graph Assisted Large Language Models",
        "arxiv_block": null,
        "pdf_url": "https://arxiv.org/pdf/2406.02962v1.pdf",
        "source": "arxiv_api",
        "abstract": "Even for a conservative estimate, 80% of enterprise data reside in unstructured files, stored in data lakes that accommodate heterogeneous formats. Classical search engines can no longer meet information seeking needs, especially when the task is to browse and explore for insight formulation. In other words, there are no obvious search keywords to use. Knowledge graphs, due to their natural visual appeals that reduce the human cognitive load, become the winning candidate for heterogeneous data integration and knowledge representation.\n  In this paper, we introduce Docs2KG, a novel framework designed to extract multimodal information from diverse and heterogeneous unstructured documents, including emails, web pages, PDF files, and Excel files. Dynamically generates a unified knowledge graph that represents the extracted key information, Docs2KG enables efficient querying and exploration of document data lakes. Unlike existing approaches that focus on domain-specific data sources or pre-designed schemas, Docs2KG offers a flexible and extensible solution that can adapt to various document structures and content types. The proposed framework unifies data processing supporting a multitude of downstream tasks with improved domain interpretability. Docs2KG is publicly accessible at https://docs2kg.ai4wa.com, and a demonstration video is available at https://docs2kg.ai4wa.com/Video.",
        "title": "Knowledge Graph Assisted Large Language Models",
        "local_pdf_path": "downloaded_papers\\Knowledge Graph Assisted Large Language Models.pdf"
    },
    {
        "index": 80,
        "raw": "Jiho Kim, Yeonsu Kwon, Yohan Jo, and Edward Choi. 2023. KG-GPT: A General Framework for Reasoning on Knowledge Graphs Using Large Language Models. In Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023 . 9410–9421.",
        "doi": null,
        "year": "2023",
        "has_doi": false,
        "title_from_ref": "KG-GPT: A General Framework for Reasoning on Knowledge Graphs Using Large Language Models",
        "arxiv_block": null,
        "pdf_url": "https://arxiv.org/pdf/2310.11220v1.pdf",
        "source": "arxiv_api",
        "abstract": "While large language models (LLMs) have made considerable advancements in understanding and generating unstructured text, their application in structured data remains underexplored. Particularly, using LLMs for complex reasoning tasks on knowledge graphs (KGs) remains largely untouched. To address this, we propose KG-GPT, a multi-purpose framework leveraging LLMs for tasks employing KGs. KG-GPT comprises three steps: Sentence Segmentation, Graph Retrieval, and Inference, each aimed at partitioning sentences, retrieving relevant graph components, and deriving logical conclusions, respectively. We evaluate KG-GPT using KG-based fact verification and KGQA benchmarks, with the model showing competitive and robust performance, even outperforming several fully-supervised models. Our work, therefore, marks a significant step in unifying structured and unstructured data processing within the realm of LLMs.",
        "title": "KG-GPT: A General Framework for Reasoning on Knowledge Graphs Using Large Language Models",
        "local_pdf_path": "downloaded_papers\\KG-GPT A General Framework for Reasoning on Knowledge Graphs Using Large Language Models.pdf"
    },
    {
        "index": 81,
        "raw": "Jaewoong Kim and Moohong Min. 2024. From RAG to QA-RAG: Integrating Generative AI for Pharmaceutical Regulatory Compliance Process. arXiv:2402.01717 [cs.CL] https://arxiv.org/abs/2402.01717",
        "doi": null,
        "year": "2024",
        "has_doi": false,
        "title_from_ref": "From RAG to QA-RAG: Integrating Generative AI for Pharmaceutical Regulatory Compliance Process",
        "arxiv_block": "arXiv:2402.01717 [cs.CL]",
        "pdf_url": "https://arxiv.org/pdf/2402.01717.pdf",
        "source": "arxiv_direct",
        "abstract": null,
        "title": "From RAG to QA-RAG: Integrating Generative AI for Pharmaceutical Regulatory Compliance Process",
        "local_pdf_path": "downloaded_papers\\From RAG to QA-RAG Integrating Generative AI for Pharmaceutical Regulatory Compliance Process.pdf"
    },
    {
        "index": 82,
        "raw": "Jiho Kim, Sungjin Park, Yeonsu Kwon, Yohan Jo, James Thorne, and Edward Choi. 2023. FactKG: Fact Verification via Reasoning on Knowledge Graphs. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023 . 16190–16206.",
        "doi": null,
        "year": "2023",
        "has_doi": false,
        "title_from_ref": "FactKG: Fact Verification via Reasoning on Knowledge Graphs",
        "arxiv_block": null,
        "pdf_url": "https://arxiv.org/pdf/2305.06590v2.pdf",
        "source": "arxiv_api",
        "abstract": "In real world applications, knowledge graphs (KG) are widely used in various domains (e.g. medical applications and dialogue agents). However, for fact verification, KGs have not been adequately utilized as a knowledge source. KGs can be a valuable knowledge source in fact verification due to their reliability and broad applicability. A KG consists of nodes and edges which makes it clear how concepts are linked together, allowing machines to reason over chains of topics. However, there are many challenges in understanding how these machine-readable concepts map to information in text. To enable the community to better use KGs, we introduce a new dataset, FactKG: Fact Verification via Reasoning on Knowledge Graphs. It consists of 108k natural language claims with five types of reasoning: One-hop, Conjunction, Existence, Multi-hop, and Negation. Furthermore, FactKG contains various linguistic patterns, including colloquial style claims as well as written style claims to increase practicality. Lastly, we develop a baseline approach and analyze FactKG over these reasoning types. We believe FactKG can advance both reliability and practicality in KG-based fact verification.",
        "title": "FactKG: Fact Verification via Reasoning on Knowledge Graphs",
        "local_pdf_path": "downloaded_papers\\FactKG Fact Verification via Reasoning on Knowledge Graphs.pdf"
    },
    {
        "index": 83,
        "raw": "Thomas N. Kipf and Max Welling. 2017. Semi-Supervised Classification with Graph Convolutional Networks. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings .",
        "doi": null,
        "year": "2017",
        "has_doi": false,
        "title_from_ref": "Semi-Supervised Classification with Graph Convolutional Networks",
        "arxiv_block": null,
        "pdf_url": "https://arxiv.org/pdf/1609.02907v4.pdf",
        "source": "arxiv_api",
        "abstract": "We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.",
        "title": "Semi-Supervised Classification with Graph Convolutional Networks",
        "local_pdf_path": "downloaded_papers\\Semi-Supervised Classification with Graph Convolutional Networks.pdf"
    },
    {
        "index": 84,
        "raw": "Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur P. Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural Questions: a Benchmark for Question Answering Research. Trans. Assoc. Comput. Linguistics 7 (2019), 452–466.",
        "doi": "10.1162/tacl_a_00276",
        "year": "2019",
        "has_doi": false,
        "title_from_ref": "Natural Questions: a Benchmark for Question Answering Research",
        "arxiv_block": null,
        "pdf_url": "https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00276/1923288/tacl_a_00276.pdf",
        "source": "semantic_scholar",
        "abstract": "We present the Natural Questions corpus, a question answering data set. Questions consist of real anonymized, aggregated queries issued to the Google search engine. An annotator is presented with a question along with a Wikipedia page from the top 5 search results, and annotates a long answer (typically a paragraph) and a short answer (one or more entities) if present on the page, or marks null if no long/short answer is present. The public release consists of 307,373 training examples with single annotations; 7,830 examples with 5-way annotations for development data; and a further 7,842 examples with 5-way annotated sequestered as test data. We present experiments validating quality of the data. We also describe analysis of 25-way annotations on 302 examples, giving insights into human variability on the annotation task. We introduce robust metrics for the purposes of evaluating question answering systems; demonstrate high human upper bounds on these metrics; and establish baseline results using competitive methods drawn from related literature.",
        "title": "Natural Questions: a Benchmark for Question Answering Research",
        "local_pdf_path": null
    },
    {
        "index": 85,
        "raw": "Yunshi Lan, Gaole He, Jinhao Jiang, Jing Jiang, Wayne Xin Zhao, and Ji-Rong Wen. 2021. A Survey on Complex Knowledge Base Question Answering: Methods, Challenges and Solutions. In Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI 2021, Virtual Event / Montreal, Canada, 19-27 August 2021 . 4483–4491.",
        "doi": "10.24963/ijcai.2021/611",
        "year": "2021",
        "has_doi": false,
        "title_from_ref": "A Survey on Complex Knowledge Base Question Answering: Methods, Challenges and Solutions",
        "arxiv_block": null,
        "pdf_url": "https://www.ijcai.org/proceedings/2021/0611.pdf",
        "source": "openalex",
        "abstract": null,
        "title": "A Survey on Complex Knowledge Base Question Answering: Methods, Challenges and Solutions",
        "local_pdf_path": "downloaded_papers\\A Survey on Complex Knowledge Base Question Answering Methods, Challenges and Solutions.pdf"
    },
    {
        "index": 86,
        "raw": "Yunshi Lan, Gaole He, Jinhao Jiang, Jing Jiang, Wayne Xin Zhao, and Ji-Rong Wen. 2023. Complex Knowledge Base Question Answering: A Survey. IEEE Trans. Knowl. Data Eng. 35, 11 (2023), 11196–11215.",
        "doi": null,
        "year": "2023",
        "has_doi": false,
        "title_from_ref": "Complex Knowledge Base Question Answering: A Survey",
        "arxiv_block": null,
        "pdf_url": "https://arxiv.org/pdf/2108.06688v5.pdf",
        "source": "arxiv_api",
        "abstract": "Knowledge base question answering (KBQA) aims to answer a question over a knowledge base (KB). Early studies mainly focused on answering simple questions over KBs and achieved great success. However, their performance on complex questions is still far from satisfactory. Therefore, in recent years, researchers propose a large number of novel methods, which looked into the challenges of answering complex questions. In this survey, we review recent advances on KBQA with the focus on solving complex questions, which usually contain multiple subjects, express compound relations, or involve numerical operations. In detail, we begin with introducing the complex KBQA task and relevant background. Then, we describe benchmark datasets for complex KBQA task and introduce the construction process of these datasets. Next, we present two mainstream categories of methods for complex KBQA, namely semantic parsing-based (SP-based) methods and information retrieval-based (IR-based) methods. Specifically, we illustrate their procedures with flow designs and discuss their major differences and similarities. After that, we summarize the challenges that these two categories of methods encounter when answering complex questions, and explicate advanced solutions and techniques used in existing work. Finally, we conclude and discuss several promising directions related to complex KBQA for future research.",
        "title": "Complex Knowledge Base Question Answering: A Survey",
        "local_pdf_path": "downloaded_papers\\Complex Knowledge Base Question Answering A Survey.pdf"
    },
    {
        "index": 87,
        "raw": "Yunshi Lan and Jing Jiang. 2020. Query Graph Generation for Answering Multi-hop Complex Questions from Knowledge Bases. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020 . 969–974.",
        "doi": "10.18653/v1/2020.acl-main.91",
        "year": "2020",
        "has_doi": false,
        "title_from_ref": "Query Graph Generation for Answering Multi-hop Complex Questions from Knowledge Bases",
        "arxiv_block": null,
        "pdf_url": "https://www.aclweb.org/anthology/2020.acl-main.91.pdf",
        "source": "openalex",
        "abstract": null,
        "title": "Query Graph Generation for Answering Multi-hop Complex Questions from Knowledge Bases",
        "local_pdf_path": "downloaded_papers\\Query Graph Generation for Answering Multi-hop Complex Questions from Knowledge Bases.pdf"
    },
    {
        "index": 88,
        "raw": "Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The Power of Scale for Parameter-Efficient Prompt Tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021 . 3045–3059.",
        "doi": null,
        "year": "2021",
        "has_doi": false,
        "title_from_ref": "The Power of Scale for Parameter-Efficient Prompt Tuning",
        "arxiv_block": null,
        "pdf_url": "https://arxiv.org/pdf/2104.08691v2.pdf",
        "source": "arxiv_api",
        "abstract": "In this work, we explore \"prompt tuning\", a simple yet effective mechanism for learning \"soft prompts\" to condition frozen language models to perform specific downstream tasks. Unlike the discrete text prompts used by GPT-3, soft prompts are learned through backpropagation and can be tuned to incorporate signal from any number of labeled examples. Our end-to-end learned approach outperforms GPT-3's \"few-shot\" learning by a large margin. More remarkably, through ablations on model size using T5, we show that prompt tuning becomes more competitive with scale: as models exceed billions of parameters, our method \"closes the gap\" and matches the strong performance of model tuning (where all model weights are tuned). This finding is especially relevant in that large models are costly to share and serve, and the ability to reuse one frozen model for multiple downstream tasks can ease this burden. Our method can be seen as a simplification of the recently proposed \"prefix tuning\" of Li and Liang (2021), and we provide a comparison to this and other similar approaches. Finally, we show that conditioning a frozen model with soft prompts confers benefits in robustness to domain transfer, as compared to full model tuning.",
        "title": "The Power of Scale for Parameter-Efficient Prompt Tuning",
        "local_pdf_path": "downloaded_papers\\The Power of Scale for Parameter-Efficient Prompt Tuning.pdf"
    },
    {
        "index": 89,
        "raw": "Dawei Li, Shu Yang, Zhen Tan, Jae Young Baik, Sukwon Yun, Joseph Lee, Aaron Chacko, Bojian Hou, Duy Duong-Tran, Ying Ding, Huan Liu, Li Shen, and Tianlong Chen. 2024. DALK: Dynamic Co-Augmentation of LLMs and KG to answer Alzheimer’s Disease Questions with Scientific Literature. arXiv:2405.04819 [cs.CL] https://arxiv.org/abs/2405.04819",
        "doi": null,
        "year": "2024",
        "has_doi": false,
        "title_from_ref": "DALK: Dynamic Co-Augmentation of LLMs and KG to answer Alzheimer’s Disease Questions with Scientific Literature",
        "arxiv_block": "arXiv:2405.04819 [cs.CL]",
        "pdf_url": "https://arxiv.org/pdf/2405.04819.pdf",
        "source": "arxiv_direct",
        "abstract": null,
        "title": "DALK: Dynamic Co-Augmentation of LLMs and KG to answer Alzheimer’s Disease Questions with Scientific Literature",
        "local_pdf_path": "downloaded_papers\\DALK Dynamic Co-Augmentation of LLMs and KG to answer Alzheimer’s Disease Questions with Scientific .pdf"
    },
    {
        "index": 90,
        "raw": "Shiyang Li, Yifan Gao, Haoming Jiang, Qingyu Yin, Zheng Li, Xifeng Yan, Chao Zhang, and Bing Yin. 2023. Graph Reasoning for Question Answering with Triplet Retrieval. In Findings of the Association for Computational Linguistics: ACL 2023, Toronto, Canada, July 9-14, 2023 . 3366–3375.",
        "doi": null,
        "year": "2023",
        "has_doi": false,
        "title_from_ref": "Graph Reasoning for Question Answering with Triplet Retrieval",
        "arxiv_block": null,
        "pdf_url": "https://arxiv.org/pdf/2305.18742v1.pdf",
        "source": "arxiv_api",
        "abstract": "Answering complex questions often requires reasoning over knowledge graphs (KGs). State-of-the-art methods often utilize entities in questions to retrieve local subgraphs, which are then fed into KG encoder, e.g. graph neural networks (GNNs), to model their local structures and integrated into language models for question answering. However, this paradigm constrains retrieved knowledge in local subgraphs and discards more diverse triplets buried in KGs that are disconnected but useful for question answering. In this paper, we propose a simple yet effective method to first retrieve the most relevant triplets from KGs and then rerank them, which are then concatenated with questions to be fed into language models. Extensive results on both CommonsenseQA and OpenbookQA datasets show that our method can outperform state-of-the-art up to 4.6% absolute accuracy.",
        "title": "Graph Reasoning for Question Answering with Triplet Retrieval",
        "local_pdf_path": "downloaded_papers\\Graph Reasoning for Question Answering with Triplet Retrieval.pdf"
    },
    {
        "index": 91,
        "raw": "Xiang Lisa Li and Percy Liang. 2021. Prefix-Tuning: Optimizing Continuous Prompts for Generation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021 . 4582–4597.",
        "doi": null,
        "year": "2021",
        "has_doi": false,
        "title_from_ref": "Prefix-Tuning: Optimizing Continuous Prompts for Generation",
        "arxiv_block": null,
        "pdf_url": "https://arxiv.org/pdf/2101.00190v1.pdf",
        "source": "arxiv_api",
        "abstract": "Fine-tuning is the de facto way to leverage large pretrained language models to perform downstream tasks. However, it modifies all the language model parameters and therefore necessitates storing a full copy for each task. In this paper, we propose prefix-tuning, a lightweight alternative to fine-tuning for natural language generation tasks, which keeps language model parameters frozen, but optimizes a small continuous task-specific vector (called the prefix). Prefix-tuning draws inspiration from prompting, allowing subsequent tokens to attend to this prefix as if it were \"virtual tokens\". We apply prefix-tuning to GPT-2 for table-to-text generation and to BART for summarization. We find that by learning only 0.1\\% of the parameters, prefix-tuning obtains comparable performance in the full data setting, outperforms fine-tuning in low-data settings, and extrapolates better to examples with topics unseen during training.",
        "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation",
        "local_pdf_path": "downloaded_papers\\Prefix-Tuning Optimizing Continuous Prompts for Generation.pdf"
    },
    {
        "index": 92,
        "raw": "Yuhan Li, Zhixun Li, Peisong Wang, Jia Li, Xiangguo Sun, Hong Cheng, and Jeffrey Xu Yu. 2024. A Survey of Graph Meets Large Language Model: Progress and Future Directions. arXiv:2311.12399 [cs.LG] https://arxiv.org/abs/2311. 12399",
        "doi": null,
        "year": "2024",
        "has_doi": false,
        "title_from_ref": "A Survey of Graph Meets Large Language Model: Progress and Future Directions",
        "arxiv_block": "arXiv:2311.12399 [cs.LG]",
        "pdf_url": "https://arxiv.org/pdf/2311.12399.pdf",
        "source": "arxiv_direct",
        "abstract": null,
        "title": "A Survey of Graph Meets Large Language Model: Progress and Future Directions",
        "local_pdf_path": "downloaded_papers\\A Survey of Graph Meets Large Language Model Progress and Future Directions.pdf"
    },
    {
        "index": 93,
        "raw": "Yinheng Li, Shaofei Wang, Han Ding, and Hang Chen. 2024. Large Language Models in Finance: A Survey. arXiv:2311.10723 [q-fin.GN] https://arxiv.org/abs/2311.10723 J. ACM, Vol. 37, No. 4, Article 111. Publication date: September 2024. 111:36 Peng et al.",
        "doi": null,
        "year": "2024",
        "has_doi": false,
        "title_from_ref": "Large Language Models in Finance: A Survey",
        "arxiv_block": "arXiv:2311.10723 [q-fin.GN]",
        "pdf_url": "https://arxiv.org/pdf/2311.10723.pdf",
        "source": "arxiv_direct",
        "abstract": null,
        "title": "Large Language Models in Finance: A Survey",
        "local_pdf_path": "downloaded_papers\\Large Language Models in Finance A Survey.pdf"
    },
    {
        "index": 94,
        "raw": "Yihao Li, Ru Zhang, and Jianyi Liu. 2024. An Enhanced Prompt-Based LLM Reasoning Scheme via Knowledge Graph-Integrated Collaboration. arXiv:2402.04978 [cs.CL] https://arxiv.org/abs/2402.04978",
        "doi": null,
        "year": "2024",
        "has_doi": false,
        "title_from_ref": "An Enhanced Prompt-Based LLM Reasoning Scheme via Knowledge Graph-Integrated Collaboration",
        "arxiv_block": "arXiv:2402.04978 [cs.CL]",
        "pdf_url": "https://arxiv.org/pdf/2402.04978.pdf",
        "source": "arxiv_direct",
        "abstract": null,
        "title": "An Enhanced Prompt-Based LLM Reasoning Scheme via Knowledge Graph-Integrated Collaboration",
        "local_pdf_path": "downloaded_papers\\An Enhanced Prompt-Based LLM Reasoning Scheme via Knowledge Graph-Integrated Collaboration.pdf"
    },
    {
        "index": 95,
        "raw": "Zhuoyang Li, Liran Deng, Hui Liu, Qiaoqiao Liu, and Junzhao Du. 2024. UniOQA: A Unified Framework for Knowledge Graph Question Answering with Large Language Models. arXiv:2406.02110 [cs.CL] https://arxiv.org/abs/2406.02110",
        "doi": null,
        "year": "2024",
        "has_doi": false,
        "title_from_ref": "UniOQA: A Unified Framework for Knowledge Graph Question Answering with Large Language Models",
        "arxiv_block": "arXiv:2406.02110 [cs.CL]",
        "pdf_url": "https://arxiv.org/pdf/2406.02110.pdf",
        "source": "arxiv_direct",
        "abstract": null,
        "title": "UniOQA: A Unified Framework for Knowledge Graph Question Answering with Large Language Models",
        "local_pdf_path": "downloaded_papers\\UniOQA A Unified Framework for Knowledge Graph Question Answering with Large Language Models.pdf"
    },
    {
        "index": 96,
        "raw": "Zijian Li, Qingyan Guo, Jiawei Shao, Lei Song, Jiang Bian, Jun Zhang, and Rui Wang. 2024. Graph Neural Network Enhanced Retrieval for Question Answering of LLMs. arXiv:2406.06572 [cs.CL] https://arxiv.org/abs/2406.06572",
        "doi": null,
        "year": "2024",
        "has_doi": false,
        "title_from_ref": "Graph Neural Network Enhanced Retrieval for Question Answering of LLMs",
        "arxiv_block": "arXiv:2406.06572 [cs.CL]",
        "pdf_url": "https://arxiv.org/pdf/2406.06572.pdf",
        "source": "arxiv_direct",
        "abstract": null,
        "title": "Graph Neural Network Enhanced Retrieval for Question Answering of LLMs",
        "local_pdf_path": "downloaded_papers\\Graph Neural Network Enhanced Retrieval for Question Answering of LLMs.pdf"
    },
    {
        "index": 97,
        "raw": "Bill Yuchen Lin, Xinyue Chen, Jamin Chen, and Xiang Ren. 2019. KagNet: Knowledge-Aware Graph Networks for Commonsense Reasoning. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019 . 2829–2839.",
        "doi": null,
        "year": "2019",
        "has_doi": false,
        "title_from_ref": "KagNet: Knowledge-Aware Graph Networks for Commonsense Reasoning",
        "arxiv_block": null,
        "pdf_url": "https://arxiv.org/pdf/1909.02151v1.pdf",
        "source": "arxiv_api",
        "abstract": "Commonsense reasoning aims to empower machines with the human ability to make presumptions about ordinary situations in our daily life. In this paper, we propose a textual inference framework for answering commonsense questions, which effectively utilizes external, structured commonsense knowledge graphs to perform explainable inferences. The framework first grounds a question-answer pair from the semantic space to the knowledge-based symbolic space as a schema graph, a related sub-graph of external knowledge graphs. It represents schema graphs with a novel knowledge-aware graph network module named KagNet, and finally scores answers with graph representations. Our model is based on graph convolutional networks and LSTMs, with a hierarchical path-based attention mechanism. The intermediate attention scores make it transparent and interpretable, which thus produce trustworthy inferences. Using ConceptNet as the only external resource for Bert-based models, we achieved state-of-the-art performance on the CommonsenseQA, a large-scale dataset for commonsense reasoning.",
        "title": "KagNet: Knowledge-Aware Graph Networks for Commonsense Reasoning",
        "local_pdf_path": "downloaded_papers\\KagNet Knowledge-Aware Graph Networks for Commonsense Reasoning.pdf"
    },
    {
        "index": 98,
        "raw": "Bill Yuchen Lin, Ziyi Wu, Yichi Yang, Dong-Ho Lee, and Xiang Ren. 2021. RiddleSense: Reasoning about Riddle Questions Featuring Linguistic Creativity and Commonsense Knowledge. In Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021, Online Event, August 1-6, 2021 (Findings of ACL, Vol. ACL/IJCNLP 2021) . 1504–1515.",
        "doi": "10.18653/v1/2021.findings-acl.131",
        "year": "2021",
        "has_doi": false,
        "title_from_ref": "RiddleSense: Reasoning about Riddle Questions Featuring Linguistic Creativity and Commonsense Knowledge",
        "arxiv_block": null,
        "pdf_url": "https://aclanthology.org/2021.findings-acl.131.pdf",
        "source": "openalex",
        "abstract": null,
        "title": "RiddleSense: Reasoning about Riddle Questions Featuring Linguistic Creativity and Commonsense Knowledge",
        "local_pdf_path": "downloaded_papers\\RiddleSense Reasoning about Riddle Questions Featuring Linguistic Creativity and Commonsense Knowled.pdf"
    },
    {
        "index": 99,
        "raw": "Guangyi Liu, Yongqi Zhang, Yong Li, and Quanming Yao. 2024. Explore then Determine: A GNN-LLM Synergy Framework for Reasoning over Knowledge Graph. arXiv:2406.01145 [cs.CL] https://arxiv.org/abs/2406.01145",
        "doi": null,
        "year": "2024",
        "has_doi": false,
        "title_from_ref": "Explore then Determine: A GNN-LLM Synergy Framework for Reasoning over Knowledge Graph",
        "arxiv_block": "arXiv:2406.01145 [cs.CL]",
        "pdf_url": "https://arxiv.org/pdf/2406.01145.pdf",
        "source": "arxiv_direct",
        "abstract": null,
        "title": "Explore then Determine: A GNN-LLM Synergy Framework for Reasoning over Knowledge Graph",
        "local_pdf_path": "downloaded_papers\\Explore then Determine A GNN-LLM Synergy Framework for Reasoning over Knowledge Graph.pdf"
    },
    {
        "index": 100,
        "raw": "H Liu and P Singh. 2004. ConceptNet—a practical commonsense reasoning tool-kit. BT technology journal 22, 4 (2004), 211–226.",
        "doi": "10.1023/b:bttj.0000047600.45421.6d",
        "year": "2004",
        "has_doi": false,
        "title_from_ref": "ConceptNet—a practical commonsense reasoning tool-kit",
        "arxiv_block": null,
        "pdf_url": null,
        "source": "openalex",
        "abstract": null,
        "title": "ConceptNet—a practical commonsense reasoning tool-kit",
        "local_pdf_path": null
    },
    {
        "index": 101,
        "raw": "Haochen Liu, Song Wang, Yaochen Zhu, Yushun Dong, and Jundong Li. 2024. Knowledge Graph-Enhanced Large Language Models via Path Selection. In Findings of the Association for Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting, August 11-16, 2024 . 6311–6321.",
        "doi": null,
        "year": "2024",
        "has_doi": false,
        "title_from_ref": "Knowledge Graph-Enhanced Large Language Models via Path Selection",
        "arxiv_block": null,
        "pdf_url": "https://arxiv.org/pdf/2406.13862v1.pdf",
        "source": "arxiv_api",
        "abstract": "Large Language Models (LLMs) have shown unprecedented performance in various real-world applications. However, they are known to generate factually inaccurate outputs, a.k.a. the hallucination problem. In recent years, incorporating external knowledge extracted from Knowledge Graphs (KGs) has become a promising strategy to improve the factual accuracy of LLM-generated outputs. Nevertheless, most existing explorations rely on LLMs themselves to perform KG knowledge extraction, which is highly inflexible as LLMs can only provide binary judgment on whether a certain knowledge (e.g., a knowledge path in KG) should be used. In addition, LLMs tend to pick only knowledge with direct semantic relationship with the input text, while potentially useful knowledge with indirect semantics can be ignored. In this work, we propose a principled framework KELP with three stages to handle the above problems. Specifically, KELP is able to achieve finer granularity of flexible knowledge extraction by generating scores for knowledge paths with input texts via latent semantic matching. Meanwhile, knowledge paths with indirect semantic relationships with the input text can also be considered via trained encoding between the selected paths in KG and the input text. Experiments on real-world datasets validate the effectiveness of KELP.",
        "title": "Knowledge Graph-Enhanced Large Language Models via Path Selection",
        "local_pdf_path": "downloaded_papers\\Knowledge Graph-Enhanced Large Language Models via Path Selection.pdf"
    },
    {
        "index": 102,
        "raw": "Jiawei Liu, Cheng Yang, Zhiyuan Lu, Junze Chen, Yibo Li, Mengmei Zhang, Ting Bai, Yuan Fang, Lichao Sun, Philip S. Yu, and Chuan Shi. 2024. Towards Graph Foundation Models: A Survey and Beyond. arXiv:2310.11829 [cs.LG] https://arxiv.org/abs/2310.11829",
        "doi": null,
        "year": "2024",
        "has_doi": false,
        "title_from_ref": "Towards Graph Foundation Models: A Survey and Beyond",
        "arxiv_block": "arXiv:2310.11829 [cs.LG]",
        "pdf_url": "https://arxiv.org/pdf/2310.11829.pdf",
        "source": "arxiv_direct",
        "abstract": null,
        "title": "Towards Graph Foundation Models: A Survey and Beyond",
        "local_pdf_path": "downloaded_papers\\Towards Graph Foundation Models A Survey and Beyond.pdf"
    },
    {
        "index": 103,
        "raw": "Lei Liu, Xiaoyan Yang, Junchi Lei, Xiaoyang Liu, Yue Shen, Zhiqiang Zhang, Peng Wei, Jinjie Gu, Zhixuan Chu, Zhan Qin, and Kui Ren. 2024. A Survey on Medical Large Language Models: Technology, Application, Trustworthiness, and Future Directions. arXiv:2406.03712 [cs.CL] https://arxiv.org/abs/2406.03712",
        "doi": null,
        "year": "2024",
        "has_doi": false,
        "title_from_ref": "A Survey on Medical Large Language Models: Technology, Application, Trustworthiness, and Future Directions",
        "arxiv_block": "arXiv:2406.03712 [cs.CL]",
        "pdf_url": "https://arxiv.org/pdf/2406.03712.pdf",
        "source": "arxiv_direct",
        "abstract": null,
        "title": "A Survey on Medical Large Language Models: Technology, Application, Trustworthiness, and Future Directions",
        "local_pdf_path": "downloaded_papers\\A Survey on Medical Large Language Models Technology, Application, Trustworthiness, and Future Direc.pdf"
    },
    {
        "index": 104,
        "raw": "Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2024. Lost in the Middle: How Language Models Use Long Contexts. Trans. Assoc. Comput. Linguistics 12 (2024), 157–173.",
        "doi": null,
        "year": "2024",
        "has_doi": false,
        "title_from_ref": "Lost in the Middle: How Language Models Use Long Contexts",
        "arxiv_block": null,
        "pdf_url": "https://arxiv.org/pdf/2307.03172v3.pdf",
        "source": "arxiv_api",
        "abstract": "While recent language models have the ability to take long contexts as input, relatively little is known about how well they use longer context. We analyze the performance of language models on two tasks that require identifying relevant information in their input contexts: multi-document question answering and key-value retrieval. We find that performance can degrade significantly when changing the position of relevant information, indicating that current language models do not robustly make use of information in long input contexts. In particular, we observe that performance is often highest when relevant information occurs at the beginning or end of the input context, and significantly degrades when models must access relevant information in the middle of long contexts, even for explicitly long-context models. Our analysis provides a better understanding of how language models use their input context and provides new evaluation protocols for future long-context language models.",
        "title": "Lost in the Middle: How Language Models Use Long Contexts",
        "local_pdf_path": "downloaded_papers\\Lost in the Middle How Language Models Use Long Contexts.pdf"
    },
    {
        "index": 105,
        "raw": "Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Tam, Zhengxiao Du, Zhilin Yang, and Jie Tang. 2022. P-Tuning: Prompt Tuning Can Be Comparable to Fine-tuning Across Scales and Tasks. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers) . 61–68.",
        "doi": "10.18653/v1/2022.acl-short.8",
        "year": "2022",
        "has_doi": false,
        "title_from_ref": "P-Tuning: Prompt Tuning Can Be Comparable to Fine-tuning Across Scales and Tasks",
        "arxiv_block": null,
        "pdf_url": "https://aclanthology.org/2022.acl-short.8.pdf",
        "source": "openalex",
        "abstract": null,
        "title": "P-Tuning: Prompt Tuning Can Be Comparable to Fine-tuning Across Scales and Tasks",
        "local_pdf_path": "downloaded_papers\\P-Tuning Prompt Tuning Can Be Comparable to Fine-tuning Across Scales and Tasks.pdf"
    },
    {
        "index": 106,
        "raw": "Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. 2023. GPT Understands, Too. arXiv:2103.10385 [cs.CL] https://arxiv.org/abs/2103.10385",
        "doi": null,
        "year": "2023",
        "has_doi": false,
        "title_from_ref": "GPT Understands, Too",
        "arxiv_block": "arXiv:2103.10385 [cs.CL]",
        "pdf_url": "https://arxiv.org/pdf/2103.10385.pdf",
        "source": "arxiv_direct",
        "abstract": null,
        "title": "GPT Understands, Too",
        "local_pdf_path": "downloaded_papers\\GPT Understands, Too.pdf"
    },
    {
        "index": 107,
        "raw": "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A Robustly Optimized BERT Pretraining Approach. arXiv:1907.11692 [cs.CL] https://arxiv.org/abs/1907.11692",
        "doi": null,
        "year": "2019",
        "has_doi": false,
        "title_from_ref": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
        "arxiv_block": "arXiv:1907.11692 [cs.CL]",
        "pdf_url": "https://arxiv.org/pdf/1907.11692.pdf",
        "source": "arxiv_direct",
        "abstract": null,
        "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
        "local_pdf_path": "downloaded_papers\\RoBERTa A Robustly Optimized BERT Pretraining Approach.pdf"
    },
    {
        "index": 108,
        "raw": "Pei-Chi Lo and Ee-Peng Lim. 2023. Contextual Path Retrieval: A Contextual Entity Relation Embedding-based Approach. ACM Trans. Inf. Syst. 41, 1 (2023), 1:1–1:38.",
        "doi": "10.1145/3502720",
        "year": "2023",
        "has_doi": false,
        "title_from_ref": "Contextual Path Retrieval: A Contextual Entity Relation Embedding-based Approach",
        "arxiv_block": null,
        "pdf_url": null,
        "source": "openalex",
        "abstract": null,
        "title": "Contextual Path Retrieval: A Contextual Entity Relation Embedding-based Approach",
        "local_pdf_path": null
    },
    {
        "index": 109,
        "raw": "Lajanugen Logeswaran, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, Jacob Devlin, and Honglak Lee. 2019. Zero-Shot Entity Linking by Reading Entity Descriptions. In Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers . 3449–3460.",
        "doi": null,
        "year": "2019",
        "has_doi": false,
        "title_from_ref": "Zero-Shot Entity Linking by Reading Entity Descriptions",
        "arxiv_block": null,
        "pdf_url": "https://arxiv.org/pdf/1906.07348v1.pdf",
        "source": "arxiv_api",
        "abstract": "We present the zero-shot entity linking task, where mentions must be linked to unseen entities without in-domain labeled data. The goal is to enable robust transfer to highly specialized domains, and so no metadata or alias tables are assumed. In this setting, entities are only identified by text descriptions, and models must rely strictly on language understanding to resolve the new entities. First, we show that strong reading comprehension models pre-trained on large unlabeled data can be used to generalize to unseen entities. Second, we propose a simple and effective adaptive pre-training strategy, which we term domain-adaptive pre-training (DAP), to address the domain shift problem associated with linking unseen entities in a new domain. We present experiments on a new dataset that we construct for this task and show that DAP improves over strong pre-training baselines, including BERT. The data and code are available at https://github.com/lajanugen/zeshel.",
        "title": "Zero-Shot Entity Linking by Reading Entity Descriptions",
        "local_pdf_path": "downloaded_papers\\Zero-Shot Entity Linking by Reading Entity Descriptions.pdf"
    },
    {
        "index": 110,
        "raw": "Dan Luo, Jiawei Sheng, Hongbo Xu, Lihong Wang, and Bin Wang. 2023. Improving Complex Knowledge Base Question Answering with Relation-Aware Subgraph Retrieval and Reasoning Network. In International Joint Conference on Neural Networks, IJCNN 2023, Gold Coast, Australia, June 18-23, 2023 . 1–8.",
        "doi": "10.1109/ijcnn54540.2023.10191339",
        "year": "2023",
        "has_doi": false,
        "title_from_ref": "Improving Complex Knowledge Base Question Answering with Relation-Aware Subgraph Retrieval and Reasoning Network",
        "arxiv_block": null,
        "pdf_url": null,
        "source": "openalex",
        "abstract": null,
        "title": "Improving Complex Knowledge Base Question Answering with Relation-Aware Subgraph Retrieval and Reasoning Network",
        "local_pdf_path": null
    },
    {
        "index": 111,
        "raw": "Haoran Luo, Haihong E, Zichen Tang, Shiyao Peng, Yikai Guo, Wentai Zhang, Chenghao Ma, Guanting Dong, Meina Song, Wei Lin, Yifan Zhu, and Luu Anh Tuan. 2024. ChatKBQA: A Generate-then-Retrieve Framework for Knowledge Base Question Answering with Fine-tuned Large Language Models. arXiv:2310.08975 [cs.CL] https://arxiv.org/abs/2310.08975",
        "doi": null,
        "year": "2024",
        "has_doi": false,
        "title_from_ref": "ChatKBQA: A Generate-then-Retrieve Framework for Knowledge Base Question Answering with Fine-tuned Large Language Models",
        "arxiv_block": "arXiv:2310.08975 [cs.CL]",
        "pdf_url": "https://arxiv.org/pdf/2310.08975.pdf",
        "source": "arxiv_direct",
        "abstract": null,
        "title": "ChatKBQA: A Generate-then-Retrieve Framework for Knowledge Base Question Answering with Fine-tuned Large Language Models",
        "local_pdf_path": "downloaded_papers\\ChatKBQA A Generate-then-Retrieve Framework for Knowledge Base Question Answering with Fine-tuned La.pdf"
    },
    {
        "index": 112,
        "raw": "Linhao Luo, Yuan-Fang Li, Gholamreza Haffari, and Shirui Pan. 2024. Reasoning on Graphs: Faithful and Interpretable Large Language Model Reasoning. arXiv:2310.01061 [cs.CL] https://arxiv.org/abs/2310.01061",
        "doi": null,
        "year": "2024",
        "has_doi": false,
        "title_from_ref": "Reasoning on Graphs: Faithful and Interpretable Large Language Model Reasoning",
        "arxiv_block": "arXiv:2310.01061 [cs.CL]",
        "pdf_url": "https://arxiv.org/pdf/2310.01061.pdf",
        "source": "arxiv_direct",
        "abstract": null,
        "title": "Reasoning on Graphs: Faithful and Interpretable Large Language Model Reasoning",
        "local_pdf_path": "downloaded_papers\\Reasoning on Graphs Faithful and Interpretable Large Language Model Reasoning.pdf"
    },
    {
        "index": 113,
        "raw": "Shengjie Ma, Chengjin Xu, Xuhui Jiang, Muzhi Li, Huaren Qu, and Jian Guo. 2024. Think-on-Graph 2.0: Deep and Interpretable Large Language Model Reasoning with Knowledge Graph-guided Retrieval. arXiv:2407.10805 [cs.CL] https://arxiv.org/abs/2407.10805",
        "doi": null,
        "year": "2024",
        "has_doi": false,
        "title_from_ref": "Think-on-Graph 2",
        "arxiv_block": "arXiv:2407.10805 [cs.CL]",
        "pdf_url": "https://arxiv.org/pdf/2407.10805.pdf",
        "source": "arxiv_direct",
        "abstract": null,
        "title": "Think-on-Graph 2",
        "local_pdf_path": "downloaded_papers\\Think-on-Graph 2.pdf"
    },
    {
        "index": 114,
        "raw": "Xinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao, and Nan Duan. 2023. Query Rewriting for Retrieval-Augmented Large Language Models. arXiv:2305.14283 [cs.CL] https://arxiv.org/abs/2305.14283",
        "doi": null,
        "year": "2023",
        "has_doi": false,
        "title_from_ref": "Query Rewriting for Retrieval-Augmented Large Language Models",
        "arxiv_block": "arXiv:2305.14283 [cs.CL]",
        "pdf_url": "https://arxiv.org/pdf/2305.14283.pdf",
        "source": "arxiv_direct",
        "abstract": null,
        "title": "Query Rewriting for Retrieval-Augmented Large Language Models",
        "local_pdf_path": "downloaded_papers\\Query Rewriting for Retrieval-Augmented Large Language Models.pdf"
    },
    {
        "index": 115,
        "raw": "Haitao Mao, Zhikai Chen, Wenzhuo Tang, Jianan Zhao, Yao Ma, Tong Zhao, Neil Shah, Mikhail Galkin, and Jiliang Tang. 2024. Position: Graph Foundation Models Are Already Here. In Forty-first International Conference on Machine J. ACM, Vol. 37, No. 4, Article 111. Publication date: September 2024. Graph Retrieval-Augmented Generation: A Survey 111:37 Learning .",
        "doi": null,
        "year": "2024",
        "has_doi": false,
        "title_from_ref": "Position: Graph Foundation Models Are Already Here",
        "arxiv_block": null,
        "pdf_url": "https://arxiv.org/pdf/2402.02216v3.pdf",
        "source": "arxiv_api",
        "abstract": "Graph Foundation Models (GFMs) are emerging as a significant research topic in the graph domain, aiming to develop graph models trained on extensive and diverse data to enhance their applicability across various tasks and domains. Developing GFMs presents unique challenges over traditional Graph Neural Networks (GNNs), which are typically trained from scratch for specific tasks on particular datasets. The primary challenge in constructing GFMs lies in effectively leveraging vast and diverse graph data to achieve positive transfer. Drawing inspiration from existing foundation models in the CV and NLP domains, we propose a novel perspective for the GFM development by advocating for a ``graph vocabulary'', in which the basic transferable units underlying graphs encode the invariance on graphs. We ground the graph vocabulary construction from essential aspects including network analysis, expressiveness, and stability. Such a vocabulary perspective can potentially advance the future GFM design in line with the neural scaling laws. All relevant resources with GFM design can be found here.",
        "title": "Position: Graph Foundation Models Are Already Here",
        "local_pdf_path": "downloaded_papers\\Position Graph Foundation Models Are Already Here.pdf"
    },
    {
        "index": 116,
        "raw": "Qiheng Mao, Zemin Liu, Chenghao Liu, Zhuo Li, and Jianling Sun. 2024. Advancing Graph Representation Learning with Large Language Models: A Comprehensive Survey of Techniques. arXiv:2402.05952 [cs.LG] https://arxiv.org/ abs/2402.05952",
        "doi": null,
        "year": "2024",
        "has_doi": false,
        "title_from_ref": "Advancing Graph Representation Learning with Large Language Models: A Comprehensive Survey of Techniques",
        "arxiv_block": "arXiv:2402.05952 [cs.LG]",
        "pdf_url": "https://arxiv.org/pdf/2402.05952.pdf",
        "source": "arxiv_direct",
        "abstract": null,
        "title": "Advancing Graph Representation Learning with Large Language Models: A Comprehensive Survey of Techniques",
        "local_pdf_path": "downloaded_papers\\Advancing Graph Representation Learning with Large Language Models A Comprehensive Survey of Techniq.pdf"
    },
    {
        "index": 117,
        "raw": "Shengyu Mao, Yong Jiang, Boli Chen, Xiao Li, Peng Wang, Xinyu Wang, Pengjun Xie, Fei Huang, Huajun Chen, and Ningyu Zhang. 2024. RaFe: Ranking Feedback Improves Query Rewriting for RAG. arXiv:2405.14431 [cs.CL] https://arxiv.org/abs/2405.14431",
        "doi": null,
        "year": "2024",
        "has_doi": false,
        "title_from_ref": "RaFe: Ranking Feedback Improves Query Rewriting for RAG",
        "arxiv_block": "arXiv:2405.14431 [cs.CL]",
        "pdf_url": "https://arxiv.org/pdf/2405.14431.pdf",
        "source": "arxiv_direct",
        "abstract": null,
        "title": "RaFe: Ranking Feedback Improves Query Rewriting for RAG",
        "local_pdf_path": "downloaded_papers\\RaFe Ranking Feedback Improves Query Rewriting for RAG.pdf"
    },
    {
        "index": 118,
        "raw": "Costas Mavromatis and George Karypis. 2022. ReaRev: Adaptive Reasoning for Question Answering over Knowledge Graphs. In Findings of the Association for Computational Linguistics: EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022 . 2447–2458.",
        "doi": null,
        "year": "2022",
        "has_doi": false,
        "title_from_ref": "ReaRev: Adaptive Reasoning for Question Answering over Knowledge Graphs",
        "arxiv_block": null,
        "pdf_url": "https://arxiv.org/pdf/2210.13650v1.pdf",
        "source": "arxiv_api",
        "abstract": "Knowledge Graph Question Answering (KGQA) involves retrieving entities as answers from a Knowledge Graph (KG) using natural language queries. The challenge is to learn to reason over question-relevant KG facts that traverse KG entities and lead to the question answers. To facilitate reasoning, the question is decoded into instructions, which are dense question representations used to guide the KG traversals. However, if the derived instructions do not exactly match the underlying KG information, they may lead to reasoning under irrelevant context. Our method, termed ReaRev, introduces a new way to KGQA reasoning with respect to both instruction decoding and execution. To improve instruction decoding, we perform reasoning in an adaptive manner, where KG-aware information is used to iteratively update the initial instructions. To improve instruction execution, we emulate breadth-first search (BFS) with graph neural networks (GNNs). The BFS strategy treats the instructions as a set and allows our method to decide on their execution order on the fly. Experimental results on three KGQA benchmarks demonstrate the ReaRev's effectiveness compared with previous state-of-the-art, especially when the KG is incomplete or when we tackle complex questions. Our code is publicly available at https://github.com/cmavro/ReaRev_KGQA.",
        "title": "ReaRev: Adaptive Reasoning for Question Answering over Knowledge Graphs",
        "local_pdf_path": "downloaded_papers\\ReaRev Adaptive Reasoning for Question Answering over Knowledge Graphs.pdf"
    },
    {
        "index": 119,
        "raw": "Costas Mavromatis and George Karypis. 2024. GNN-RAG: Graph Neural Retrieval for Large Language Model Reasoning. arXiv:2405.20139 [cs.CL] https://arxiv.org/abs/2405.20139",
        "doi": null,
        "year": "2024",
        "has_doi": false,
        "title_from_ref": "GNN-RAG: Graph Neural Retrieval for Large Language Model Reasoning",
        "arxiv_block": "arXiv:2405.20139 [cs.CL]",
        "pdf_url": "https://arxiv.org/pdf/2405.20139.pdf",
        "source": "arxiv_direct",
        "abstract": null,
        "title": "GNN-RAG: Graph Neural Retrieval for Large Language Model Reasoning",
        "local_pdf_path": "downloaded_papers\\GNN-RAG Graph Neural Retrieval for Large Language Model Reasoning.pdf"
    },
    {
        "index": 120,
        "raw": "Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. 2018. Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018 . 2381–2391.",
        "doi": "10.18653/v1/d18-1260",
        "year": "2018",
        "has_doi": false,
        "title_from_ref": "Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering",
        "arxiv_block": null,
        "pdf_url": "https://www.aclweb.org/anthology/D18-1260.pdf",
        "source": "openalex",
        "abstract": null,
        "title": "Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering",
        "local_pdf_path": "downloaded_papers\\Can a Suit of Armor Conduct Electricity A New Dataset for Open Book Question Answering.pdf"
    },
    {
        "index": 121,
        "raw": "Alexander H. Miller, Adam Fisch, Jesse Dodge, Amir-Hossein Karimi, Antoine Bordes, and Jason Weston. 2016. Key-Value Memory Networks for Directly Reading Documents. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, EMNLP 2016, Austin, Texas, USA, November 1-4, 2016 . 1400–1409.",
        "doi": null,
        "year": "2016",
        "has_doi": false,
        "title_from_ref": "Key-Value Memory Networks for Directly Reading Documents",
        "arxiv_block": null,
        "pdf_url": "https://arxiv.org/pdf/1606.03126v2.pdf",
        "source": "arxiv_api",
        "abstract": "Directly reading documents and being able to answer questions from them is an unsolved challenge. To avoid its inherent difficulty, question answering (QA) has been directed towards using Knowledge Bases (KBs) instead, which has proven effective. Unfortunately KBs often suffer from being too restrictive, as the schema cannot support certain types of answers, and too sparse, e.g. Wikipedia contains much more information than Freebase. In this work we introduce a new method, Key-Value Memory Networks, that makes reading documents more viable by utilizing different encodings in the addressing and output stages of the memory read operation. To compare using KBs, information extraction or Wikipedia documents directly in a single framework we construct an analysis tool, WikiMovies, a QA dataset that contains raw text alongside a preprocessed KB, in the domain of movies. Our method reduces the gap between all three settings. It also achieves state-of-the-art results on the existing WikiQA benchmark.",
        "title": "Key-Value Memory Networks for Directly Reading Documents",
        "local_pdf_path": "downloaded_papers\\Key-Value Memory Networks for Directly Reading Documents.pdf"
    },
    {
        "index": 122,
        "raw": "Seungwhan Moon, Pararth Shah, Anuj Kumar, and Rajen Subba. 2019. OpenDialKG: Explainable Conversational Reasoning with Attention-based Walks over Knowledge Graphs. In Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers . 845–854.",
        "doi": "10.18653/v1/p19-1081",
        "year": "2019",
        "has_doi": false,
        "title_from_ref": "OpenDialKG: Explainable Conversational Reasoning with Attention-based Walks over Knowledge Graphs",
        "arxiv_block": null,
        "pdf_url": "https://www.aclweb.org/anthology/P19-1081.pdf",
        "source": "openalex",
        "abstract": null,
        "title": "OpenDialKG: Explainable Conversational Reasoning with Attention-based Walks over Knowledge Graphs",
        "local_pdf_path": "downloaded_papers\\OpenDialKG Explainable Conversational Reasoning with Attention-based Walks over Knowledge Graphs.pdf"
    },
    {
        "index": 123,
        "raw": "Christopher Morris, Nils M. Kriege, Franka Bause, Kristian Kersting, Petra Mutzel, and Marion Neumann. 2020. TUDataset: A collection of benchmark datasets for learning with graphs. In ICML 2020 Workshop on Graph Representation Learning and Beyond (GRL+ 2020) .",
        "doi": null,
        "year": "2020",
        "has_doi": false,
        "title_from_ref": "TUDataset: A collection of benchmark datasets for learning with graphs",
        "arxiv_block": null,
        "pdf_url": "https://arxiv.org/pdf/2007.08663v1.pdf",
        "source": "arxiv_api",
        "abstract": "Recently, there has been an increasing interest in (supervised) learning with graph data, especially using graph neural networks. However, the development of meaningful benchmark datasets and standardized evaluation procedures is lagging, consequently hindering advancements in this area. To address this, we introduce the TUDataset for graph classification and regression. The collection consists of over 120 datasets of varying sizes from a wide range of applications. We provide Python-based data loaders, kernel and graph neural network baseline implementations, and evaluation tools. Here, we give an overview of the datasets, standardized evaluation procedures, and provide baseline experiments. All datasets are available at www.graphlearning.io. The experiments are fully reproducible from the code available at www.github.com/chrsmrrs/tudataset.",
        "title": "TUDataset: A collection of benchmark datasets for learning with graphs",
        "local_pdf_path": "downloaded_papers\\TUDataset A collection of benchmark datasets for learning with graphs.pdf"
    },
    {
        "index": 124,
        "raw": "Sai Munikoti, Anurag Acharya, Sridevi Wagle, and Sameera Horawalavithana. 2023. ATLANTIC: Structure-Aware Retrieval-Augmented Language Model for Interdisciplinary Science. arXiv:2311.12289 [cs.CL] https://arxiv.org/abs/ 2311.12289",
        "doi": null,
        "year": "2023",
        "has_doi": false,
        "title_from_ref": "ATLANTIC: Structure-Aware Retrieval-Augmented Language Model for Interdisciplinary Science",
        "arxiv_block": "arXiv:2311.12289 [cs.CL]",
        "pdf_url": "https://arxiv.org/pdf/2311.12289.pdf",
        "source": "arxiv_direct",
        "abstract": null,
        "title": "ATLANTIC: Structure-Aware Retrieval-Augmented Language Model for Interdisciplinary Science",
        "local_pdf_path": "downloaded_papers\\ATLANTIC Structure-Aware Retrieval-Augmented Language Model for Interdisciplinary Science.pdf"
    },
    {
        "index": 125,
        "raw": "Yuqi Nie, Yaxuan Kong, Xiaowen Dong, John M. Mulvey, H. Vincent Poor, Qingsong Wen, and Stefan Zohren. 2024. A Survey of Large Language Models for Financial Applications: Progress, Prospects and Challenges. arXiv:2406.11903 [qfin.GN] https://arxiv.org/abs/2406.11903",
        "doi": null,
        "year": "2024",
        "has_doi": false,
        "title_from_ref": "A Survey of Large Language Models for Financial Applications: Progress, Prospects and Challenges",
        "arxiv_block": "arXiv:2406.11903 [qfin.GN]",
        "pdf_url": "https://arxiv.org/pdf/2406.11903.pdf",
        "source": "arxiv_direct",
        "abstract": null,
        "title": "A Survey of Large Language Models for Financial Applications: Progress, Prospects and Challenges",
        "local_pdf_path": "downloaded_papers\\A Survey of Large Language Models for Financial Applications Progress, Prospects and Challenges.pdf"
    },
    {
        "index": 126,
        "raw": "Yasumasa Onoe, Michael J. Q. Zhang, Eunsol Choi, and Greg Durrett. 2021. CREAK: A Dataset for Commonsense Reasoning over Entity Knowledge. In Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual .",
        "doi": null,
        "year": "2021",
        "has_doi": false,
        "title_from_ref": "CREAK: A Dataset for Commonsense Reasoning over Entity Knowledge",
        "arxiv_block": null,
        "pdf_url": "https://arxiv.org/pdf/2109.01653v1.pdf",
        "source": "arxiv_api",
        "abstract": "Most benchmark datasets targeting commonsense reasoning focus on everyday scenarios: physical knowledge like knowing that you could fill a cup under a waterfall [Talmor et al., 2019], social knowledge like bumping into someone is awkward [Sap et al., 2019], and other generic situations. However, there is a rich space of commonsense inferences anchored to knowledge about specific entities: for example, deciding the truthfulness of a claim \"Harry Potter can teach classes on how to fly on a broomstick.\" Can models learn to combine entity knowledge with commonsense reasoning in this fashion? We introduce CREAK, a testbed for commonsense reasoning about entity knowledge, bridging fact-checking about entities (Harry Potter is a wizard and is skilled at riding a broomstick) with commonsense inferences (if you're good at a skill you can teach others how to do it). Our dataset consists of 13k human-authored English claims about entities that are either true or false, in addition to a small contrast set. Crowdworkers can easily come up with these statements and human performance on the dataset is high (high 90s); we argue that models should be able to blend entity knowledge and commonsense reasoning to do well here. In our experiments, we focus on the closed-book setting and observe that a baseline model finetuned on existing fact verification benchmark struggles on CREAK. Training a model on CREAK improves accuracy by a substantial margin, but still falls short of human performance. Our benchmark provides a unique probe into natural language understanding models, testing both its ability to retrieve facts (e.g., who teaches at the University of Chicago?) and unstated commonsense knowledge (e.g., butlers do not yell at guests).",
        "title": "CREAK: A Dataset for Commonsense Reasoning over Entity Knowledge",
        "local_pdf_path": "downloaded_papers\\CREAK A Dataset for Commonsense Reasoning over Entity Knowledge.pdf"
    },
    {
        "index": 127,
        "raw": "OpenAI. 2024. GPT-4 Technical Report. arXiv:2303.08774 [cs.CL] https://arxiv.org/abs/2303.08774",
        "doi": null,
        "year": "2024",
        "has_doi": false,
        "title_from_ref": "GPT-4 Technical Report",
        "arxiv_block": "arXiv:2303.08774 [cs.CL]",
        "pdf_url": "https://arxiv.org/pdf/2303.08774.pdf",
        "source": "arxiv_direct",
        "abstract": null,
        "title": "GPT-4 Technical Report",
        "local_pdf_path": "downloaded_papers\\GPT-4 Technical Report.pdf"
    },
    {
        "index": 128,
        "raw": "Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al .2022. Training language models to follow instructions with human feedback. Advances in neural information processing systems 35 (2022), 27730–27744.",
        "doi": null,
        "year": "2022",
        "has_doi": false,
        "title_from_ref": "Training language models to follow instructions with human feedback",
        "arxiv_block": null,
        "pdf_url": "https://arxiv.org/pdf/2203.02155v1.pdf",
        "source": "arxiv_api",
        "abstract": "Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.",
        "title": "Training language models to follow instructions with human feedback",
        "local_pdf_path": "downloaded_papers\\Training language models to follow instructions with human feedback.pdf"
    },
    {
        "index": 129,
        "raw": "Vardaan Pahuja, Boshi Wang, Hugo Latapie, Jayanth Srinivasa, and Yu Su. 2023. A Retrieve-and-Read Framework for Knowledge Graph Link Prediction. In Proceedings of the 32nd ACM International Conference on Information and Knowledge Management, CIKM 2023, Birmingham, United Kingdom, October 21-25, 2023 . 1992–2002.",
        "doi": null,
        "year": "2023",
        "has_doi": false,
        "title_from_ref": "A Retrieve-and-Read Framework for Knowledge Graph Link Prediction",
        "arxiv_block": null,
        "pdf_url": "https://arxiv.org/pdf/2212.09724v3.pdf",
        "source": "arxiv_api",
        "abstract": "Knowledge graph (KG) link prediction aims to infer new facts based on existing facts in the KG. Recent studies have shown that using the graph neighborhood of a node via graph neural networks (GNNs) provides more useful information compared to just using the query information. Conventional GNNs for KG link prediction follow the standard message-passing paradigm on the entire KG, which leads to superfluous computation, over-smoothing of node representations, and also limits their expressive power. On a large scale, it becomes computationally expensive to aggregate useful information from the entire KG for inference. To address the limitations of existing KG link prediction frameworks, we propose a novel retrieve-and-read framework, which first retrieves a relevant subgraph context for the query and then jointly reasons over the context and the query with a high-capacity reader. As part of our exemplar instantiation for the new framework, we propose a novel Transformer-based GNN as the reader, which incorporates graph-based attention structure and cross-attention between query and context for deep fusion. This simple yet effective design enables the model to focus on salient context information relevant to the query. Empirical results on two standard KG link prediction datasets demonstrate the competitive performance of the proposed method. Furthermore, our analysis yields valuable insights for designing improved retrievers within the framework.",
        "title": "A Retrieve-and-Read Framework for Knowledge Graph Link Prediction",
        "local_pdf_path": "downloaded_papers\\A Retrieve-and-Read Framework for Knowledge Graph Link Prediction.pdf"
    },
    {
        "index": 130,
        "raw": "Jeff Z. Pan, Simon Razniewski, Jan-Christoph Kalo, Sneha Singhania, Jiaoyan Chen, Stefan Dietze, Hajira Jabeen, Janna Omeliyanenko, Wen Zhang, Matteo Lissandrini, Russa Biswas, Gerard de Melo, Angela Bonifati, Edlira Vakaj, Mauro Dragoni, and Damien Graux. 2023. Large Language Models and Knowledge Graphs: Opportunities and Challenges. TGDK 1, 1 (2023), 2:1–2:38.",
        "doi": "10.3389/fcomp.2025.1590632",
        "year": "2023",
        "has_doi": false,
        "title_from_ref": "Large Language Models and Knowledge Graphs: Opportunities and Challenges",
        "arxiv_block": null,
        "pdf_url": "https://arxiv.org/pdf/2308.06374",
        "source": "semantic_scholar",
        "abstract": "Large Language Models (LLMs) have taken Knowledge Representation -- and the world -- by storm. This inflection point marks a shift from explicit knowledge representation to a renewed focus on the hybrid representation of both explicit knowledge and parametric knowledge. In this position paper, we will discuss some of the common debate points within the community on LLMs (parametric knowledge) and Knowledge Graphs (explicit knowledge) and speculate on opportunities and visions that the renewed focus brings, as well as related research topics and challenges.",
        "title": "Large Language Models and Knowledge Graphs: Opportunities and Challenges",
        "local_pdf_path": "downloaded_papers\\Large Language Models and Knowledge Graphs Opportunities and Challenges.pdf"
    },
    {
        "index": 131,
        "raw": "Shirui Pan, Linhao Luo, Yufei Wang, Chen Chen, Jiapu Wang, and Xindong Wu. 2024. Unifying Large Language Models and Knowledge Graphs: A Roadmap. IEEE Trans. Knowl. Data Eng. 36, 7 (2024), 3580–3599.",
        "doi": "10.1109/tkde.2024.3352100",
        "year": "2024",
        "has_doi": false,
        "title_from_ref": "Unifying Large Language Models and Knowledge Graphs: A Roadmap",
        "arxiv_block": null,
        "pdf_url": "http://arxiv.org/pdf/2306.08302",
        "source": "semantic_scholar",
        "abstract": "Large language models (LLMs), such as ChatGPT and GPT4, are making new waves in the field of natural language processing and artificial intelligence, due to their emergent ability and generalizability. However, LLMs are black-box models, which often fall short of capturing and accessing factual knowledge. In contrast, Knowledge Graphs (KGs), Wikipedia, and Huapu for example, are structured knowledge models that explicitly store rich factual knowledge. KGs can enhance LLMs by providing external knowledge for inference and interpretability. Meanwhile, KGs are difficult to construct and evolve by nature, which challenges the existing methods in KGs to generate new facts and represent unseen knowledge. Therefore, it is complementary to unify LLMs and KGs together and, simultaneously, leverage their advantages. In this article, we present a forward-looking roadmap for the unification of LLMs and KGs. Our roadmap consists of three general frameworks, namely: 1) KG-enhanced LLMs, which incorporate KGs during the pre-training and inference phases of LLMs, or for the purpose of enhancing understanding of the knowledge learned by LLMs; 2) LLM-augmented KGs, that leverage LLMs for different KG tasks such as embedding, completion, construction, graph-to-text generation, and question answering; and 3) Synergized LLMs + KGs, in which LLMs and KGs play equal roles and work in a mutually beneficial way to enhance both LLMs and KGs for bidirectional reasoning driven by both data and knowledge. We review and summarize existing efforts within these three frameworks in our roadmap and pinpoint their future research directions.",
        "title": "Unifying Large Language Models and Knowledge Graphs: A Roadmap",
        "local_pdf_path": "downloaded_papers\\Unifying Large Language Models and Knowledge Graphs A Roadmap.pdf"
    },
    {
        "index": 132,
        "raw": "Wenjun Peng, Guiyang Li, Yue Jiang, Zilong Wang, Dan Ou, Xiaoyi Zeng, Derong Xu, Tong Xu, and Enhong Chen. 2024. Large Language Model based Long-tail Query Rewriting in Taobao Search. In Companion Proceedings of the ACM on Web Conference 2024, WWW 2024, Singapore, Singapore, May 13-17, 2024 . 20–28.",
        "doi": null,
        "year": "2024",
        "has_doi": false,
        "title_from_ref": "Large Language Model based Long-tail Query Rewriting in Taobao Search",
        "arxiv_block": null,
        "pdf_url": "https://arxiv.org/pdf/2311.03758v3.pdf",
        "source": "arxiv_api",
        "abstract": "In the realm of e-commerce search, the significance of semantic matching cannot be overstated, as it directly impacts both user experience and company revenue. Along this line, query rewriting, serving as an important technique to bridge the semantic gaps inherent in the semantic matching process, has attached wide attention from the industry and academia. However, existing query rewriting methods often struggle to effectively optimize long-tail queries and alleviate the phenomenon of \"few-recall\" caused by semantic gap. In this paper, we present BEQUE, a comprehensive framework that Bridges the sEmantic gap for long-tail QUEries. In detail, BEQUE comprises three stages: multi-instruction supervised fine tuning (SFT), offline feedback, and objective alignment. We first construct a rewriting dataset based on rejection sampling and auxiliary tasks mixing to fine-tune our large language model (LLM) in a supervised fashion. Subsequently, with the well-trained LLM, we employ beam search to generate multiple candidate rewrites, and feed them into Taobao offline system to obtain the partial order. Leveraging the partial order of rewrites, we introduce a contrastive learning method to highlight the distinctions between rewrites, and align the model with the Taobao online objectives. Offline experiments prove the effectiveness of our method in bridging semantic gap. Online A/B tests reveal that our method can significantly boost gross merchandise volume (GMV), number of transaction (#Trans) and unique visitor (UV) for long-tail queries. BEQUE has been deployed on Taobao, one of most popular online shopping platforms in China, since October 2023.",
        "title": "Large Language Model based Long-tail Query Rewriting in Taobao Search",
        "local_pdf_path": "downloaded_papers\\Large Language Model based Long-tail Query Rewriting in Taobao Search.pdf"
    },
    {
        "index": 133,
        "raw": "Zhuoyi Peng and Yi Yang. 2024. Connecting the Dots: Inferring Patent Phrase Similarity with Retrieved Phrase Graphs. arXiv:2403.16265 [cs.CL] https://arxiv.org/abs/2403.16265",
        "doi": null,
        "year": "2024",
        "has_doi": false,
        "title_from_ref": "Connecting the Dots: Inferring Patent Phrase Similarity with Retrieved Phrase Graphs",
        "arxiv_block": "arXiv:2403.16265 [cs.CL]",
        "pdf_url": "https://arxiv.org/pdf/2403.16265.pdf",
        "source": "arxiv_direct",
        "abstract": null,
        "title": "Connecting the Dots: Inferring Patent Phrase Similarity with Retrieved Phrase Graphs",
        "local_pdf_path": "downloaded_papers\\Connecting the Dots Inferring Patent Phrase Similarity with Retrieved Phrase Graphs.pdf"
    },
    {
        "index": 134,
        "raw": "Aleksandr Perevalov, Dennis Diefenbach, Ricardo Usbeck, and Andreas Both. 2022. QALD-9-plus: A Multilingual Dataset for Question Answering over DBpedia and Wikidata Translated by Native Speakers. In 16th IEEE International Conference on Semantic Computing, ICSC 2022, Laguna Hills, CA, USA, January 26-28, 2022 . 229–234.",
        "doi": "10.1109/icsc52841.2022.00045",
        "year": "2022",
        "has_doi": false,
        "title_from_ref": "QALD-9-plus: A Multilingual Dataset for Question Answering over DBpedia and Wikidata Translated by Native Speakers",
        "arxiv_block": null,
        "pdf_url": null,
        "source": "openalex",
        "abstract": null,
        "title": "QALD-9-plus: A Multilingual Dataset for Question Answering over DBpedia and Wikidata Translated by Native Speakers",
        "local_pdf_path": null
    },
    {
        "index": 135,
        "raw": "Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick S. H. Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine Jernite, Vladimir Karpukhin, Jean Maillard, Vassilis Plachouras, Tim Rocktäschel, and Sebastian Riedel. 2021. KILT: a Benchmark for Knowledge Intensive Language Tasks. In Proceedings of the 2021 Conference of the North J. ACM, Vol. 37, No. 4, Article 111. Publication date: September 2024. 111:38 Peng et al. American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021 . 2523–2544.",
        "doi": null,
        "year": "2021",
        "has_doi": false,
        "title_from_ref": "KILT: a Benchmark for Knowledge Intensive Language Tasks",
        "arxiv_block": null,
        "pdf_url": "https://arxiv.org/pdf/2009.02252v4.pdf",
        "source": "arxiv_api",
        "abstract": "Challenging problems such as open-domain question answering, fact checking, slot filling and entity linking require access to large, external knowledge sources. While some models do well on individual tasks, developing general models is difficult as each task might require computationally expensive indexing of custom knowledge sources, in addition to dedicated infrastructure. To catalyze research on models that condition on specific information in large textual resources, we present a benchmark for knowledge-intensive language tasks (KILT). All tasks in KILT are grounded in the same snapshot of Wikipedia, reducing engineering turnaround through the re-use of components, as well as accelerating research into task-agnostic memory architectures. We test both task-specific and general baselines, evaluating downstream performance in addition to the ability of the models to provide provenance. We find that a shared dense vector index coupled with a seq2seq model is a strong baseline, outperforming more tailor-made approaches for fact checking, open-domain question answering and dialogue, and yielding competitive results on entity linking and slot filling, by generating disambiguated text. KILT data and code are available at https://github.com/facebookresearch/KILT.",
        "title": "KILT: a Benchmark for Knowledge Intensive Language Tasks",
        "local_pdf_path": "downloaded_papers\\KILT a Benchmark for Knowledge Intensive Language Tasks.pdf"
    },
    {
        "index": 136,
        "raw": "Zhixiao Qi, Yijiong Yu, Meiqi Tu, Junyi Tan, and Yongfeng Huang. 2023. FoodGPT: A Large Language Model in Food Testing Domain with Incremental Pre-training and Knowledge Graph Prompt. arXiv:2308.10173 [cs.CL] https://arxiv.org/abs/2308.10173",
        "doi": null,
        "year": "2023",
        "has_doi": false,
        "title_from_ref": "FoodGPT: A Large Language Model in Food Testing Domain with Incremental Pre-training and Knowledge Graph Prompt",
        "arxiv_block": "arXiv:2308.10173 [cs.CL]",
        "pdf_url": "https://arxiv.org/pdf/2308.10173.pdf",
        "source": "arxiv_direct",
        "abstract": null,
        "title": "FoodGPT: A Large Language Model in Food Testing Domain with Incremental Pre-training and Knowledge Graph Prompt",
        "local_pdf_path": "downloaded_papers\\FoodGPT A Large Language Model in Food Testing Domain with Incremental Pre-training and Knowledge Gr.pdf"
    },
    {
        "index": 137,
        "raw": "Zile Qiao, Wei Ye, Yong Jiang, Tong Mo, Pengjun Xie, Weiping Li, Fei Huang, and Shikun Zhang. 2024. Supportivenessbased Knowledge Rewriting for Retrieval-augmented Language Modeling. arXiv:2406.08116 [cs.CL] https://arxiv. org/abs/2406.08116",
        "doi": null,
        "year": "2024",
        "has_doi": false,
        "title_from_ref": "Supportivenessbased Knowledge Rewriting for Retrieval-augmented Language Modeling",
        "arxiv_block": "arXiv:2406.08116 [cs.CL]",
        "pdf_url": "https://arxiv.org/pdf/2406.08116.pdf",
        "source": "arxiv_direct",
        "abstract": null,
        "title": "Supportivenessbased Knowledge Rewriting for Retrieval-augmented Language Modeling",
        "local_pdf_path": "downloaded_papers\\Supportivenessbased Knowledge Rewriting for Retrieval-augmented Language Modeling.pdf"
    },
    {
        "index": 138,
        "raw": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. J. Mach. Learn. Res.21 (2020), 140:1–140:67.",
        "doi": null,
        "year": "2020",
        "has_doi": false,
        "title_from_ref": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
        "arxiv_block": null,
        "pdf_url": "https://arxiv.org/pdf/1910.10683v4.pdf",
        "source": "arxiv_api",
        "abstract": "Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.",
        "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
        "local_pdf_path": "downloaded_papers\\Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.pdf"
    },
    {
        "index": 139,
        "raw": "Priyanka Ranade and Anupam Joshi. 2023. FABULA: Intelligence Report Generation Using Retrieval-Augmented Narrative Construction. In Proceedings of the International Conference on Advances in Social Networks Analysis and Mining, ASONAM 2023, Kusadasi, Turkey, November 6-9, 2023 . 603–610.",
        "doi": null,
        "year": "2023",
        "has_doi": false,
        "title_from_ref": "FABULA: Intelligence Report Generation Using Retrieval-Augmented Narrative Construction",
        "arxiv_block": null,
        "pdf_url": "https://arxiv.org/pdf/2310.13848v2.pdf",
        "source": "arxiv_api",
        "abstract": "Narrative construction is the process of representing disparate event information into a logical plot structure that models an end to end story. Intelligence analysis is an example of a domain that can benefit tremendously from narrative construction techniques, particularly in aiding analysts during the largely manual and costly process of synthesizing event information into comprehensive intelligence reports. Manual intelligence report generation is often prone to challenges such as integrating dynamic event information, writing fine-grained queries, and closing information gaps. This motivates the development of a system that retrieves and represents critical aspects of events in a form that aids in automatic generation of intelligence reports.\n  We introduce a Retrieval Augmented Generation (RAG) approach to augment prompting of an autoregressive decoder by retrieving structured information asserted in a knowledge graph to generate targeted information based on a narrative plot model. We apply our approach to the problem of neural intelligence report generation and introduce FABULA, framework to augment intelligence analysis workflows using RAG. An analyst can use FABULA to query an Event Plot Graph (EPG) to retrieve relevant event plot points, which can be used to augment prompting of a Large Language Model (LLM) during intelligence report generation. Our evaluation studies show that the plot points included in the generated intelligence reports have high semantic relevance, high coherency, and low data redundancy.",
        "title": "FABULA: Intelligence Report Generation Using Retrieval-Augmented Narrative Construction",
        "local_pdf_path": "downloaded_papers\\FABULA Intelligence Report Generation Using Retrieval-Augmented Narrative Construction.pdf"
    },
    {
        "index": 140,
        "raw": "Nils Reimers and Iryna Gurevych. 2019. Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019 . 3980–3990.",
        "doi": null,
        "year": "2019",
        "has_doi": false,
        "title_from_ref": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks",
        "arxiv_block": null,
        "pdf_url": "https://arxiv.org/pdf/1908.10084v1.pdf",
        "source": "arxiv_api",
        "abstract": "BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations (~65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering.\n  In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT.\n  We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.",
        "title": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks",
        "local_pdf_path": "downloaded_papers\\Sentence-BERT Sentence Embeddings using Siamese BERT-Networks.pdf"
    },
    {
        "index": 141,
        "raw": "Yu Rong, Wenbing Huang, Tingyang Xu, and Junzhou Huang. 2020. DropEdge: Towards Deep Graph Convolutional Networks on Node Classification. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020 .",
        "doi": null,
        "year": "2020",
        "has_doi": false,
        "title_from_ref": "DropEdge: Towards Deep Graph Convolutional Networks on Node Classification",
        "arxiv_block": null,
        "pdf_url": "https://arxiv.org/pdf/1907.10903v4.pdf",
        "source": "arxiv_api",
        "abstract": "\\emph{Over-fitting} and \\emph{over-smoothing} are two main obstacles of developing deep Graph Convolutional Networks (GCNs) for node classification. In particular, over-fitting weakens the generalization ability on small dataset, while over-smoothing impedes model training by isolating output representations from the input features with the increase in network depth. This paper proposes DropEdge, a novel and flexible technique to alleviate both issues. At its core, DropEdge randomly removes a certain number of edges from the input graph at each training epoch, acting like a data augmenter and also a message passing reducer. Furthermore, we theoretically demonstrate that DropEdge either reduces the convergence speed of over-smoothing or relieves the information loss caused by it. More importantly, our DropEdge is a general skill that can be equipped with many other backbone models (e.g. GCN, ResGCN, GraphSAGE, and JKNet) for enhanced performance. Extensive experiments on several benchmarks verify that DropEdge consistently improves the performance on a variety of both shallow and deep GCNs. The effect of DropEdge on preventing over-smoothing is empirically visualized and validated as well. Codes are released on~\\url{https://github.com/DropEdge/DropEdge}.",
        "title": "DropEdge: Towards Deep Graph Convolutional Networks on Node Classification",
        "local_pdf_path": "downloaded_papers\\DropEdge Towards Deep Graph Convolutional Networks on Node Classification.pdf"
    },
    {
        "index": 142,
        "raw": "Maarten Sap, Ronan Le Bras, Emily Allaway, Chandra Bhagavatula, Nicholas Lourie, Hannah Rashkin, Brendan Roof, Noah A. Smith, and Yejin Choi. 2019. ATOMIC: An Atlas of Machine Commonsense for If-Then Reasoning. In The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019, Honolulu, Hawaii, USA, January 27 - February 1, 2019 . 3027–3035.",
        "doi": null,
        "year": "2019",
        "has_doi": false,
        "title_from_ref": "ATOMIC: An Atlas of Machine Commonsense for If-Then Reasoning",
        "arxiv_block": null,
        "pdf_url": "https://arxiv.org/pdf/1811.00146v3.pdf",
        "source": "arxiv_api",
        "abstract": "We present ATOMIC, an atlas of everyday commonsense reasoning, organized through 877k textual descriptions of inferential knowledge. Compared to existing resources that center around taxonomic knowledge, ATOMIC focuses on inferential knowledge organized as typed if-then relations with variables (e.g., \"if X pays Y a compliment, then Y will likely return the compliment\"). We propose nine if-then relation types to distinguish causes vs. effects, agents vs. themes, voluntary vs. involuntary events, and actions vs. mental states. By generatively training on the rich inferential knowledge described in ATOMIC, we show that neural models can acquire simple commonsense capabilities and reason about previously unseen events. Experimental results demonstrate that multitask models that incorporate the hierarchical structure of if-then relation types lead to more accurate inference compared to models trained in isolation, as measured by both automatic and human evaluation.",
        "title": "ATOMIC: An Atlas of Machine Commonsense for If-Then Reasoning",
        "local_pdf_path": "downloaded_papers\\ATOMIC An Atlas of Machine Commonsense for If-Then Reasoning.pdf"
    },
    {
        "index": 143,
        "raw": "Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi. 2019. SocialIQA: Commonsense Reasoning about Social Interactions. arXiv:1904.09728 [cs.CL] https://arxiv.org/abs/1904.09728",
        "doi": null,
        "year": "2019",
        "has_doi": false,
        "title_from_ref": "SocialIQA: Commonsense Reasoning about Social Interactions",
        "arxiv_block": "arXiv:1904.09728 [cs.CL]",
        "pdf_url": "https://arxiv.org/pdf/1904.09728.pdf",
        "source": "arxiv_direct",
        "abstract": null,
        "title": "SocialIQA: Commonsense Reasoning about Social Interactions",
        "local_pdf_path": "downloaded_papers\\SocialIQA Commonsense Reasoning about Social Interactions.pdf"
    },
    {
        "index": 144,
        "raw": "Bhaskarjit Sarmah, Benika Hall, Rohan Rao, Sunil Patel, Stefano Pasquali, and Dhagash Mehta. 2024. HybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented Generation for Efficient Information Extraction. arXiv:2408.04948 [cs.CL] https://arxiv.org/abs/2408.04948",
        "doi": null,
        "year": "2024",
        "has_doi": false,
        "title_from_ref": "HybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented Generation for Efficient Information Extraction",
        "arxiv_block": "arXiv:2408.04948 [cs.CL]",
        "pdf_url": "https://arxiv.org/pdf/2408.04948.pdf",
        "source": "arxiv_direct",
        "abstract": null,
        "title": "HybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented Generation for Efficient Information Extraction",
        "local_pdf_path": "downloaded_papers\\HybridRAG Integrating Knowledge Graphs and Vector Retrieval Augmented Generation for Efficient Infor.pdf"
    },
    {
        "index": 145,
        "raw": "Apoorv Saxena, Aditay Tripathi, and Partha P. Talukdar. 2020. Improving Multi-hop Question Answering over Knowledge Graphs using Knowledge Base Embeddings. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020 . 4498–4507.",
        "doi": "10.18653/v1/2020.acl-main.412",
        "year": "2020",
        "has_doi": false,
        "title_from_ref": "Improving Multi-hop Question Answering over Knowledge Graphs using Knowledge Base Embeddings",
        "arxiv_block": null,
        "pdf_url": "https://www.aclweb.org/anthology/2020.acl-main.412.pdf",
        "source": "semantic_scholar",
        "abstract": "Knowledge Graphs (KG) are multi-relational graphs consisting of entities as nodes and relations among them as typed edges. Goal of the Question Answering over KG (KGQA) task is to answer natural language queries posed over the KG. Multi-hop KGQA requires reasoning over multiple edges of the KG to arrive at the right answer. KGs are often incomplete with many missing links, posing additional challenges for KGQA, especially for multi-hop KGQA. Recent research on multi-hop KGQA has attempted to handle KG sparsity using relevant external text, which isn’t always readily available. In a separate line of research, KG embedding methods have been proposed to reduce KG sparsity by performing missing link prediction. Such KG embedding methods, even though highly relevant, have not been explored for multi-hop KGQA so far. We fill this gap in this paper and propose EmbedKGQA. EmbedKGQA is particularly effective in performing multi-hop KGQA over sparse KGs. EmbedKGQA also relaxes the requirement of answer selection from a pre-specified neighborhood, a sub-optimal constraint enforced by previous multi-hop KGQA methods. Through extensive experiments on multiple benchmark datasets, we demonstrate EmbedKGQA’s effectiveness over other state-of-the-art baselines.",
        "title": "Improving Multi-hop Question Answering over Knowledge Graphs using Knowledge Base Embeddings",
        "local_pdf_path": "downloaded_papers\\Improving Multi-hop Question Answering over Knowledge Graphs using Knowledge Base Embeddings.pdf"
    },
    {
        "index": 146,
        "raw": "Priyanka Sen, Alham Fikri Aji, and Amir Saffari. 2022. Mintaka: A Complex, Natural, and Multilingual Dataset for End-to-End Question Answering. In Proceedings of the 29th International Conference on Computational Linguistics, COLING 2022, Gyeongju, Republic of Korea, October 12-17, 2022 . 1604–1619.",
        "doi": "https://doi.org/10.48550/arxiv.2210.01613",
        "year": "2022",
        "has_doi": false,
        "title_from_ref": "Mintaka: A Complex, Natural, and Multilingual Dataset for End-to-End Question Answering",
        "arxiv_block": null,
        "pdf_url": "https://arxiv.org/pdf/2210.01613",
        "source": "openalex",
        "abstract": null,
        "title": "Mintaka: A Complex, Natural, and Multilingual Dataset for End-to-End Question Answering",
        "local_pdf_path": "downloaded_papers\\Mintaka A Complex, Natural, and Multilingual Dataset for End-to-End Question Answering.pdf"
    },
    {
        "index": 147,
        "raw": "Ahsan Shehzad, Feng Xia, Shagufta Abid, Ciyuan Peng, Shuo Yu, Dongyu Zhang, and Karin Verspoor. 2024. Graph Transformers: A Survey. arXiv:2407.09777 [cs.LG] https://arxiv.org/abs/2407.09777",
        "doi": null,
        "year": "2024",
        "has_doi": false,
        "title_from_ref": "Graph Transformers: A Survey",
        "arxiv_block": "arXiv:2407.09777 [cs.LG]",
        "pdf_url": "https://arxiv.org/pdf/2407.09777.pdf",
        "source": "arxiv_direct",
        "abstract": null,
        "title": "Graph Transformers: A Survey",
        "local_pdf_path": "downloaded_papers\\Graph Transformers A Survey.pdf"
    },
    {
        "index": 148,
        "raw": "Yiheng Shu, Zhiwei Yu, Yuhan Li, Börje F. Karlsson, Tingting Ma, Yuzhong Qu, and Chin-Yew Lin. 2022. TIARA: Multi-grained Retrieval for Robust Question Answering over Large Knowledge Bases. arXiv:2210.12925 [cs.CL] https://arxiv.org/abs/2210.12925",
        "doi": null,
        "year": "2022",
        "has_doi": false,
        "title_from_ref": "TIARA: Multi-grained Retrieval for Robust Question Answering over Large Knowledge Bases",
        "arxiv_block": "arXiv:2210.12925 [cs.CL]",
        "pdf_url": "https://arxiv.org/pdf/2210.12925.pdf",
        "source": "arxiv_direct",
        "abstract": null,
        "title": "TIARA: Multi-grained Retrieval for Robust Question Answering over Large Knowledge Bases",
        "local_pdf_path": "downloaded_papers\\TIARA Multi-grained Retrieval for Robust Question Answering over Large Knowledge Bases.pdf"
    },
    {
        "index": 149,
        "raw": "Saurabh Srivastava, Milind D Jain, Harshita Jain, Kritik Jaroli, VJ Mayank Patel, and L Khan. 2020. IOT monitoring bin for smart cities. In 3rd Smart Cities Symposium (SCS 2020) , Vol. 2020. IET, 533–536.",
        "doi": "10.1049/icp.2021.0968",
        "year": "2020",
        "has_doi": false,
        "title_from_ref": "IOT monitoring bin for smart cities",
        "arxiv_block": null,
        "pdf_url": null,
        "source": "semantic_scholar",
        "abstract": null,
        "title": "IOT monitoring bin for smart cities",
        "local_pdf_path": null
    },
    {
        "index": 150,
        "raw": "Fabian M Suchanek, Gjergji Kasneci, and Gerhard Weikum. 2007. Yago: a core of semantic knowledge. In Proceedings of the 16th international conference on World Wide Web . 697–706.",
        "doi": "10.7763/ijmlc.2011.v1.17",
        "year": "2007",
        "has_doi": false,
        "title_from_ref": "Yago: a core of semantic knowledge",
        "arxiv_block": null,
        "pdf_url": "https://hal.science/hal-01472497/file/www2007.pdf",
        "source": "semantic_scholar",
        "abstract": null,
        "title": "Yago: a core of semantic knowledge",
        "local_pdf_path": "downloaded_papers\\Yago a core of semantic knowledge.pdf"
    },
    {
        "index": 151,
        "raw": "Haitian Sun, Tania Bedrax-Weiss, and William W. Cohen. 2019. PullNet: Open Domain Question Answering with Iterative Retrieval on Knowledge Bases and Text. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019 . 2380–2390.",
        "doi": "10.18653/v1/d19-1242",
        "year": "2019",
        "has_doi": false,
        "title_from_ref": "PullNet: Open Domain Question Answering with Iterative Retrieval on Knowledge Bases and Text",
        "arxiv_block": null,
        "pdf_url": "https://arxiv.org/pdf/1904.09537",
        "source": "semantic_scholar",
        "abstract": "We consider open-domain question answering (QA) where answers are drawn from either a corpus, a knowledge base (KB), or a combination of both of these. We focus on a setting in which a corpus is supplemented with a large but incomplete KB, and on questions that require non-trivial (e.g., “multi-hop”) reasoning. We describe PullNet, an integrated framework for (1) learning what to retrieve and (2) reasoning with this heterogeneous information to find the best answer. PullNet uses an iterative process to construct a question-specific subgraph that contains information relevant to the question. In each iteration, a graph convolutional network (graph CNN) is used to identify subgraph nodes that should be expanded using retrieval (or “pull”) operations on the corpus and/or KB. After the subgraph is complete, another graph CNN is used to extract the answer from the subgraph. This retrieve-and-reason process allows us to answer multi-hop questions using large KBs and corpora. PullNet is weakly supervised, requiring question-answer pairs but not gold inference paths. Experimentally PullNet improves over the prior state-of-the art, and in the setting where a corpus is used with incomplete KB these improvements are often dramatic. PullNet is also often superior to prior systems in a KB-only setting or a text-only setting.",
        "title": "PullNet: Open Domain Question Answering with Iterative Retrieval on Knowledge Bases and Text",
        "local_pdf_path": "downloaded_papers\\PullNet Open Domain Question Answering with Iterative Retrieval on Knowledge Bases and Text.pdf"
    },
    {
        "index": 152,
        "raw": "Haitian Sun, Bhuwan Dhingra, Manzil Zaheer, Kathryn Mazaitis, Ruslan Salakhutdinov, and William W. Cohen. 2018. Open Domain Question Answering Using Early Fusion of Knowledge Bases and Text. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018 . 4231–4242.",
        "doi": "10.18653/v1/d18-1455",
        "year": "2018",
        "has_doi": false,
        "title_from_ref": "Open Domain Question Answering Using Early Fusion of Knowledge Bases and Text",
        "arxiv_block": null,
        "pdf_url": "https://www.aclweb.org/anthology/D18-1455.pdf",
        "source": "semantic_scholar",
        "abstract": "Open Domain Question Answering (QA) is evolving from complex pipelined systems to end-to-end deep neural networks. Specialized neural models have been developed for extracting answers from either text alone or Knowledge Bases (KBs) alone. In this paper we look at a more practical setting, namely QA over the combination of a KB and entity-linked text, which is appropriate when an incomplete KB is available with a large text corpus. Building on recent advances in graph representation learning we propose a novel model, GRAFT-Net, for extracting answers from a question-specific subgraph containing text and KB entities and relations. We construct a suite of benchmark tasks for this problem, varying the difficulty of questions, the amount of training data, and KB completeness. We show that GRAFT-Net is competitive with the state-of-the-art when tested using either KBs or text alone, and vastly outperforms existing methods in the combined setting.",
        "title": "Open Domain Question Answering Using Early Fusion of Knowledge Bases and Text",
        "local_pdf_path": "downloaded_papers\\Open Domain Question Answering Using Early Fusion of Knowledge Bases and Text.pdf"
    },
    {
        "index": 153,
        "raw": "Hao Sun, Yang Li, Liwei Deng, Bowen Li, Binyuan Hui, Binhua Li, Yunshi Lan, Yan Zhang, and Yongbin Li. 2023. History Semantic Graph Enhanced Conversational KBQA with Temporal Information Modeling. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023 . 3521–3533. J. ACM, Vol. 37, No. 4, Article 111. Publication date: September 2024. Graph Retrieval-Augmented Generation: A Survey 111:39",
        "doi": null,
        "year": "2023",
        "has_doi": false,
        "title_from_ref": "History Semantic Graph Enhanced Conversational KBQA with Temporal Information Modeling",
        "arxiv_block": null,
        "pdf_url": "https://arxiv.org/pdf/2306.06872v1.pdf",
        "source": "arxiv_api",
        "abstract": "Context information modeling is an important task in conversational KBQA. However, existing methods usually assume the independence of utterances and model them in isolation. In this paper, we propose a History Semantic Graph Enhanced KBQA model (HSGE) that is able to effectively model long-range semantic dependencies in conversation history while maintaining low computational cost. The framework incorporates a context-aware encoder, which employs a dynamic memory decay mechanism and models context at different levels of granularity. We evaluate HSGE on a widely used benchmark dataset for complex sequential question answering. Experimental results demonstrate that it outperforms existing baselines averaged on all question types.",
        "title": "History Semantic Graph Enhanced Conversational KBQA with Temporal Information Modeling",
        "local_pdf_path": "downloaded_papers\\History Semantic Graph Enhanced Conversational KBQA with Temporal Information Modeling.pdf"
    },
    {
        "index": 154,
        "raw": "Jiashuo Sun, Chengjin Xu, Lumingyuan Tang, Saizhuo Wang, Chen Lin, Yeyun Gong, Lionel M. Ni, Heung-Yeung Shum, and Jian Guo. 2024. Think-on-Graph: Deep and Responsible Reasoning of Large Language Model on Knowledge Graph. arXiv:2307.07697 [cs.CL] https://arxiv.org/abs/2307.07697",
        "doi": null,
        "year": "2024",
        "has_doi": false,
        "title_from_ref": "Think-on-Graph: Deep and Responsible Reasoning of Large Language Model on Knowledge Graph",
        "arxiv_block": "arXiv:2307.07697 [cs.CL]",
        "pdf_url": "https://arxiv.org/pdf/2307.07697.pdf",
        "source": "arxiv_direct",
        "abstract": null,
        "title": "Think-on-Graph: Deep and Responsible Reasoning of Large Language Model on Knowledge Graph",
        "local_pdf_path": "downloaded_papers\\Think-on-Graph Deep and Responsible Reasoning of Large Language Model on Knowledge Graph.pdf"
    },
    {
        "index": 155,
        "raw": "Lei Sun, Zhengwei Tao, Youdi Li, and Hiroshi Arakawa. 2024. ODA: Observation-Driven Agent for integrating LLMs and Knowledge Graphs. arXiv:2404.07677 [cs.CL] https://arxiv.org/abs/2404.07677",
        "doi": null,
        "year": "2024",
        "has_doi": false,
        "title_from_ref": "ODA: Observation-Driven Agent for integrating LLMs and Knowledge Graphs",
        "arxiv_block": "arXiv:2404.07677 [cs.CL]",
        "pdf_url": "https://arxiv.org/pdf/2404.07677.pdf",
        "source": "arxiv_direct",
        "abstract": null,
        "title": "ODA: Observation-Driven Agent for integrating LLMs and Knowledge Graphs",
        "local_pdf_path": "downloaded_papers\\ODA Observation-Driven Agent for integrating LLMs and Knowledge Graphs.pdf"
    },
    {
        "index": 156,
        "raw": "Alon Talmor and Jonathan Berant. 2018. The Web as a Knowledge-Base for Answering Complex Questions. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018, New Orleans, Louisiana, USA, June 1-6, 2018, Volume 1 (Long Papers) . 641–651.",
        "doi": null,
        "year": "2018",
        "has_doi": false,
        "title_from_ref": "The Web as a Knowledge-Base for Answering Complex Questions",
        "arxiv_block": null,
        "pdf_url": "https://arxiv.org/pdf/1803.06643v1.pdf",
        "source": "arxiv_api",
        "abstract": "Answering complex questions is a time-consuming activity for humans that requires reasoning and integration of information. Recent work on reading comprehension made headway in answering simple questions, but tackling complex questions is still an ongoing research challenge. Conversely, semantic parsers have been successful at handling compositionality, but only when the information resides in a target knowledge-base. In this paper, we present a novel framework for answering broad and complex questions, assuming answering simple questions is possible using a search engine and a reading comprehension model. We propose to decompose complex questions into a sequence of simple questions, and compute the final answer from the sequence of answers. To illustrate the viability of our approach, we create a new dataset of complex questions, ComplexWebQuestions, and present a model that decomposes questions and interacts with the web to compute an answer. We empirically demonstrate that question decomposition improves performance from 20.8 precision@1 to 27.5 precision@1 on this new dataset.",
        "title": "The Web as a Knowledge-Base for Answering Complex Questions",
        "local_pdf_path": "downloaded_papers\\The Web as a Knowledge-Base for Answering Complex Questions.pdf"
    },
    {
        "index": 157,
        "raw": "Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2019. CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers) . 4149–4158.",
        "doi": null,
        "year": "2019",
        "has_doi": false,
        "title_from_ref": "CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge",
        "arxiv_block": null,
        "pdf_url": "https://arxiv.org/pdf/1811.00937v2.pdf",
        "source": "arxiv_api",
        "abstract": "When answering a question, people often draw upon their rich world knowledge in addition to the particular context. Recent work has focused primarily on answering questions given some relevant document or context, and required very little general background. To investigate question answering with prior knowledge, we present CommonsenseQA: a challenging new dataset for commonsense question answering. To capture common sense beyond associations, we extract from ConceptNet (Speer et al., 2017) multiple target concepts that have the same semantic relation to a single source concept. Crowd-workers are asked to author multiple-choice questions that mention the source concept and discriminate in turn between each of the target concepts. This encourages workers to create questions with complex semantics that often require prior knowledge. We create 12,247 questions through this procedure and demonstrate the difficulty of our task with a large number of strong baselines. Our best baseline is based on BERT-large (Devlin et al., 2018) and obtains 56% accuracy, well below human performance, which is 89%.",
        "title": "CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge",
        "local_pdf_path": "downloaded_papers\\CommonsenseQA A Question Answering Challenge Targeting Commonsense Knowledge.pdf"
    },
    {
        "index": 158,
        "raw": "Dhaval Taunk, Lakshya Khanna, Siri Venkata Pavan Kumar Kandru, Vasudeva Varma, Charu Sharma, and Makarand Tapaswi. 2023. GrapeQA: GRaph Augmentation and Pruning to Enhance Question-Answering. In Companion Proceedings of the ACM Web Conference 2023, WWW 2023, Austin, TX, USA, 30 April 2023 - 4 May 2023 . 1138–1144.",
        "doi": "10.1145/3543873.3587651",
        "year": "2023",
        "has_doi": false,
        "title_from_ref": "GrapeQA: GRaph Augmentation and Pruning to Enhance Question-Answering",
        "arxiv_block": null,
        "pdf_url": "https://arxiv.org/pdf/2303.12320",
        "source": "semantic_scholar",
        "abstract": "Commonsense question-answering (QA) methods combine the power of pre-trained Language Models (LM) with the reasoning provided by Knowledge Graphs (KG). A typical approach collects nodes relevant to the QA pair from a KG to form a Working Graph (WG) followed by reasoning using Graph Neural Networks (GNNs). This faces two major challenges: (i) it is difficult to capture all the information from the QA in the WG, and (ii) the WG contains some irrelevant nodes from the KG. To address these, we propose GrapeQA with two simple improvements on the WG: (i) Prominent Entities for Graph Augmentation identifies relevant text chunks from the QA pair and augments the WG with corresponding latent representations from the LM, and (ii) Context-Aware Node Pruning removes nodes that are less relevant to the QA pair. We evaluate our results on OpenBookQA, CommonsenseQA and MedQA-USMLE and see that GrapeQA shows consistent improvements over its LM + KG predecessor (QA-GNN in particular) and large improvements on OpenBookQA.",
        "title": "GrapeQA: GRaph Augmentation and Pruning to Enhance Question-Answering",
        "local_pdf_path": "downloaded_papers\\GrapeQA GRaph Augmentation and Pruning to Enhance Question-Answering.pdf"
    },
    {
        "index": 159,
        "raw": "Kristina Toutanova, Danqi Chen, Patrick Pantel, Hoifung Poon, Pallavi Choudhury, and Michael Gamon. 2015. Representing Text for Joint Embedding of Text and Knowledge Bases. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, EMNLP 2015, Lisbon, Portugal, September 17-21, 2015 . 1499–1509.",
        "doi": "10.18653/v1/d15-1174",
        "year": "2015",
        "has_doi": false,
        "title_from_ref": "Representing Text for Joint Embedding of Text and Knowledge Bases",
        "arxiv_block": null,
        "pdf_url": "https://www.aclweb.org/anthology/D15-1174.pdf",
        "source": "openalex",
        "abstract": null,
        "title": "Representing Text for Joint Embedding of Text and Knowledge Bases",
        "local_pdf_path": "downloaded_papers\\Representing Text for Joint Embedding of Text and Knowledge Bases.pdf"
    },
    {
        "index": 160,
        "raw": "Hugo Touvron, Louis Martin, and et al. 2023. Llama 2: Open Foundation and Fine-Tuned Chat Models. arXiv:2307.09288 [cs.CL] https://arxiv.org/abs/2307.09288",
        "doi": null,
        "year": "2023",
        "has_doi": false,
        "title_from_ref": "Llama 2: Open Foundation and Fine-Tuned Chat Models",
        "arxiv_block": "arXiv:2307.09288 [cs.CL]",
        "pdf_url": "https://arxiv.org/pdf/2307.09288.pdf",
        "source": "arxiv_direct",
        "abstract": null,
        "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models",
        "local_pdf_path": "downloaded_papers\\Llama 2 Open Foundation and Fine-Tuned Chat Models.pdf"
    },
    {
        "index": 161,
        "raw": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is All you Need. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA . 5998–6008.",
        "doi": null,
        "year": "2017",
        "has_doi": false,
        "title_from_ref": "Attention is All you Need",
        "arxiv_block": null,
        "pdf_url": "https://arxiv.org/pdf/2104.04692v3.pdf",
        "source": "arxiv_api",
        "abstract": "Beyond the success story of pre-trained language models (PrLMs) in recent natural language processing, they are susceptible to over-fitting due to unusual large model size. To this end, dropout serves as a therapy. However, existing methods like random-based, knowledge-based and search-based dropout are more general but less effective onto self-attention based models, which are broadly chosen as the fundamental architecture of PrLMs. In this paper, we propose a novel dropout method named AttendOut to let self-attention empowered PrLMs capable of more robust task-specific tuning. We demonstrate that state-of-the-art models with elaborate training design may achieve much stronger results. We verify the universality of our approach on extensive natural language processing tasks.",
        "title": "Attention is All you Need",
        "local_pdf_path": "downloaded_papers\\Attention is All you Need.pdf"
    },
    {
        "index": 162,
        "raw": "Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua Bengio. 2018. Graph Attention Networks. arXiv:1710.10903 [stat.ML] https://arxiv.org/abs/1710.10903",
        "doi": null,
        "year": "2018",
        "has_doi": false,
        "title_from_ref": "Graph Attention Networks",
        "arxiv_block": "arXiv:1710.10903 [stat.ML]",
        "pdf_url": "https://arxiv.org/pdf/1710.10903.pdf",
        "source": "arxiv_direct",
        "abstract": null,
        "title": "Graph Attention Networks",
        "local_pdf_path": "downloaded_papers\\Graph Attention Networks.pdf"
    },
    {
        "index": 163,
        "raw": "Denny Vrandečić and Markus Krötzsch. 2014. Wikidata: a free collaborative knowledgebase. Commun. ACM 57, 10 (2014), 78–85.",
        "doi": "10.1016/j.jbi.2019.103292",
        "year": "2014",
        "has_doi": false,
        "title_from_ref": "Wikidata: a free collaborative knowledgebase",
        "arxiv_block": null,
        "pdf_url": "https://academic.oup.com/nar/article-pdf/49/D1/D1144/35364847/gkaa1084.pdf",
        "source": "openalex",
        "abstract": null,
        "title": "Wikidata: a free collaborative knowledgebase",
        "local_pdf_path": null
    },
    {
        "index": 164,
        "raw": "Chaojie Wang, Yishi Xu, Zhong Peng, Chenxi Zhang, Bo Chen, Xinrun Wang, Lei Feng, and Bo An. 2023. keqing: knowledge-based question answering is a nature chain-of-thought mentor of LLM. arXiv:2401.00426 [cs.CL] https://arxiv.org/abs/2401.00426",
        "doi": null,
        "year": "2023",
        "has_doi": false,
        "title_from_ref": "keqing: knowledge-based question answering is a nature chain-of-thought mentor of LLM",
        "arxiv_block": "arXiv:2401.00426 [cs.CL]",
        "pdf_url": "https://arxiv.org/pdf/2401.00426.pdf",
        "source": "arxiv_direct",
        "abstract": null,
        "title": "keqing: knowledge-based question answering is a nature chain-of-thought mentor of LLM",
        "local_pdf_path": "downloaded_papers\\keqing knowledge-based question answering is a nature chain-of-thought mentor of LLM.pdf"
    },
    {
        "index": 165,
        "raw": "Heng Wang, Shangbin Feng, Tianxing He, Zhaoxuan Tan, Xiaochuang Han, and Yulia Tsvetkov. 2023. Can Language Models Solve Graph Problems in Natural Language?. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023 .",
        "doi": "10.18653/v1/2024.emnlp-main.92",
        "year": "2023",
        "has_doi": false,
        "title_from_ref": "Can Language Models Solve Graph Problems in Natural Language?",
        "arxiv_block": null,
        "pdf_url": "https://arxiv.org/pdf/2305.10037",
        "source": "openalex",
        "abstract": null,
        "title": "Can Language Models Solve Graph Problems in Natural Language?",
        "local_pdf_path": "downloaded_papers\\Can Language Models Solve Graph Problems in Natural Language.pdf"
    },
    {
        "index": 166,
        "raw": "Jinqiang Wang, Huansheng Ning, Yi Peng, Qikai Wei, Daniel Tesfai, Wenwei Mao, Tao Zhu, and Runhe Huang. 2024. A Survey on Large Language Models from General Purpose to Medical Applications: Datasets, Methodologies, and Evaluations. arXiv:2406.10303 [cs.CL] https://arxiv.org/abs/2406.10303",
        "doi": null,
        "year": "2024",
        "has_doi": false,
        "title_from_ref": "A Survey on Large Language Models from General Purpose to Medical Applications: Datasets, Methodologies, and Evaluations",
        "arxiv_block": "arXiv:2406.10303 [cs.CL]",
        "pdf_url": "https://arxiv.org/pdf/2406.10303.pdf",
        "source": "arxiv_direct",
        "abstract": null,
        "title": "A Survey on Large Language Models from General Purpose to Medical Applications: Datasets, Methodologies, and Evaluations",
        "local_pdf_path": "downloaded_papers\\A Survey on Large Language Models from General Purpose to Medical Applications Datasets, Methodologi.pdf"
    },
    {
        "index": 167,
        "raw": "Keheng Wang, Feiyu Duan, Sirui Wang, Peiguang Li, Yunsen Xian, Chuantao Yin, Wenge Rong, and Zhang Xiong. 2023. Knowledge-Driven CoT: Exploring Faithful Reasoning in LLMs for Knowledge-intensive Question Answering. arXiv:2308.13259 [cs.CL] https://arxiv.org/abs/2308.13259",
        "doi": null,
        "year": "2023",
        "has_doi": false,
        "title_from_ref": "Knowledge-Driven CoT: Exploring Faithful Reasoning in LLMs for Knowledge-intensive Question Answering",
        "arxiv_block": "arXiv:2308.13259 [cs.CL]",
        "pdf_url": "https://arxiv.org/pdf/2308.13259.pdf",
        "source": "arxiv_direct",
        "abstract": null,
        "title": "Knowledge-Driven CoT: Exploring Faithful Reasoning in LLMs for Knowledge-intensive Question Answering",
        "local_pdf_path": "downloaded_papers\\Knowledge-Driven CoT Exploring Faithful Reasoning in LLMs for Knowledge-intensive Question Answering.pdf"
    },
    {
        "index": 168,
        "raw": "Ruijie Wang, Zheng Li, Danqing Zhang, Qingyu Yin, Tong Zhao, Bing Yin, and Tarek F. Abdelzaher. 2022. RETE: Retrieval-Enhanced Temporal Event Forecasting on Unified Query Product Evolutionary Graph. In WWW ’22: The ACM Web Conference 2022, Virtual Event, Lyon, France, April 25 - 29, 2022 . 462–472.",
        "doi": null,
        "year": "2022",
        "has_doi": false,
        "title_from_ref": "RETE: Retrieval-Enhanced Temporal Event Forecasting on Unified Query Product Evolutionary Graph",
        "arxiv_block": null,
        "pdf_url": "https://arxiv.org/pdf/2202.06129v1.pdf",
        "source": "arxiv_api",
        "abstract": "With the increasing demands on e-commerce platforms, numerous user action history is emerging. Those enriched action records are vital to understand users' interests and intents. Recently, prior works for user behavior prediction mainly focus on the interactions with product-side information. However, the interactions with search queries, which usually act as a bridge between users and products, are still under investigated. In this paper, we explore a new problem named temporal event forecasting, a generalized user behavior prediction task in a unified query product evolutionary graph, to embrace both query and product recommendation in a temporal manner. To fulfill this setting, there involves two challenges: (1) the action data for most users is scarce; (2) user preferences are dynamically evolving and shifting over time. To tackle those issues, we propose a novel Retrieval-Enhanced Temporal Event (RETE) forecasting framework. Unlike existing methods that enhance user representations via roughly absorbing information from connected entities in the whole graph, RETE efficiently and dynamically retrieves relevant entities centrally on each user as high-quality subgraphs, preventing the noise propagation from the densely evolutionary graph structures that incorporate abundant search queries. And meanwhile, RETE autoregressively accumulates retrieval-enhanced user representations from each time step, to capture evolutionary patterns for joint query and product prediction. Empirically, extensive experiments on both the public benchmark and four real-world industrial datasets demonstrate the effectiveness of the proposed RETE method.",
        "title": "RETE: Retrieval-Enhanced Temporal Event Forecasting on Unified Query Product Evolutionary Graph",
        "local_pdf_path": "downloaded_papers\\RETE Retrieval-Enhanced Temporal Event Forecasting on Unified Query Product Evolutionary Graph.pdf"
    },
    {
        "index": 169,
        "raw": "Shen Wang, Tianlong Xu, Hang Li, Chaoli Zhang, Joleen Liang, Jiliang Tang, Philip S. Yu, and Qingsong Wen. 2024. Large Language Models for Education: A Survey and Outlook. arXiv:2403.18105 [cs.CL] https://arxiv.org/abs/2403. 18105",
        "doi": null,
        "year": "2024",
        "has_doi": false,
        "title_from_ref": "Large Language Models for Education: A Survey and Outlook",
        "arxiv_block": "arXiv:2403.18105 [cs.CL]",
        "pdf_url": "https://arxiv.org/pdf/2403.18105.pdf",
        "source": "arxiv_direct",
        "abstract": null,
        "title": "Large Language Models for Education: A Survey and Outlook",
        "local_pdf_path": "downloaded_papers\\Large Language Models for Education A Survey and Outlook.pdf"
    },
    {
        "index": 170,
        "raw": "Xintao Wang, Qianwen Yang, Yongting Qiu, Jiaqing Liang, Qianyu He, Zhouhong Gu, Yanghua Xiao, and Wei Wang. 2023. KnowledGPT: Enhancing Large Language Models with Retrieval and Storage Access on Knowledge Bases. arXiv:2308.11761 [cs.CL] https://arxiv.org/abs/2308.11761",
        "doi": null,
        "year": "2023",
        "has_doi": false,
        "title_from_ref": "KnowledGPT: Enhancing Large Language Models with Retrieval and Storage Access on Knowledge Bases",
        "arxiv_block": "arXiv:2308.11761 [cs.CL]",
        "pdf_url": "https://arxiv.org/pdf/2308.11761.pdf",
        "source": "arxiv_direct",
        "abstract": null,
        "title": "KnowledGPT: Enhancing Large Language Models with Retrieval and Storage Access on Knowledge Bases",
        "local_pdf_path": "downloaded_papers\\KnowledGPT Enhancing Large Language Models with Retrieval and Storage Access on Knowledge Bases.pdf"
    },
    {
        "index": 171,
        "raw": "Yuqi Wang, Boran Jiang, Yi Luo, Dawei He, Peng Cheng, and Liangcai Gao. 2024. Reasoning on Efficient Knowledge Paths:Knowledge Graph Guides Large Language Model for Domain Question Answering. arXiv:2404.10384 [cs.CL] https://arxiv.org/abs/2404.10384",
        "doi": null,
        "year": "2024",
        "has_doi": false,
        "title_from_ref": "Reasoning on Efficient Knowledge Paths:Knowledge Graph Guides Large Language Model for Domain Question Answering",
        "arxiv_block": "arXiv:2404.10384 [cs.CL]",
        "pdf_url": "https://arxiv.org/pdf/2404.10384.pdf",
        "source": "arxiv_direct",
        "abstract": null,
        "title": "Reasoning on Efficient Knowledge Paths:Knowledge Graph Guides Large Language Model for Domain Question Answering",
        "local_pdf_path": "downloaded_papers\\Reasoning on Efficient Knowledge PathsKnowledge Graph Guides Large Language Model for Domain Questio.pdf"
    },
    {
        "index": 172,
        "raw": "Yu Wang, Nedim Lipka, Ryan A. Rossi, Alexa F. Siu, Ruiyi Zhang, and Tyler Derr. 2024. Knowledge Graph Prompting for Multi-Document Question Answering. In Thirty-Eighth AAAI Conference on Artificial Intelligence, AAAI 2024, Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence, IAAI 2024, Fourteenth Symposium on Educational Advances in Artificial Intelligence, EAAI 2014, February 20-27, 2024, Vancouver, Canada . 19206–19214. J. ACM, Vol. 37, No. 4, Article 111. Publication date: September 2024. 111:40 Peng et al.",
        "doi": null,
        "year": "2024",
        "has_doi": false,
        "title_from_ref": "Knowledge Graph Prompting for Multi-Document Question Answering",
        "arxiv_block": null,
        "pdf_url": "https://arxiv.org/pdf/2308.11730v3.pdf",
        "source": "arxiv_api",
        "abstract": "The `pre-train, prompt, predict' paradigm of large language models (LLMs) has achieved remarkable success in open-domain question answering (OD-QA). However, few works explore this paradigm in the scenario of multi-document question answering (MD-QA), a task demanding a thorough understanding of the logical associations among the contents and structures of different documents. To fill this crucial gap, we propose a Knowledge Graph Prompting (KGP) method to formulate the right context in prompting LLMs for MD-QA, which consists of a graph construction module and a graph traversal module. For graph construction, we create a knowledge graph (KG) over multiple documents with nodes symbolizing passages or document structures (e.g., pages/tables), and edges denoting the semantic/lexical similarity between passages or intra-document structural relations. For graph traversal, we design an LLM-based graph traversal agent that navigates across nodes and gathers supporting passages assisting LLMs in MD-QA. The constructed graph serves as the global ruler that regulates the transitional space among passages and reduces retrieval latency. Concurrently, the graph traversal agent acts as a local navigator that gathers pertinent context to progressively approach the question and guarantee retrieval quality. Extensive experiments underscore the efficacy of KGP for MD-QA, signifying the potential of leveraging graphs in enhancing the prompt design for LLMs. Our code: https://github.com/YuWVandy/KG-LLM-MDQA.",
        "title": "Knowledge Graph Prompting for Multi-Document Question Answering",
        "local_pdf_path": "downloaded_papers\\Knowledge Graph Prompting for Multi-Document Question Answering.pdf"
    },
    {
        "index": 173,
        "raw": "Yaoke Wang, Yun Zhu, Wenqiao Zhang, Yueting Zhuang, Yunfei Li, and Siliang Tang. 2024. Bridging Local Details and Global Context in Text-Attributed Graphs. arXiv:2406.12608 [cs.CL] https://arxiv.org/abs/2406.12608",
        "doi": null,
        "year": "2024",
        "has_doi": false,
        "title_from_ref": "Bridging Local Details and Global Context in Text-Attributed Graphs",
        "arxiv_block": "arXiv:2406.12608 [cs.CL]",
        "pdf_url": "https://arxiv.org/pdf/2406.12608.pdf",
        "source": "arxiv_direct",
        "abstract": null,
        "title": "Bridging Local Details and Global Context in Text-Attributed Graphs",
        "local_pdf_path": "downloaded_papers\\Bridging Local Details and Global Context in Text-Attributed Graphs.pdf"
    },
    {
        "index": 174,
        "raw": "Yinwei Wei, Xiang Wang, Liqiang Nie, Xiangnan He, Richang Hong, and Tat-Seng Chua. 2019. MMGCN: Multimodal graph convolution network for personalized recommendation of micro-video. In Proceedings of the 27th ACM international conference on multimedia . 1437–1445.",
        "doi": "10.1109/tmm.2022.3151026",
        "year": "2019",
        "has_doi": false,
        "title_from_ref": "MMGCN: Multimodal graph convolution network for personalized recommendation of micro-video",
        "arxiv_block": null,
        "pdf_url": null,
        "source": "semantic_scholar",
        "abstract": "Personalized recommendation plays a central role in many online content sharing platforms. To provide quality micro-video recommendation service, it is of crucial importance to consider the interactions between users and items (i.e. micro-videos) as well as the item contents from various modalities (e.g. visual, acoustic, and textual). Existing works on multimedia recommendation largely exploit multi-modal contents to enrich item representations, while less effort is made to leverage information interchange between users and items to enhance user representations and further capture user's fine-grained preferences on different modalities. In this paper, we propose to exploit user-item interactions to guide the representation learning in each modality, and further personalized micro-video recommendation. We design a Multi-modal Graph Convolution Network (MMGCN) framework built upon the message-passing idea of graph neural networks, which can yield modal-specific representations of users and micro-videos to better capture user preferences. Specifically, we construct a user-item bipartite graph in each modality, and enrich the representation of each node with the topological structure and features of its neighbors. Through extensive experiments on three publicly available datasets, Tiktok, Kwai, and MovieLens, we demonstrate that our proposed model is able to significantly outperform state-of-the-art multi-modal recommendation methods.",
        "title": "MMGCN: Multimodal graph convolution network for personalized recommendation of micro-video",
        "local_pdf_path": null
    },
    {
        "index": 175,
        "raw": "Yilin Wen, Zifeng Wang, and Jimeng Sun. 2024. MindMap: Knowledge Graph Prompting Sparks Graph of Thoughts in Large Language Models. arXiv:2308.09729 [cs.AI] https://arxiv.org/abs/2308.09729",
        "doi": null,
        "year": "2024",
        "has_doi": false,
        "title_from_ref": "MindMap: Knowledge Graph Prompting Sparks Graph of Thoughts in Large Language Models",
        "arxiv_block": "arXiv:2308.09729 [cs.AI]",
        "pdf_url": "https://arxiv.org/pdf/2308.09729.pdf",
        "source": "arxiv_direct",
        "abstract": null,
        "title": "MindMap: Knowledge Graph Prompting Sparks Graph of Thoughts in Large Language Models",
        "local_pdf_path": "downloaded_papers\\MindMap Knowledge Graph Prompting Sparks Graph of Thoughts in Large Language Models.pdf"
    },
    {
        "index": 176,
        "raw": "Sondre Wold, Lilja Øvrelid, and Erik Velldal. 2023. Text-To-KG Alignment: Comparing Current Methods on Classification Tasks. arXiv:2306.02871 [cs.CL] https://arxiv.org/abs/2306.02871",
        "doi": null,
        "year": "2023",
        "has_doi": false,
        "title_from_ref": "Text-To-KG Alignment: Comparing Current Methods on Classification Tasks",
        "arxiv_block": "arXiv:2306.02871 [cs.CL]",
        "pdf_url": "https://arxiv.org/pdf/2306.02871.pdf",
        "source": "arxiv_direct",
        "abstract": null,
        "title": "Text-To-KG Alignment: Comparing Current Methods on Classification Tasks",
        "local_pdf_path": "downloaded_papers\\Text-To-KG Alignment Comparing Current Methods on Classification Tasks.pdf"
    },
    {
        "index": 177,
        "raw": "Junde Wu, Jiayuan Zhu, and Yunli Qi. 2024. Medical Graph RAG: Towards Safe Medical Large Language Model via Graph Retrieval-Augmented Generation. arXiv:2408.04187 [cs.CV] https://arxiv.org/abs/2408.04187",
        "doi": null,
        "year": "2024",
        "has_doi": false,
        "title_from_ref": "Medical Graph RAG: Towards Safe Medical Large Language Model via Graph Retrieval-Augmented Generation",
        "arxiv_block": "arXiv:2408.04187 [cs.CV]",
        "pdf_url": "https://arxiv.org/pdf/2408.04187.pdf",
        "source": "arxiv_direct",
        "abstract": null,
        "title": "Medical Graph RAG: Towards Safe Medical Large Language Model via Graph Retrieval-Augmented Generation",
        "local_pdf_path": "downloaded_papers\\Medical Graph RAG Towards Safe Medical Large Language Model via Graph Retrieval-Augmented Generation.pdf"
    },
    {
        "index": 178,
        "raw": "Shangyu Wu, Ying Xiong, Yufei Cui, Haolun Wu, Can Chen, Ye Yuan, Lianming Huang, Xue Liu, Tei-Wei Kuo, Nan Guan, and Chun Jason Xue. 2024. Retrieval-Augmented Generation for Natural Language Processing: A Survey. arXiv:2407.13193 [cs.CL] https://arxiv.org/abs/2407.13193",
        "doi": null,
        "year": "2024",
        "has_doi": false,
        "title_from_ref": "Retrieval-Augmented Generation for Natural Language Processing: A Survey",
        "arxiv_block": "arXiv:2407.13193 [cs.CL]",
        "pdf_url": "https://arxiv.org/pdf/2407.13193.pdf",
        "source": "arxiv_direct",
        "abstract": null,
        "title": "Retrieval-Augmented Generation for Natural Language Processing: A Survey",
        "local_pdf_path": "downloaded_papers\\Retrieval-Augmented Generation for Natural Language Processing A Survey.pdf"
    },
    {
        "index": 179,
        "raw": "Shirley Wu, Shiyu Zhao, Michihiro Yasunaga, Kexin Huang, Kaidi Cao, Qian Huang, Vassilis N. Ioannidis, Karthik Subbian, James Zou, and Jure Leskovec. 2024. STaRK: Benchmarking LLM Retrieval on Textual and Relational Knowledge Bases. arXiv:2404.13207 [cs.IR] https://arxiv.org/abs/2404.13207",
        "doi": null,
        "year": "2024",
        "has_doi": false,
        "title_from_ref": "STaRK: Benchmarking LLM Retrieval on Textual and Relational Knowledge Bases",
        "arxiv_block": "arXiv:2404.13207 [cs.IR]",
        "pdf_url": "https://arxiv.org/pdf/2404.13207.pdf",
        "source": "arxiv_direct",
        "abstract": null,
        "title": "STaRK: Benchmarking LLM Retrieval on Textual and Relational Knowledge Bases",
        "local_pdf_path": "downloaded_papers\\STaRK Benchmarking LLM Retrieval on Textual and Relational Knowledge Bases.pdf"
    },
    {
        "index": 180,
        "raw": "Taiqiang Wu, Xingyu Bai, Weigang Guo, Weijie Liu, Siheng Li, and Yujiu Yang. 2023. Modeling Fine-grained Information via Knowledge-aware Hierarchical Graph for Zero-shot Entity Retrieval. In Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining, WSDM 2023, Singapore, 27 February 2023 - 3 March 2023. 1021–1029.",
        "doi": null,
        "year": "2023",
        "has_doi": false,
        "title_from_ref": "Modeling Fine-grained Information via Knowledge-aware Hierarchical Graph for Zero-shot Entity Retrieval",
        "arxiv_block": null,
        "pdf_url": "https://arxiv.org/pdf/2211.10991v1.pdf",
        "source": "arxiv_api",
        "abstract": "Zero-shot entity retrieval, aiming to link mentions to candidate entities under the zero-shot setting, is vital for many tasks in Natural Language Processing. Most existing methods represent mentions/entities via the sentence embeddings of corresponding context from the Pre-trained Language Model. However, we argue that such coarse-grained sentence embeddings can not fully model the mentions/entities, especially when the attention scores towards mentions/entities are relatively low. In this work, we propose GER, a \\textbf{G}raph enhanced \\textbf{E}ntity \\textbf{R}etrieval framework, to capture more fine-grained information as complementary to sentence embeddings. We extract the knowledge units from the corresponding context and then construct a mention/entity centralized graph. Hence, we can learn the fine-grained information about mention/entity by aggregating information from these knowledge units. To avoid the graph information bottleneck for the central mention/entity node, we construct a hierarchical graph and design a novel Hierarchical Graph Attention Network~(HGAN). Experimental results on popular benchmarks demonstrate that our proposed GER framework performs better than previous state-of-the-art models. The code has been available at https://github.com/wutaiqiang/GER-WSDM2023.",
        "title": "Modeling Fine-grained Information via Knowledge-aware Hierarchical Graph for Zero-shot Entity Retrieval",
        "local_pdf_path": "downloaded_papers\\Modeling Fine-grained Information via Knowledge-aware Hierarchical Graph for Zero-shot Entity Retrie.pdf"
    },
    {
        "index": 181,
        "raw": "Yuxia Wu, Yuan Fang, and Lizi Liao. 2024. Retrieval Augmented Generation for Dynamic Graph Modeling. arXiv:2408.14523 [cs.LG] https://arxiv.org/abs/2408.14523",
        "doi": null,
        "year": "2024",
        "has_doi": false,
        "title_from_ref": "Retrieval Augmented Generation for Dynamic Graph Modeling",
        "arxiv_block": "arXiv:2408.14523 [cs.LG]",
        "pdf_url": "https://arxiv.org/pdf/2408.14523.pdf",
        "source": "arxiv_direct",
        "abstract": null,
        "title": "Retrieval Augmented Generation for Dynamic Graph Modeling",
        "local_pdf_path": "downloaded_papers\\Retrieval Augmented Generation for Dynamic Graph Modeling.pdf"
    },
    {
        "index": 182,
        "raw": "Yike Wu, Nan Hu, Sheng Bi, Guilin Qi, Jie Ren, Anhuan Xie, and Wei Song. 2023. Retrieve-Rewrite-Answer: A KG-to-Text Enhanced LLMs Framework for Knowledge Graph Question Answering. arXiv:2309.11206 [cs.CL] https://arxiv.org/abs/2309.11206",
        "doi": null,
        "year": "2023",
        "has_doi": false,
        "title_from_ref": "Retrieve-Rewrite-Answer: A KG-to-Text Enhanced LLMs Framework for Knowledge Graph Question Answering",
        "arxiv_block": "arXiv:2309.11206 [cs.CL]",
        "pdf_url": "https://arxiv.org/pdf/2309.11206.pdf",
        "source": "arxiv_direct",
        "abstract": null,
        "title": "Retrieve-Rewrite-Answer: A KG-to-Text Enhanced LLMs Framework for Knowledge Graph Question Answering",
        "local_pdf_path": "downloaded_papers\\Retrieve-Rewrite-Answer A KG-to-Text Enhanced LLMs Framework for Knowledge Graph Question Answering.pdf"
    },
    {
        "index": 183,
        "raw": "Zhentao Xu, Mark Jerome Cruz, Matthew Guevara, Tie Wang, Manasi Deshpande, Xiaofeng Wang, and Zheng Li. 2024. Retrieval-Augmented Generation with Knowledge Graphs for Customer Service Question Answering. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2024, Washington DC, USA, July 14-18, 2024 . 2905–2909.",
        "doi": null,
        "year": "2024",
        "has_doi": false,
        "title_from_ref": "Retrieval-Augmented Generation with Knowledge Graphs for Customer Service Question Answering",
        "arxiv_block": null,
        "pdf_url": "https://arxiv.org/pdf/2404.17723v2.pdf",
        "source": "arxiv_api",
        "abstract": "In customer service technical support, swiftly and accurately retrieving relevant past issues is critical for efficiently resolving customer inquiries. The conventional retrieval methods in retrieval-augmented generation (RAG) for large language models (LLMs) treat a large corpus of past issue tracking tickets as plain text, ignoring the crucial intra-issue structure and inter-issue relations, which limits performance. We introduce a novel customer service question-answering method that amalgamates RAG with a knowledge graph (KG). Our method constructs a KG from historical issues for use in retrieval, retaining the intra-issue structure and inter-issue relations. During the question-answering phase, our method parses consumer queries and retrieves related sub-graphs from the KG to generate answers. This integration of a KG not only improves retrieval accuracy by preserving customer service structure information but also enhances answering quality by mitigating the effects of text segmentation. Empirical assessments on our benchmark datasets, utilizing key retrieval (MRR, Recall@K, NDCG@K) and text generation (BLEU, ROUGE, METEOR) metrics, reveal that our method outperforms the baseline by 77.6% in MRR and by 0.32 in BLEU. Our method has been deployed within LinkedIn's customer service team for approximately six months and has reduced the median per-issue resolution time by 28.6%.",
        "title": "Retrieval-Augmented Generation with Knowledge Graphs for Customer Service Question Answering",
        "local_pdf_path": "downloaded_papers\\Retrieval-Augmented Generation with Knowledge Graphs for Customer Service Question Answering.pdf"
    },
    {
        "index": 184,
        "raw": "An Yang, Baosong Yang, and et al. 2024. Qwen2 Technical Report. arXiv:2407.10671 [cs.CL] https://arxiv.org/abs/ 2407.10671",
        "doi": null,
        "year": "2024",
        "has_doi": false,
        "title_from_ref": "Qwen2 Technical Report",
        "arxiv_block": "arXiv:2407.10671 [cs.CL]",
        "pdf_url": "https://arxiv.org/pdf/2407.10671.pdf",
        "source": "arxiv_direct",
        "abstract": null,
        "title": "Qwen2 Technical Report",
        "local_pdf_path": "downloaded_papers\\Qwen2 Technical Report.pdf"
    },
    {
        "index": 185,
        "raw": "Rui Yang, Haoran Liu, Edison Marrese-Taylor, Qingcheng Zeng, Yu He Ke, Wanxin Li, Lechao Cheng, Qingyu Chen, James Caverlee, Yutaka Matsuo, and Irene Li. 2024. KG-Rank: Enhancing Large Language Models for Medical QA with Knowledge Graphs and Ranking Techniques. arXiv:2403.05881 [cs.CL] https://arxiv.org/abs/2403.05881",
        "doi": null,
        "year": "2024",
        "has_doi": false,
        "title_from_ref": "KG-Rank: Enhancing Large Language Models for Medical QA with Knowledge Graphs and Ranking Techniques",
        "arxiv_block": "arXiv:2403.05881 [cs.CL]",
        "pdf_url": "https://arxiv.org/pdf/2403.05881.pdf",
        "source": "arxiv_direct",
        "abstract": null,
        "title": "KG-Rank: Enhancing Large Language Models for Medical QA with Knowledge Graphs and Ranking Techniques",
        "local_pdf_path": "downloaded_papers\\KG-Rank Enhancing Large Language Models for Medical QA with Knowledge Graphs and Ranking Techniques.pdf"
    },
    {
        "index": 186,
        "raw": "Xiao Yang, Kai Sun, Hao Xin, Yushi Sun, Nikita Bhalla, Xiangsen Chen, Sajal Choudhary, Rongze Daniel Gui, Ziran Will Jiang, Ziyu Jiang, Lingkun Kong, Brian Moran, Jiaqi Wang, Yifan Ethan Xu, An Yan, Chenyu Yang, Eting Yuan, Hanwen Zha, Nan Tang, Lei Chen, Nicolas Scheffer, Yue Liu, Nirav Shah, Rakesh Wanga, Anuj Kumar, Wen tau Yih, and Xin Luna Dong. 2024. CRAG – Comprehensive RAG Benchmark. arXiv:2406.04744 [cs.CL] https://arxiv.org/abs/2406.04744",
        "doi": null,
        "year": "2024",
        "has_doi": false,
        "title_from_ref": "CRAG – Comprehensive RAG Benchmark",
        "arxiv_block": "arXiv:2406.04744 [cs.CL]",
        "pdf_url": "https://arxiv.org/pdf/2406.04744.pdf",
        "source": "arxiv_direct",
        "abstract": null,
        "title": "CRAG – Comprehensive RAG Benchmark",
        "local_pdf_path": "downloaded_papers\\CRAG – Comprehensive RAG Benchmark.pdf"
    },
    {
        "index": 187,
        "raw": "Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018. 2369–2380.",
        "doi": null,
        "year": "2018",
        "has_doi": false,
        "title_from_ref": "HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering",
        "arxiv_block": null,
        "pdf_url": "https://arxiv.org/pdf/1809.09600v1.pdf",
        "source": "arxiv_api",
        "abstract": "Existing question answering (QA) datasets fail to train QA systems to perform complex reasoning and provide explanations for answers. We introduce HotpotQA, a new dataset with 113k Wikipedia-based question-answer pairs with four key features: (1) the questions require finding and reasoning over multiple supporting documents to answer; (2) the questions are diverse and not constrained to any pre-existing knowledge bases or knowledge schemas; (3) we provide sentence-level supporting facts required for reasoning, allowing QA systems to reason with strong supervision and explain the predictions; (4) we offer a new type of factoid comparison questions to test QA systems' ability to extract relevant facts and perform necessary comparison. We show that HotpotQA is challenging for the latest QA systems, and the supporting facts enable models to improve performance and make explainable predictions.",
        "title": "HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering",
        "local_pdf_path": "downloaded_papers\\HotpotQA A Dataset for Diverse, Explainable Multi-hop Question Answering.pdf"
    },
    {
        "index": 188,
        "raw": "Mohammad Yani and Adila Alfa Krisnadhi. 2021. Challenges, Techniques, and Trends of Simple Knowledge Graph Question Answering: A Survey. Inf.12, 7 (2021), 271.",
        "doi": "10.3390/info12070271",
        "year": "2021",
        "has_doi": false,
        "title_from_ref": "Challenges, Techniques, and Trends of Simple Knowledge Graph Question Answering: A Survey",
        "arxiv_block": null,
        "pdf_url": "https://www.mdpi.com/2078-2489/12/7/271/pdf?version=1625454305",
        "source": "semantic_scholar",
        "abstract": "Simple questions are the most common type of questions used for evaluating a knowledge graph question answering (KGQA). A simple question is a question whose answer can be captured by a factoid statement with one relation or predicate. Knowledge graph question answering (KGQA) systems are systems whose aim is to automatically answer natural language questions (NLQs) over knowledge graphs (KGs). There are varieties of researches with different approaches in this area. However, the lack of a comprehensive study to focus on addressing simple questions from all aspects is tangible. In this paper, we present a comprehensive survey of answering simple questions to classify available techniques and compare their advantages and drawbacks in order to have better insights of existing issues and recommendations to direct future works.",
        "title": "Challenges, Techniques, and Trends of Simple Knowledge Graph Question Answering: A Survey",
        "local_pdf_path": null
    },
    {
        "index": 189,
        "raw": "Michihiro Yasunaga, Hongyu Ren, Antoine Bosselut, Percy Liang, and Jure Leskovec. 2021. QA-GNN: Reasoning with Language Models and Knowledge Graphs for Question Answering. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021 . 535–546.",
        "doi": "10.18653/v1/2021.naacl-main.45",
        "year": "2021",
        "has_doi": false,
        "title_from_ref": "QA-GNN: Reasoning with Language Models and Knowledge Graphs for Question Answering",
        "arxiv_block": null,
        "pdf_url": "https://aclanthology.org/2021.naacl-main.45.pdf",
        "source": "semantic_scholar",
        "abstract": "The problem of answering questions using knowledge from pre-trained language models (LMs) and knowledge graphs (KGs) presents two challenges: given a QA context (question and answer choice), methods need to (i) identify relevant knowledge from large KGs, and (ii) perform joint reasoning over the QA context and KG. Here we propose a new model, QA-GNN, which addresses the above challenges through two key innovations: (i) relevance scoring, where we use LMs to estimate the importance of KG nodes relative to the given QA context, and (ii) joint reasoning, where we connect the QA context and KG to form a joint graph, and mutually update their representations through graph-based message passing. We evaluate QA-GNN on the CommonsenseQA and OpenBookQA datasets, and show its improvement over existing LM and LM+KG models, as well as its capability to perform interpretable and structured reasoning, e.g., correctly handling negation in questions.",
        "title": "QA-GNN: Reasoning with Language Models and Knowledge Graphs for Question Answering",
        "local_pdf_path": "downloaded_papers\\QA-GNN Reasoning with Language Models and Knowledge Graphs for Question Answering.pdf"
    },
    {
        "index": 190,
        "raw": "Ruosong Ye, Caiqi Zhang, Runhui Wang, Shuyuan Xu, and Yongfeng Zhang. 2024. Language is All a Graph Needs. arXiv:2308.07134 [cs.CL] https://arxiv.org/abs/2308.07134",
        "doi": null,
        "year": "2024",
        "has_doi": false,
        "title_from_ref": "Language is All a Graph Needs",
        "arxiv_block": "arXiv:2308.07134 [cs.CL]",
        "pdf_url": "https://arxiv.org/pdf/2308.07134.pdf",
        "source": "arxiv_direct",
        "abstract": null,
        "title": "Language is All a Graph Needs",
        "local_pdf_path": "downloaded_papers\\Language is All a Graph Needs.pdf"
    },
    {
        "index": 191,
        "raw": "Xi Ye, Semih Yavuz, Kazuma Hashimoto, Yingbo Zhou, and Caiming Xiong. 2021. Rng-kbqa: Generation augmented iterative ranking for knowledge base question answering. arXiv preprint arXiv:2109.08678 (2021).",
        "doi": null,
        "year": "2021",
        "has_doi": false,
        "title_from_ref": "Rng-kbqa: Generation augmented iterative ranking for knowledge base question answering",
        "arxiv_block": null,
        "pdf_url": "https://arxiv.org/pdf/2109.08678.pdf",
        "source": "arxiv_direct",
        "abstract": null,
        "title": "Rng-kbqa: Generation augmented iterative ranking for knowledge base question answering",
        "local_pdf_path": "downloaded_papers\\Rng-kbqa Generation augmented iterative ranking for knowledge base question answering.pdf"
    },
    {
        "index": 192,
        "raw": "Wen-tau Yih, Matthew Richardson, Christopher Meek, Ming-Wei Chang, and Jina Suh. 2016. The Value of Semantic Parse Labeling for Knowledge Base Question Answering. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany, Volume 2: Short Papers . J. ACM, Vol. 37, No. 4, Article 111. Publication date: September 2024. Graph Retrieval-Augmented Generation: A Survey 111:41",
        "doi": "10.18653/v1/p16-2033",
        "year": "2016",
        "has_doi": false,
        "title_from_ref": "The Value of Semantic Parse Labeling for Knowledge Base Question Answering",
        "arxiv_block": null,
        "pdf_url": null,
        "source": "openalex",
        "abstract": null,
        "title": "The Value of Semantic Parse Labeling for Knowledge Base Question Answering",
        "local_pdf_path": null
    },
    {
        "index": 193,
        "raw": "Donghan Yu, Sheng Zhang, Patrick Ng, Henghui Zhu, Alexander Hanbo Li, Jun Wang, Yiqun Hu, William Yang Wang, Zhiguo Wang, and Bing Xiang. 2023. DecAF: Joint Decoding of Answers and Logical Forms for Question Answering over Knowledge Bases. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023 .",
        "doi": "10.1109/mis.2015.70",
        "year": "2023",
        "has_doi": false,
        "title_from_ref": "DecAF: Joint Decoding of Answers and Logical Forms for Question Answering over Knowledge Bases",
        "arxiv_block": null,
        "pdf_url": "http://arxiv.org/pdf/2210.00063",
        "source": "semantic_scholar",
        "abstract": "Question answering over knowledge bases (KBs) aims to answer natural language questions with factual information such as entities and relations in KBs. Previous methods either generate logical forms that can be executed over KBs to obtain final answers or predict answers directly. Empirical results show that the former often produces more accurate answers, but it suffers from non-execution issues due to potential syntactic and semantic errors in the generated logical forms. In this work, we propose a novel framework DecAF that jointly generates both logical forms and direct answers, and then combines the merits of them to get the final answers. Moreover, different from most of the previous methods, DecAF is based on simple free-text retrieval without relying on any entity linking tools -- this simplification eases its adaptation to different datasets. DecAF achieves new state-of-the-art accuracy on WebQSP, FreebaseQA, and GrailQA benchmarks, while getting competitive results on the ComplexWebQuestions benchmark.",
        "title": "DecAF: Joint Decoding of Answers and Logical Forms for Question Answering over Knowledge Bases",
        "local_pdf_path": "downloaded_papers\\DecAF Joint Decoding of Answers and Logical Forms for Question Answering over Knowledge Bases.pdf"
    },
    {
        "index": 194,
        "raw": "Donghan Yu, Chenguang Zhu, Yuwei Fang, Wenhao Yu, Shuohang Wang, Yichong Xu, Xiang Ren, Yiming Yang, and Michael Zeng. 2022. KG-FiD: Infusing Knowledge Graph in Fusion-in-Decoder for Open-Domain Question Answering. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022 . 4961–4974.",
        "doi": null,
        "year": "2022",
        "has_doi": false,
        "title_from_ref": "KG-FiD: Infusing Knowledge Graph in Fusion-in-Decoder for Open-Domain Question Answering",
        "arxiv_block": null,
        "pdf_url": "https://arxiv.org/pdf/2110.04330v2.pdf",
        "source": "arxiv_api",
        "abstract": "Current Open-Domain Question Answering (ODQA) model paradigm often contains a retrieving module and a reading module. Given an input question, the reading module predicts the answer from the relevant passages which are retrieved by the retriever. The recent proposed Fusion-in-Decoder (FiD), which is built on top of the pretrained generative model T5, achieves the state-of-the-art performance in the reading module. Although being effective, it remains constrained by inefficient attention on all retrieved passages which contain a lot of noise. In this work, we propose a novel method KG-FiD, which filters noisy passages by leveraging the structural relationship among the retrieved passages with a knowledge graph. We initiate the passage node embedding from the FiD encoder and then use graph neural network (GNN) to update the representation for reranking. To improve the efficiency, we build the GNN on top of the intermediate layer output of the FiD encoder and only pass a few top reranked passages into the higher layers of encoder and decoder for answer generation. We also apply the proposed GNN based reranking method to enhance the passage retrieval results in the retrieving module. Extensive experiments on common ODQA benchmark datasets (Natural Question and TriviaQA) demonstrate that KG-FiD can improve vanilla FiD by up to 1.5% on answer exact match score and achieve comparable performance with FiD with only 40% of computation cost.",
        "title": "KG-FiD: Infusing Knowledge Graph in Fusion-in-Decoder for Open-Domain Question Answering",
        "local_pdf_path": "downloaded_papers\\KG-FiD Infusing Knowledge Graph in Fusion-in-Decoder for Open-Domain Question Answering.pdf"
    },
    {
        "index": 195,
        "raw": "Hao Yu, Aoran Gan, Kai Zhang, Shiwei Tong, Qi Liu, and Zhaofeng Liu. 2024. Evaluation of Retrieval-Augmented Generation: A Survey. arXiv:2405.07437 [cs.CL] https://arxiv.org/abs/2405.07437",
        "doi": null,
        "year": "2024",
        "has_doi": false,
        "title_from_ref": "Evaluation of Retrieval-Augmented Generation: A Survey",
        "arxiv_block": "arXiv:2405.07437 [cs.CL]",
        "pdf_url": "https://arxiv.org/pdf/2405.07437.pdf",
        "source": "arxiv_direct",
        "abstract": null,
        "title": "Evaluation of Retrieval-Augmented Generation: A Survey",
        "local_pdf_path": "downloaded_papers\\Evaluation of Retrieval-Augmented Generation A Survey.pdf"
    },
    {
        "index": 196,
        "raw": "Jing Zhang, Xiaokang Zhang, Jifan Yu, Jian Tang, Jie Tang, Cuiping Li, and Hong Chen. 2022. Subgraph Retrieval Enhanced Model for Multi-hop Knowledge Base Question Answering. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022 . 5773–5784.",
        "doi": null,
        "year": "2022",
        "has_doi": false,
        "title_from_ref": "Subgraph Retrieval Enhanced Model for Multi-hop Knowledge Base Question Answering",
        "arxiv_block": null,
        "pdf_url": "https://arxiv.org/pdf/2202.13296v2.pdf",
        "source": "arxiv_api",
        "abstract": "Recent works on knowledge base question answering (KBQA) retrieve subgraphs for easier reasoning. A desired subgraph is crucial as a small one may exclude the answer but a large one might introduce more noises. However, the existing retrieval is either heuristic or interwoven with the reasoning, causing reasoning on the partial subgraphs, which increases the reasoning bias when the intermediate supervision is missing. This paper proposes a trainable subgraph retriever (SR) decoupled from the subsequent reasoning process, which enables a plug-and-play framework to enhance any subgraph-oriented KBQA model. Extensive experiments demonstrate SR achieves significantly better retrieval and QA performance than existing retrieval methods. Via weakly supervised pre-training as well as the end-to-end fine-tuning, SRl achieves new state-of-the-art performance when combined with NSM, a subgraph-oriented reasoner, for embedding-based KBQA methods.",
        "title": "Subgraph Retrieval Enhanced Model for Multi-hop Knowledge Base Question Answering",
        "local_pdf_path": "downloaded_papers\\Subgraph Retrieval Enhanced Model for Multi-hop Knowledge Base Question Answering.pdf"
    },
    {
        "index": 197,
        "raw": "Mengmei Zhang, Mingwei Sun, Peng Wang, Shen Fan, Yanhu Mo, Xiaoxiao Xu, Hong Liu, Cheng Yang, and Chuan Shi. 2024. GraphTranslator: Aligning Graph Model to Large Language Model for Open-ended Tasks. In Proceedings of the ACM on Web Conference 2024, WWW 2024, Singapore, May 13-17, 2024 . 1003–1014.",
        "doi": null,
        "year": "2024",
        "has_doi": false,
        "title_from_ref": "GraphTranslator: Aligning Graph Model to Large Language Model for Open-ended Tasks",
        "arxiv_block": null,
        "pdf_url": "https://arxiv.org/pdf/2402.07197v4.pdf",
        "source": "arxiv_api",
        "abstract": "Large language models (LLMs) like ChatGPT, exhibit powerful zero-shot and instruction-following capabilities, have catalyzed a revolutionary transformation across diverse fields, especially for open-ended tasks. While the idea is less explored in the graph domain, despite the availability of numerous powerful graph models (GMs), they are restricted to tasks in a pre-defined form. Although several methods applying LLMs to graphs have been proposed, they fail to simultaneously handle the pre-defined and open-ended tasks, with LLM as a node feature enhancer or as a standalone predictor. To break this dilemma, we propose to bridge the pretrained GM and LLM by a Translator, named GraphTranslator, aiming to leverage GM to handle the pre-defined tasks effectively and utilize the extended interface of LLMs to offer various open-ended tasks for GM. To train such Translator, we propose a Producer capable of constructing the graph-text alignment data along node information, neighbor information and model information. By translating node representation into tokens, GraphTranslator empowers an LLM to make predictions based on language instructions, providing a unified perspective for both pre-defined and open-ended tasks. Extensive results demonstrate the effectiveness of our proposed GraphTranslator on zero-shot node classification. The graph question answering experiments reveal our GraphTranslator potential across a broad spectrum of open-ended tasks through language instructions. Our code is available at: https://github.com/alibaba/GraphTranslator.",
        "title": "GraphTranslator: Aligning Graph Model to Large Language Model for Open-ended Tasks",
        "local_pdf_path": "downloaded_papers\\GraphTranslator Aligning Graph Model to Large Language Model for Open-ended Tasks.pdf"
    },
    {
        "index": 198,
        "raw": "Qinggang Zhang, Junnan Dong, Hao Chen, Daochen Zha, Zailiang Yu, and Xiao Huang. 2024. KnowGPT: Knowledge Graph based Prompting for Large Language Models. arXiv:2312.06185 [cs.CL] https://arxiv.org/abs/2312.06185",
        "doi": null,
        "year": "2024",
        "has_doi": false,
        "title_from_ref": "KnowGPT: Knowledge Graph based Prompting for Large Language Models",
        "arxiv_block": "arXiv:2312.06185 [cs.CL]",
        "pdf_url": "https://arxiv.org/pdf/2312.06185.pdf",
        "source": "arxiv_direct",
        "abstract": null,
        "title": "KnowGPT: Knowledge Graph based Prompting for Large Language Models",
        "local_pdf_path": "downloaded_papers\\KnowGPT Knowledge Graph based Prompting for Large Language Models.pdf"
    },
    {
        "index": 199,
        "raw": "Xikun Zhang, Antoine Bosselut, Michihiro Yasunaga, Hongyu Ren, Percy Liang, Christopher D. Manning, and Jure Leskovec. 2022. GreaseLM: Graph REASoning Enhanced Language Models. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022 .",
        "doi": null,
        "year": "2022",
        "has_doi": false,
        "title_from_ref": "GreaseLM: Graph REASoning Enhanced Language Models",
        "arxiv_block": null,
        "pdf_url": "https://arxiv.org/pdf/2201.08860v1.pdf",
        "source": "arxiv_api",
        "abstract": "Answering complex questions about textual narratives requires reasoning over both stated context and the world knowledge that underlies it. However, pretrained language models (LM), the foundation of most modern QA systems, do not robustly represent latent relationships between concepts, which is necessary for reasoning. While knowledge graphs (KG) are often used to augment LMs with structured representations of world knowledge, it remains an open question how to effectively fuse and reason over the KG representations and the language context, which provides situational constraints and nuances. In this work, we propose GreaseLM, a new model that fuses encoded representations from pretrained LMs and graph neural networks over multiple layers of modality interaction operations. Information from both modalities propagates to the other, allowing language context representations to be grounded by structured world knowledge, and allowing linguistic nuances (e.g., negation, hedging) in the context to inform the graph representations of knowledge. Our results on three benchmarks in the commonsense reasoning (i.e., CommonsenseQA, OpenbookQA) and medical question answering (i.e., MedQA-USMLE) domains demonstrate that GreaseLM can more reliably answer questions that require reasoning over both situational constraints and structured knowledge, even outperforming models 8x larger.",
        "title": "GreaseLM: Graph REASoning Enhanced Language Models",
        "local_pdf_path": "downloaded_papers\\GreaseLM Graph REASoning Enhanced Language Models.pdf"
    },
    {
        "index": 200,
        "raw": "Yuyu Zhang, Hanjun Dai, Zornitsa Kozareva, Alexander J. Smola, and Le Song. 2018. Variational Reasoning for Question Answering With Knowledge Graph. In Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018 . 6069–6076.",
        "doi": null,
        "year": "2018",
        "has_doi": false,
        "title_from_ref": "Variational Reasoning for Question Answering With Knowledge Graph",
        "arxiv_block": null,
        "pdf_url": "https://arxiv.org/pdf/1709.04071v5.pdf",
        "source": "arxiv_api",
        "abstract": "Knowledge graph (KG) is known to be helpful for the task of question answering (QA), since it provides well-structured relational information between entities, and allows one to further infer indirect facts. However, it is challenging to build QA systems which can learn to reason over knowledge graphs based on question-answer pairs alone. First, when people ask questions, their expressions are noisy (for example, typos in texts, or variations in pronunciations), which is non-trivial for the QA system to match those mentioned entities to the knowledge graph. Second, many questions require multi-hop logic reasoning over the knowledge graph to retrieve the answers. To address these challenges, we propose a novel and unified deep learning architecture, and an end-to-end variational learning algorithm which can handle noise in questions, and learn multi-hop reasoning simultaneously. Our method achieves state-of-the-art performance on a recent benchmark dataset in the literature. We also derive a series of new benchmark datasets, including questions for multi-hop reasoning, questions paraphrased by neural translation model, and questions in human voice. Our method yields very promising results on all these challenging datasets.",
        "title": "Variational Reasoning for Question Answering With Knowledge Graph",
        "local_pdf_path": "downloaded_papers\\Variational Reasoning for Question Answering With Knowledge Graph.pdf"
    },
    {
        "index": 201,
        "raw": "Jianan Zhao, Le Zhuo, Yikang Shen, Meng Qu, Kai Liu, Michael Bronstein, Zhaocheng Zhu, and Jian Tang. 2023. GraphText: Graph Reasoning in Text Space. arXiv:2310.01089 [cs.CL] https://arxiv.org/abs/2310.01089",
        "doi": null,
        "year": "2023",
        "has_doi": false,
        "title_from_ref": "GraphText: Graph Reasoning in Text Space",
        "arxiv_block": "arXiv:2310.01089 [cs.CL]",
        "pdf_url": "https://arxiv.org/pdf/2310.01089.pdf",
        "source": "arxiv_direct",
        "abstract": null,
        "title": "GraphText: Graph Reasoning in Text Space",
        "local_pdf_path": "downloaded_papers\\GraphText Graph Reasoning in Text Space.pdf"
    },
    {
        "index": 202,
        "raw": "Penghao Zhao, Hailin Zhang, Qinhan Yu, Zhengren Wang, Yunteng Geng, Fangcheng Fu, Ling Yang, Wentao Zhang, Jie Jiang, and Bin Cui. 2024. Retrieval-Augmented Generation for AI-Generated Content: A Survey. arXiv:2402.19473 [cs.CV] https://arxiv.org/abs/2402.19473",
        "doi": null,
        "year": "2024",
        "has_doi": false,
        "title_from_ref": "Retrieval-Augmented Generation for AI-Generated Content: A Survey",
        "arxiv_block": "arXiv:2402.19473 [cs.CV]",
        "pdf_url": "https://arxiv.org/pdf/2402.19473.pdf",
        "source": "arxiv_direct",
        "abstract": null,
        "title": "Retrieval-Augmented Generation for AI-Generated Content: A Survey",
        "local_pdf_path": "downloaded_papers\\Retrieval-Augmented Generation for AI-Generated Content A Survey.pdf"
    },
    {
        "index": 203,
        "raw": "Yanxin Zheng, Wensheng Gan, Zefeng Chen, Zhenlian Qi, Qian Liang, and Philip S. Yu. 2024. Large Language Models for Medicine: A Survey. arXiv:2405.13055 [cs.CL] https://arxiv.org/abs/2405.13055",
        "doi": null,
        "year": "2024",
        "has_doi": false,
        "title_from_ref": "Large Language Models for Medicine: A Survey",
        "arxiv_block": "arXiv:2405.13055 [cs.CL]",
        "pdf_url": "https://arxiv.org/pdf/2405.13055.pdf",
        "source": "arxiv_direct",
        "abstract": null,
        "title": "Large Language Models for Medicine: A Survey",
        "local_pdf_path": "downloaded_papers\\Large Language Models for Medicine A Survey.pdf"
    },
    {
        "index": 204,
        "raw": "Yun Zhu, Yaoke Wang, Haizhou Shi, and Siliang Tang. 2024. Efficient Tuning and Inference for Large Language Models on Textual Graphs. arXiv:2401.15569 [cs.CL] https://arxiv.org/abs/2401.15569 J. ACM, Vol. 37, No. 4, Article 111. Publication date: September 2024.",
        "doi": null,
        "year": "2024",
        "has_doi": false,
        "title_from_ref": "Efficient Tuning and Inference for Large Language Models on Textual Graphs",
        "arxiv_block": "arXiv:2401.15569 [cs.CL]",
        "pdf_url": "https://arxiv.org/pdf/2401.15569.pdf",
        "source": "arxiv_direct",
        "abstract": null,
        "title": "Efficient Tuning and Inference for Large Language Models on Textual Graphs",
        "local_pdf_path": "downloaded_papers\\Efficient Tuning and Inference for Large Language Models on Textual Graphs.pdf"
    }
]